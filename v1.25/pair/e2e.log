I0825 02:48:50.441298      22 e2e.go:116] Starting e2e run "065f2d67-9c1e-46c1-8ee8-6bb72077ffef" on Ginkgo node 1
Aug 25 02:48:50.457: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1661395730 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Aug 25 02:48:50.542: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 02:48:50.543: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 25 02:48:50.561: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 25 02:48:50.588: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 25 02:48:50.588: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 25 02:48:50.588: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 25 02:48:50.594: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 25 02:48:50.594: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Aug 25 02:48:50.594: INFO: e2e test version: v1.25.0
Aug 25 02:48:50.595: INFO: kube-apiserver version: v1.25.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Aug 25 02:48:50.595: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 02:48:50.601: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.059 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Aug 25 02:48:50.542: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 02:48:50.543: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 25 02:48:50.561: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 25 02:48:50.588: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 25 02:48:50.588: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Aug 25 02:48:50.588: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 25 02:48:50.594: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Aug 25 02:48:50.594: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
    Aug 25 02:48:50.594: INFO: e2e test version: v1.25.0
    Aug 25 02:48:50.595: INFO: kube-apiserver version: v1.25.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Aug 25 02:48:50.595: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 02:48:50.601: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:48:50.62
Aug 25 02:48:50.620: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename discovery 08/25/22 02:48:50.621
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:48:50.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:48:50.639
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/25/22 02:48:50.645
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 25 02:48:50.988: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 25 02:48:50.990: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 25 02:48:50.990: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 25 02:48:50.990: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 25 02:48:50.990: INFO: Checking APIGroup: apps
Aug 25 02:48:50.992: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 25 02:48:50.992: INFO: Versions found [{apps/v1 v1}]
Aug 25 02:48:50.992: INFO: apps/v1 matches apps/v1
Aug 25 02:48:50.992: INFO: Checking APIGroup: events.k8s.io
Aug 25 02:48:50.994: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 25 02:48:50.994: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 25 02:48:50.994: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 25 02:48:50.994: INFO: Checking APIGroup: authentication.k8s.io
Aug 25 02:48:50.996: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 25 02:48:50.996: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 25 02:48:50.996: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 25 02:48:50.996: INFO: Checking APIGroup: authorization.k8s.io
Aug 25 02:48:50.997: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 25 02:48:50.997: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 25 02:48:50.997: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 25 02:48:50.997: INFO: Checking APIGroup: autoscaling
Aug 25 02:48:50.998: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 25 02:48:50.998: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Aug 25 02:48:50.998: INFO: autoscaling/v2 matches autoscaling/v2
Aug 25 02:48:50.998: INFO: Checking APIGroup: batch
Aug 25 02:48:51.000: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 25 02:48:51.000: INFO: Versions found [{batch/v1 v1}]
Aug 25 02:48:51.000: INFO: batch/v1 matches batch/v1
Aug 25 02:48:51.000: INFO: Checking APIGroup: certificates.k8s.io
Aug 25 02:48:51.006: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 25 02:48:51.006: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 25 02:48:51.006: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 25 02:48:51.006: INFO: Checking APIGroup: networking.k8s.io
Aug 25 02:48:51.007: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 25 02:48:51.007: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 25 02:48:51.007: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 25 02:48:51.007: INFO: Checking APIGroup: policy
Aug 25 02:48:51.009: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 25 02:48:51.009: INFO: Versions found [{policy/v1 v1}]
Aug 25 02:48:51.009: INFO: policy/v1 matches policy/v1
Aug 25 02:48:51.009: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 25 02:48:51.011: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 25 02:48:51.011: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 25 02:48:51.011: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 25 02:48:51.011: INFO: Checking APIGroup: storage.k8s.io
Aug 25 02:48:51.013: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 25 02:48:51.013: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 25 02:48:51.013: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 25 02:48:51.013: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 25 02:48:51.015: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 25 02:48:51.015: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 25 02:48:51.015: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 25 02:48:51.015: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 25 02:48:51.016: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 25 02:48:51.016: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 25 02:48:51.016: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 25 02:48:51.016: INFO: Checking APIGroup: scheduling.k8s.io
Aug 25 02:48:51.018: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 25 02:48:51.018: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 25 02:48:51.018: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 25 02:48:51.018: INFO: Checking APIGroup: coordination.k8s.io
Aug 25 02:48:51.020: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 25 02:48:51.020: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 25 02:48:51.020: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 25 02:48:51.020: INFO: Checking APIGroup: node.k8s.io
Aug 25 02:48:51.021: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 25 02:48:51.021: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 25 02:48:51.021: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 25 02:48:51.021: INFO: Checking APIGroup: discovery.k8s.io
Aug 25 02:48:51.023: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 25 02:48:51.023: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 25 02:48:51.023: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 25 02:48:51.023: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 25 02:48:51.024: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Aug 25 02:48:51.024: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 25 02:48:51.024: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Aug 25 02:48:51.024: INFO: Checking APIGroup: acme.cert-manager.io
Aug 25 02:48:51.025: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Aug 25 02:48:51.025: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Aug 25 02:48:51.025: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Aug 25 02:48:51.025: INFO: Checking APIGroup: cert-manager.io
Aug 25 02:48:51.027: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Aug 25 02:48:51.027: INFO: Versions found [{cert-manager.io/v1 v1}]
Aug 25 02:48:51.027: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Aug 25 02:48:51.027: INFO: Checking APIGroup: helm.fluxcd.io
Aug 25 02:48:51.029: INFO: PreferredVersion.GroupVersion: helm.fluxcd.io/v1
Aug 25 02:48:51.029: INFO: Versions found [{helm.fluxcd.io/v1 v1}]
Aug 25 02:48:51.029: INFO: helm.fluxcd.io/v1 matches helm.fluxcd.io/v1
Aug 25 02:48:51.029: INFO: Checking APIGroup: monitoring.coreos.com
Aug 25 02:48:51.031: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 25 02:48:51.031: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 25 02:48:51.031: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 25 02:48:51.031: INFO: Checking APIGroup: projectcontour.io
Aug 25 02:48:51.032: INFO: PreferredVersion.GroupVersion: projectcontour.io/v1
Aug 25 02:48:51.032: INFO: Versions found [{projectcontour.io/v1 v1} {projectcontour.io/v1alpha1 v1alpha1}]
Aug 25 02:48:51.032: INFO: projectcontour.io/v1 matches projectcontour.io/v1
Aug 25 02:48:51.032: INFO: Checking APIGroup: serving.knative.dev
Aug 25 02:48:51.034: INFO: PreferredVersion.GroupVersion: serving.knative.dev/v1
Aug 25 02:48:51.034: INFO: Versions found [{serving.knative.dev/v1 v1} {serving.knative.dev/v1beta1 v1beta1} {serving.knative.dev/v1alpha1 v1alpha1}]
Aug 25 02:48:51.034: INFO: serving.knative.dev/v1 matches serving.knative.dev/v1
Aug 25 02:48:51.034: INFO: Checking APIGroup: autoscaling.internal.knative.dev
Aug 25 02:48:51.035: INFO: PreferredVersion.GroupVersion: autoscaling.internal.knative.dev/v1alpha1
Aug 25 02:48:51.035: INFO: Versions found [{autoscaling.internal.knative.dev/v1alpha1 v1alpha1}]
Aug 25 02:48:51.035: INFO: autoscaling.internal.knative.dev/v1alpha1 matches autoscaling.internal.knative.dev/v1alpha1
Aug 25 02:48:51.035: INFO: Checking APIGroup: caching.internal.knative.dev
Aug 25 02:48:51.037: INFO: PreferredVersion.GroupVersion: caching.internal.knative.dev/v1alpha1
Aug 25 02:48:51.037: INFO: Versions found [{caching.internal.knative.dev/v1alpha1 v1alpha1}]
Aug 25 02:48:51.037: INFO: caching.internal.knative.dev/v1alpha1 matches caching.internal.knative.dev/v1alpha1
Aug 25 02:48:51.037: INFO: Checking APIGroup: externaldns.k8s.io
Aug 25 02:48:51.038: INFO: PreferredVersion.GroupVersion: externaldns.k8s.io/v1alpha1
Aug 25 02:48:51.038: INFO: Versions found [{externaldns.k8s.io/v1alpha1 v1alpha1}]
Aug 25 02:48:51.038: INFO: externaldns.k8s.io/v1alpha1 matches externaldns.k8s.io/v1alpha1
Aug 25 02:48:51.038: INFO: Checking APIGroup: networking.internal.knative.dev
Aug 25 02:48:51.040: INFO: PreferredVersion.GroupVersion: networking.internal.knative.dev/v1alpha1
Aug 25 02:48:51.040: INFO: Versions found [{networking.internal.knative.dev/v1alpha1 v1alpha1}]
Aug 25 02:48:51.040: INFO: networking.internal.knative.dev/v1alpha1 matches networking.internal.knative.dev/v1alpha1
Aug 25 02:48:51.040: INFO: Checking APIGroup: operator.knative.dev
Aug 25 02:48:51.042: INFO: PreferredVersion.GroupVersion: operator.knative.dev/v1beta1
Aug 25 02:48:51.042: INFO: Versions found [{operator.knative.dev/v1beta1 v1beta1}]
Aug 25 02:48:51.042: INFO: operator.knative.dev/v1beta1 matches operator.knative.dev/v1beta1
Aug 25 02:48:51.042: INFO: Checking APIGroup: metrics.k8s.io
Aug 25 02:48:51.044: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 25 02:48:51.044: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 25 02:48:51.044: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Aug 25 02:48:51.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5862" for this suite. 08/25/22 02:48:51.048
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":1,"skipped":5,"failed":0}
------------------------------
• [0.433 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:48:50.62
    Aug 25 02:48:50.620: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename discovery 08/25/22 02:48:50.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:48:50.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:48:50.639
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/25/22 02:48:50.645
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 25 02:48:50.988: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 25 02:48:50.990: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 25 02:48:50.990: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 25 02:48:50.990: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 25 02:48:50.990: INFO: Checking APIGroup: apps
    Aug 25 02:48:50.992: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 25 02:48:50.992: INFO: Versions found [{apps/v1 v1}]
    Aug 25 02:48:50.992: INFO: apps/v1 matches apps/v1
    Aug 25 02:48:50.992: INFO: Checking APIGroup: events.k8s.io
    Aug 25 02:48:50.994: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 25 02:48:50.994: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 25 02:48:50.994: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 25 02:48:50.994: INFO: Checking APIGroup: authentication.k8s.io
    Aug 25 02:48:50.996: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 25 02:48:50.996: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 25 02:48:50.996: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 25 02:48:50.996: INFO: Checking APIGroup: authorization.k8s.io
    Aug 25 02:48:50.997: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 25 02:48:50.997: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 25 02:48:50.997: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 25 02:48:50.997: INFO: Checking APIGroup: autoscaling
    Aug 25 02:48:50.998: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 25 02:48:50.998: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Aug 25 02:48:50.998: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 25 02:48:50.998: INFO: Checking APIGroup: batch
    Aug 25 02:48:51.000: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 25 02:48:51.000: INFO: Versions found [{batch/v1 v1}]
    Aug 25 02:48:51.000: INFO: batch/v1 matches batch/v1
    Aug 25 02:48:51.000: INFO: Checking APIGroup: certificates.k8s.io
    Aug 25 02:48:51.006: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 25 02:48:51.006: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 25 02:48:51.006: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 25 02:48:51.006: INFO: Checking APIGroup: networking.k8s.io
    Aug 25 02:48:51.007: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 25 02:48:51.007: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 25 02:48:51.007: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 25 02:48:51.007: INFO: Checking APIGroup: policy
    Aug 25 02:48:51.009: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 25 02:48:51.009: INFO: Versions found [{policy/v1 v1}]
    Aug 25 02:48:51.009: INFO: policy/v1 matches policy/v1
    Aug 25 02:48:51.009: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 25 02:48:51.011: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 25 02:48:51.011: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 25 02:48:51.011: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 25 02:48:51.011: INFO: Checking APIGroup: storage.k8s.io
    Aug 25 02:48:51.013: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 25 02:48:51.013: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 25 02:48:51.013: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 25 02:48:51.013: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 25 02:48:51.015: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 25 02:48:51.015: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 25 02:48:51.015: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 25 02:48:51.015: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 25 02:48:51.016: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 25 02:48:51.016: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 25 02:48:51.016: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 25 02:48:51.016: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 25 02:48:51.018: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 25 02:48:51.018: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 25 02:48:51.018: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 25 02:48:51.018: INFO: Checking APIGroup: coordination.k8s.io
    Aug 25 02:48:51.020: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 25 02:48:51.020: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 25 02:48:51.020: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 25 02:48:51.020: INFO: Checking APIGroup: node.k8s.io
    Aug 25 02:48:51.021: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 25 02:48:51.021: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 25 02:48:51.021: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 25 02:48:51.021: INFO: Checking APIGroup: discovery.k8s.io
    Aug 25 02:48:51.023: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 25 02:48:51.023: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 25 02:48:51.023: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 25 02:48:51.023: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 25 02:48:51.024: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Aug 25 02:48:51.024: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Aug 25 02:48:51.024: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Aug 25 02:48:51.024: INFO: Checking APIGroup: acme.cert-manager.io
    Aug 25 02:48:51.025: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Aug 25 02:48:51.025: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Aug 25 02:48:51.025: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Aug 25 02:48:51.025: INFO: Checking APIGroup: cert-manager.io
    Aug 25 02:48:51.027: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Aug 25 02:48:51.027: INFO: Versions found [{cert-manager.io/v1 v1}]
    Aug 25 02:48:51.027: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Aug 25 02:48:51.027: INFO: Checking APIGroup: helm.fluxcd.io
    Aug 25 02:48:51.029: INFO: PreferredVersion.GroupVersion: helm.fluxcd.io/v1
    Aug 25 02:48:51.029: INFO: Versions found [{helm.fluxcd.io/v1 v1}]
    Aug 25 02:48:51.029: INFO: helm.fluxcd.io/v1 matches helm.fluxcd.io/v1
    Aug 25 02:48:51.029: INFO: Checking APIGroup: monitoring.coreos.com
    Aug 25 02:48:51.031: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Aug 25 02:48:51.031: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.031: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Aug 25 02:48:51.031: INFO: Checking APIGroup: projectcontour.io
    Aug 25 02:48:51.032: INFO: PreferredVersion.GroupVersion: projectcontour.io/v1
    Aug 25 02:48:51.032: INFO: Versions found [{projectcontour.io/v1 v1} {projectcontour.io/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.032: INFO: projectcontour.io/v1 matches projectcontour.io/v1
    Aug 25 02:48:51.032: INFO: Checking APIGroup: serving.knative.dev
    Aug 25 02:48:51.034: INFO: PreferredVersion.GroupVersion: serving.knative.dev/v1
    Aug 25 02:48:51.034: INFO: Versions found [{serving.knative.dev/v1 v1} {serving.knative.dev/v1beta1 v1beta1} {serving.knative.dev/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.034: INFO: serving.knative.dev/v1 matches serving.knative.dev/v1
    Aug 25 02:48:51.034: INFO: Checking APIGroup: autoscaling.internal.knative.dev
    Aug 25 02:48:51.035: INFO: PreferredVersion.GroupVersion: autoscaling.internal.knative.dev/v1alpha1
    Aug 25 02:48:51.035: INFO: Versions found [{autoscaling.internal.knative.dev/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.035: INFO: autoscaling.internal.knative.dev/v1alpha1 matches autoscaling.internal.knative.dev/v1alpha1
    Aug 25 02:48:51.035: INFO: Checking APIGroup: caching.internal.knative.dev
    Aug 25 02:48:51.037: INFO: PreferredVersion.GroupVersion: caching.internal.knative.dev/v1alpha1
    Aug 25 02:48:51.037: INFO: Versions found [{caching.internal.knative.dev/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.037: INFO: caching.internal.knative.dev/v1alpha1 matches caching.internal.knative.dev/v1alpha1
    Aug 25 02:48:51.037: INFO: Checking APIGroup: externaldns.k8s.io
    Aug 25 02:48:51.038: INFO: PreferredVersion.GroupVersion: externaldns.k8s.io/v1alpha1
    Aug 25 02:48:51.038: INFO: Versions found [{externaldns.k8s.io/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.038: INFO: externaldns.k8s.io/v1alpha1 matches externaldns.k8s.io/v1alpha1
    Aug 25 02:48:51.038: INFO: Checking APIGroup: networking.internal.knative.dev
    Aug 25 02:48:51.040: INFO: PreferredVersion.GroupVersion: networking.internal.knative.dev/v1alpha1
    Aug 25 02:48:51.040: INFO: Versions found [{networking.internal.knative.dev/v1alpha1 v1alpha1}]
    Aug 25 02:48:51.040: INFO: networking.internal.knative.dev/v1alpha1 matches networking.internal.knative.dev/v1alpha1
    Aug 25 02:48:51.040: INFO: Checking APIGroup: operator.knative.dev
    Aug 25 02:48:51.042: INFO: PreferredVersion.GroupVersion: operator.knative.dev/v1beta1
    Aug 25 02:48:51.042: INFO: Versions found [{operator.knative.dev/v1beta1 v1beta1}]
    Aug 25 02:48:51.042: INFO: operator.knative.dev/v1beta1 matches operator.knative.dev/v1beta1
    Aug 25 02:48:51.042: INFO: Checking APIGroup: metrics.k8s.io
    Aug 25 02:48:51.044: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 25 02:48:51.044: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 25 02:48:51.044: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Aug 25 02:48:51.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-5862" for this suite. 08/25/22 02:48:51.048
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:48:51.054
Aug 25 02:48:51.054: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 02:48:51.055
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:48:51.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:48:51.073
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 08/25/22 02:48:51.076
Aug 25 02:48:51.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762" in namespace "projected-633" to be "Succeeded or Failed"
Aug 25 02:48:51.087: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537044ms
Aug 25 02:48:53.092: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009092762s
Aug 25 02:48:55.092: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0087627s
Aug 25 02:48:57.142: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Running", Reason="", readiness=true. Elapsed: 6.058395198s
Aug 25 02:48:59.093: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Running", Reason="", readiness=false. Elapsed: 8.009765783s
Aug 25 02:49:01.093: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.009718132s
STEP: Saw pod success 08/25/22 02:49:01.093
Aug 25 02:49:01.093: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762" satisfied condition "Succeeded or Failed"
Aug 25 02:49:01.098: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762 container client-container: <nil>
STEP: delete the pod 08/25/22 02:49:01.116
Aug 25 02:49:01.124: INFO: Waiting for pod downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762 to disappear
Aug 25 02:49:01.127: INFO: Pod downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 02:49:01.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-633" for this suite. 08/25/22 02:49:01.131
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":2,"skipped":5,"failed":0}
------------------------------
• [SLOW TEST] [10.082 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:48:51.054
    Aug 25 02:48:51.054: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 02:48:51.055
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:48:51.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:48:51.073
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 08/25/22 02:48:51.076
    Aug 25 02:48:51.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762" in namespace "projected-633" to be "Succeeded or Failed"
    Aug 25 02:48:51.087: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537044ms
    Aug 25 02:48:53.092: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009092762s
    Aug 25 02:48:55.092: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0087627s
    Aug 25 02:48:57.142: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Running", Reason="", readiness=true. Elapsed: 6.058395198s
    Aug 25 02:48:59.093: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Running", Reason="", readiness=false. Elapsed: 8.009765783s
    Aug 25 02:49:01.093: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.009718132s
    STEP: Saw pod success 08/25/22 02:49:01.093
    Aug 25 02:49:01.093: INFO: Pod "downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762" satisfied condition "Succeeded or Failed"
    Aug 25 02:49:01.098: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762 container client-container: <nil>
    STEP: delete the pod 08/25/22 02:49:01.116
    Aug 25 02:49:01.124: INFO: Waiting for pod downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762 to disappear
    Aug 25 02:49:01.127: INFO: Pod downwardapi-volume-dea25f75-2217-4c51-853c-05eb1cfe6762 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 02:49:01.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-633" for this suite. 08/25/22 02:49:01.131
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:49:01.137
Aug 25 02:49:01.137: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename watch 08/25/22 02:49:01.139
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:01.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:01.154
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/25/22 02:49:01.158
STEP: modifying the configmap once 08/25/22 02:49:01.162
STEP: modifying the configmap a second time 08/25/22 02:49:01.169
STEP: deleting the configmap 08/25/22 02:49:01.174
STEP: creating a watch on configmaps from the resource version returned by the first update 08/25/22 02:49:01.178
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/25/22 02:49:01.18
Aug 25 02:49:01.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-795  4af88129-2d9a-4ecb-ac10-3229f29f8e9e 4374 0 2022-08-25 02:49:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-08-25 02:49:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 02:49:01.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-795  4af88129-2d9a-4ecb-ac10-3229f29f8e9e 4375 0 2022-08-25 02:49:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-08-25 02:49:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 25 02:49:01.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-795" for this suite. 08/25/22 02:49:01.184
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":3,"skipped":19,"failed":0}
------------------------------
• [0.051 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:49:01.137
    Aug 25 02:49:01.137: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename watch 08/25/22 02:49:01.139
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:01.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:01.154
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/25/22 02:49:01.158
    STEP: modifying the configmap once 08/25/22 02:49:01.162
    STEP: modifying the configmap a second time 08/25/22 02:49:01.169
    STEP: deleting the configmap 08/25/22 02:49:01.174
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/25/22 02:49:01.178
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/25/22 02:49:01.18
    Aug 25 02:49:01.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-795  4af88129-2d9a-4ecb-ac10-3229f29f8e9e 4374 0 2022-08-25 02:49:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-08-25 02:49:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 02:49:01.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-795  4af88129-2d9a-4ecb-ac10-3229f29f8e9e 4375 0 2022-08-25 02:49:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-08-25 02:49:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 25 02:49:01.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-795" for this suite. 08/25/22 02:49:01.184
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:49:01.188
Aug 25 02:49:01.188: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 02:49:01.189
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:01.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:01.201
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-3452 08/25/22 02:49:01.205
STEP: creating service affinity-nodeport in namespace services-3452 08/25/22 02:49:01.205
STEP: creating replication controller affinity-nodeport in namespace services-3452 08/25/22 02:49:01.215
I0825 02:49:01.220717      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3452, replica count: 3
I0825 02:49:04.272659      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 02:49:04.284: INFO: Creating new exec pod
Aug 25 02:49:04.289: INFO: Waiting up to 5m0s for pod "execpod-affinityw9dz9" in namespace "services-3452" to be "running"
Aug 25 02:49:04.294: INFO: Pod "execpod-affinityw9dz9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027374ms
Aug 25 02:49:06.297: INFO: Pod "execpod-affinityw9dz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007144511s
Aug 25 02:49:06.297: INFO: Pod "execpod-affinityw9dz9" satisfied condition "running"
Aug 25 02:49:07.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 25 02:49:07.519: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 25 02:49:07.519: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 02:49:07.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.198.236 80'
Aug 25 02:49:07.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.198.236 80\nConnection to 10.99.198.236 80 port [tcp/http] succeeded!\n"
Aug 25 02:49:07.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 02:49:07.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 31265'
Aug 25 02:49:07.893: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 31265\nConnection to 86.109.11.217 31265 port [tcp/*] succeeded!\n"
Aug 25 02:49:07.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 02:49:07.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:31265/ ; done'
Aug 25 02:49:08.167: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n"
Aug 25 02:49:08.167: INFO: stdout: "\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4"
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
Aug 25 02:49:08.167: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3452, will wait for the garbage collector to delete the pods 08/25/22 02:49:08.176
Aug 25 02:49:08.237: INFO: Deleting ReplicationController affinity-nodeport took: 5.520738ms
Aug 25 02:49:08.338: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.998933ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 02:49:10.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3452" for this suite. 08/25/22 02:49:10.555
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":4,"skipped":22,"failed":0}
------------------------------
• [SLOW TEST] [9.371 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:49:01.188
    Aug 25 02:49:01.188: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 02:49:01.189
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:01.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:01.201
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-3452 08/25/22 02:49:01.205
    STEP: creating service affinity-nodeport in namespace services-3452 08/25/22 02:49:01.205
    STEP: creating replication controller affinity-nodeport in namespace services-3452 08/25/22 02:49:01.215
    I0825 02:49:01.220717      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3452, replica count: 3
    I0825 02:49:04.272659      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 02:49:04.284: INFO: Creating new exec pod
    Aug 25 02:49:04.289: INFO: Waiting up to 5m0s for pod "execpod-affinityw9dz9" in namespace "services-3452" to be "running"
    Aug 25 02:49:04.294: INFO: Pod "execpod-affinityw9dz9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027374ms
    Aug 25 02:49:06.297: INFO: Pod "execpod-affinityw9dz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007144511s
    Aug 25 02:49:06.297: INFO: Pod "execpod-affinityw9dz9" satisfied condition "running"
    Aug 25 02:49:07.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Aug 25 02:49:07.519: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 25 02:49:07.519: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 02:49:07.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.198.236 80'
    Aug 25 02:49:07.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.198.236 80\nConnection to 10.99.198.236 80 port [tcp/http] succeeded!\n"
    Aug 25 02:49:07.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 02:49:07.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 31265'
    Aug 25 02:49:07.893: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 31265\nConnection to 86.109.11.217 31265 port [tcp/*] succeeded!\n"
    Aug 25 02:49:07.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 02:49:07.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-3452 exec execpod-affinityw9dz9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:31265/ ; done'
    Aug 25 02:49:08.167: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31265/\n"
    Aug 25 02:49:08.167: INFO: stdout: "\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4\naffinity-nodeport-mgwn4"
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Received response from host: affinity-nodeport-mgwn4
    Aug 25 02:49:08.167: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3452, will wait for the garbage collector to delete the pods 08/25/22 02:49:08.176
    Aug 25 02:49:08.237: INFO: Deleting ReplicationController affinity-nodeport took: 5.520738ms
    Aug 25 02:49:08.338: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.998933ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 02:49:10.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3452" for this suite. 08/25/22 02:49:10.555
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:49:10.562
Aug 25 02:49:10.562: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 02:49:10.564
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:10.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:10.583
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 25 02:49:10.586: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 02:49:11.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7997" for this suite. 08/25/22 02:49:11.131
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":5,"skipped":39,"failed":0}
------------------------------
• [0.574 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:49:10.562
    Aug 25 02:49:10.562: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 02:49:10.564
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:10.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:10.583
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 25 02:49:10.586: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 02:49:11.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7997" for this suite. 08/25/22 02:49:11.131
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:49:11.136
Aug 25 02:49:11.136: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename cronjob 08/25/22 02:49:11.138
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:11.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:11.153
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/25/22 02:49:11.157
STEP: Ensuring no jobs are scheduled 08/25/22 02:49:11.161
STEP: Ensuring no job exists by listing jobs explicitly 08/25/22 02:54:11.169
STEP: Removing cronjob 08/25/22 02:54:11.173
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 25 02:54:11.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2329" for this suite. 08/25/22 02:54:11.183
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":6,"skipped":42,"failed":0}
------------------------------
• [SLOW TEST] [300.052 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:49:11.136
    Aug 25 02:49:11.136: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename cronjob 08/25/22 02:49:11.138
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:49:11.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:49:11.153
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/25/22 02:49:11.157
    STEP: Ensuring no jobs are scheduled 08/25/22 02:49:11.161
    STEP: Ensuring no job exists by listing jobs explicitly 08/25/22 02:54:11.169
    STEP: Removing cronjob 08/25/22 02:54:11.173
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 25 02:54:11.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2329" for this suite. 08/25/22 02:54:11.183
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:54:11.191
Aug 25 02:54:11.191: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 02:54:11.192
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:11.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:11.208
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 08/25/22 02:54:11.213
STEP: waiting for pod running 08/25/22 02:54:11.219
Aug 25 02:54:11.219: INFO: Waiting up to 2m0s for pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" in namespace "var-expansion-9066" to be "running"
Aug 25 02:54:11.223: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04263ms
Aug 25 02:54:13.228: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008667489s
Aug 25 02:54:15.230: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Running", Reason="", readiness=true. Elapsed: 4.011064798s
Aug 25 02:54:15.230: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" satisfied condition "running"
STEP: creating a file in subpath 08/25/22 02:54:15.23
Aug 25 02:54:15.234: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9066 PodName:var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 02:54:15.234: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 02:54:15.235: INFO: ExecWithOptions: Clientset creation
Aug 25 02:54:15.235: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9066/pods/var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/25/22 02:54:15.352
Aug 25 02:54:15.357: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9066 PodName:var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 02:54:15.357: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 02:54:15.358: INFO: ExecWithOptions: Clientset creation
Aug 25 02:54:15.358: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9066/pods/var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/25/22 02:54:15.455
Aug 25 02:54:15.967: INFO: Successfully updated pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530"
STEP: waiting for annotated pod running 08/25/22 02:54:15.967
Aug 25 02:54:15.967: INFO: Waiting up to 2m0s for pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" in namespace "var-expansion-9066" to be "running"
Aug 25 02:54:15.970: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Running", Reason="", readiness=true. Elapsed: 3.389857ms
Aug 25 02:54:15.970: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" satisfied condition "running"
STEP: deleting the pod gracefully 08/25/22 02:54:15.97
Aug 25 02:54:15.970: INFO: Deleting pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" in namespace "var-expansion-9066"
Aug 25 02:54:15.976: INFO: Wait up to 5m0s for pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 02:54:47.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9066" for this suite. 08/25/22 02:54:47.99
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":7,"skipped":65,"failed":0}
------------------------------
• [SLOW TEST] [36.804 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:54:11.191
    Aug 25 02:54:11.191: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 02:54:11.192
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:11.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:11.208
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 08/25/22 02:54:11.213
    STEP: waiting for pod running 08/25/22 02:54:11.219
    Aug 25 02:54:11.219: INFO: Waiting up to 2m0s for pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" in namespace "var-expansion-9066" to be "running"
    Aug 25 02:54:11.223: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04263ms
    Aug 25 02:54:13.228: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008667489s
    Aug 25 02:54:15.230: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Running", Reason="", readiness=true. Elapsed: 4.011064798s
    Aug 25 02:54:15.230: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" satisfied condition "running"
    STEP: creating a file in subpath 08/25/22 02:54:15.23
    Aug 25 02:54:15.234: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9066 PodName:var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 02:54:15.234: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 02:54:15.235: INFO: ExecWithOptions: Clientset creation
    Aug 25 02:54:15.235: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9066/pods/var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/25/22 02:54:15.352
    Aug 25 02:54:15.357: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9066 PodName:var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 02:54:15.357: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 02:54:15.358: INFO: ExecWithOptions: Clientset creation
    Aug 25 02:54:15.358: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9066/pods/var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/25/22 02:54:15.455
    Aug 25 02:54:15.967: INFO: Successfully updated pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530"
    STEP: waiting for annotated pod running 08/25/22 02:54:15.967
    Aug 25 02:54:15.967: INFO: Waiting up to 2m0s for pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" in namespace "var-expansion-9066" to be "running"
    Aug 25 02:54:15.970: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530": Phase="Running", Reason="", readiness=true. Elapsed: 3.389857ms
    Aug 25 02:54:15.970: INFO: Pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" satisfied condition "running"
    STEP: deleting the pod gracefully 08/25/22 02:54:15.97
    Aug 25 02:54:15.970: INFO: Deleting pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" in namespace "var-expansion-9066"
    Aug 25 02:54:15.976: INFO: Wait up to 5m0s for pod "var-expansion-98bb988d-752c-4fd3-9d2d-d2e5432a5530" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 02:54:47.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9066" for this suite. 08/25/22 02:54:47.99
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:54:47.996
Aug 25 02:54:47.996: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 02:54:47.997
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:48.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:48.012
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-243a59f3-5171-4699-9898-7d1ee3929069 08/25/22 02:54:48.016
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 02:54:48.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3708" for this suite. 08/25/22 02:54:48.021
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":8,"skipped":74,"failed":0}
------------------------------
• [0.029 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:54:47.996
    Aug 25 02:54:47.996: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 02:54:47.997
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:48.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:48.012
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-243a59f3-5171-4699-9898-7d1ee3929069 08/25/22 02:54:48.016
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 02:54:48.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3708" for this suite. 08/25/22 02:54:48.021
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:54:48.025
Aug 25 02:54:48.026: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename podtemplate 08/25/22 02:54:48.027
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:48.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:48.044
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Aug 25 02:54:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3789" for this suite. 08/25/22 02:54:48.077
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":9,"skipped":75,"failed":0}
------------------------------
• [0.055 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:54:48.025
    Aug 25 02:54:48.026: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename podtemplate 08/25/22 02:54:48.027
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:48.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:48.044
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Aug 25 02:54:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3789" for this suite. 08/25/22 02:54:48.077
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:54:48.082
Aug 25 02:54:48.082: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 02:54:48.083
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:48.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:48.099
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 08/25/22 02:54:48.102
Aug 25 02:54:48.108: INFO: Waiting up to 5m0s for pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f" in namespace "downward-api-8844" to be "running and ready"
Aug 25 02:54:48.112: INFO: Pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485175ms
Aug 25 02:54:48.112: INFO: The phase of Pod labelsupdatee1628d44-7785-409a-8ba5-25c71092169f is Pending, waiting for it to be Running (with Ready = true)
Aug 25 02:54:50.117: INFO: Pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009419853s
Aug 25 02:54:50.118: INFO: The phase of Pod labelsupdatee1628d44-7785-409a-8ba5-25c71092169f is Running (Ready = true)
Aug 25 02:54:50.118: INFO: Pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f" satisfied condition "running and ready"
Aug 25 02:54:50.654: INFO: Successfully updated pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 02:54:54.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8844" for this suite. 08/25/22 02:54:54.678
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":10,"skipped":95,"failed":0}
------------------------------
• [SLOW TEST] [6.601 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:54:48.082
    Aug 25 02:54:48.082: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 02:54:48.083
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:48.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:48.099
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 08/25/22 02:54:48.102
    Aug 25 02:54:48.108: INFO: Waiting up to 5m0s for pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f" in namespace "downward-api-8844" to be "running and ready"
    Aug 25 02:54:48.112: INFO: Pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485175ms
    Aug 25 02:54:48.112: INFO: The phase of Pod labelsupdatee1628d44-7785-409a-8ba5-25c71092169f is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 02:54:50.117: INFO: Pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009419853s
    Aug 25 02:54:50.118: INFO: The phase of Pod labelsupdatee1628d44-7785-409a-8ba5-25c71092169f is Running (Ready = true)
    Aug 25 02:54:50.118: INFO: Pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f" satisfied condition "running and ready"
    Aug 25 02:54:50.654: INFO: Successfully updated pod "labelsupdatee1628d44-7785-409a-8ba5-25c71092169f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 02:54:54.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8844" for this suite. 08/25/22 02:54:54.678
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:54:54.687
Aug 25 02:54:54.687: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 02:54:54.688
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:54.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:54.703
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-3a8edbbe-8572-4f7d-b7da-59048eaa4a89 08/25/22 02:54:54.71
STEP: Creating the pod 08/25/22 02:54:54.714
Aug 25 02:54:54.721: INFO: Waiting up to 5m0s for pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23" in namespace "configmap-2420" to be "running"
Aug 25 02:54:54.723: INFO: Pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233968ms
Aug 25 02:54:56.729: INFO: Pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23": Phase="Running", Reason="", readiness=false. Elapsed: 2.008301346s
Aug 25 02:54:56.729: INFO: Pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23" satisfied condition "running"
STEP: Waiting for pod with text data 08/25/22 02:54:56.729
STEP: Waiting for pod with binary data 08/25/22 02:54:56.734
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 02:54:56.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2420" for this suite. 08/25/22 02:54:56.743
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":11,"skipped":147,"failed":0}
------------------------------
• [2.061 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:54:54.687
    Aug 25 02:54:54.687: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 02:54:54.688
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:54.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:54.703
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-3a8edbbe-8572-4f7d-b7da-59048eaa4a89 08/25/22 02:54:54.71
    STEP: Creating the pod 08/25/22 02:54:54.714
    Aug 25 02:54:54.721: INFO: Waiting up to 5m0s for pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23" in namespace "configmap-2420" to be "running"
    Aug 25 02:54:54.723: INFO: Pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233968ms
    Aug 25 02:54:56.729: INFO: Pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23": Phase="Running", Reason="", readiness=false. Elapsed: 2.008301346s
    Aug 25 02:54:56.729: INFO: Pod "pod-configmaps-21ebcf04-ca28-4165-b89f-84f17377fe23" satisfied condition "running"
    STEP: Waiting for pod with text data 08/25/22 02:54:56.729
    STEP: Waiting for pod with binary data 08/25/22 02:54:56.734
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 02:54:56.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2420" for this suite. 08/25/22 02:54:56.743
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:54:56.749
Aug 25 02:54:56.749: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 02:54:56.751
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:56.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:56.769
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-1867 08/25/22 02:54:56.773
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[] 08/25/22 02:54:56.782
Aug 25 02:54:56.785: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 25 02:54:57.795: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1867 08/25/22 02:54:57.795
Aug 25 02:54:57.804: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1867" to be "running and ready"
Aug 25 02:54:57.807: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.466729ms
Aug 25 02:54:57.807: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 02:54:59.812: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008755718s
Aug 25 02:54:59.812: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 25 02:54:59.813: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[pod1:[100]] 08/25/22 02:54:59.816
Aug 25 02:54:59.827: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1867 08/25/22 02:54:59.827
Aug 25 02:54:59.832: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1867" to be "running and ready"
Aug 25 02:54:59.835: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86833ms
Aug 25 02:54:59.835: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 02:55:01.840: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00741614s
Aug 25 02:55:01.840: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 25 02:55:01.840: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[pod1:[100] pod2:[101]] 08/25/22 02:55:01.843
Aug 25 02:55:01.856: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/25/22 02:55:01.856
Aug 25 02:55:01.856: INFO: Creating new exec pod
Aug 25 02:55:01.859: INFO: Waiting up to 5m0s for pod "execpodqxw7f" in namespace "services-1867" to be "running"
Aug 25 02:55:01.862: INFO: Pod "execpodqxw7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.194182ms
Aug 25 02:55:03.867: INFO: Pod "execpodqxw7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007320347s
Aug 25 02:55:03.867: INFO: Pod "execpodqxw7f" satisfied condition "running"
Aug 25 02:55:04.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 25 02:55:05.068: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 25 02:55:05.068: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 02:55:05.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.144.224 80'
Aug 25 02:55:05.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.144.224 80\nConnection to 10.101.144.224 80 port [tcp/http] succeeded!\n"
Aug 25 02:55:05.238: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 02:55:05.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 25 02:55:05.419: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 25 02:55:05.419: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 02:55:05.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.144.224 81'
Aug 25 02:55:05.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.144.224 81\nConnection to 10.101.144.224 81 port [tcp/*] succeeded!\n"
Aug 25 02:55:05.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1867 08/25/22 02:55:05.605
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[pod2:[101]] 08/25/22 02:55:05.612
Aug 25 02:55:05.623: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1867 08/25/22 02:55:05.623
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[] 08/25/22 02:55:05.628
Aug 25 02:55:06.646: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 02:55:06.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1867" for this suite. 08/25/22 02:55:06.663
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":12,"skipped":153,"failed":0}
------------------------------
• [SLOW TEST] [9.919 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:54:56.749
    Aug 25 02:54:56.749: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 02:54:56.751
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:54:56.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:54:56.769
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-1867 08/25/22 02:54:56.773
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[] 08/25/22 02:54:56.782
    Aug 25 02:54:56.785: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 25 02:54:57.795: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1867 08/25/22 02:54:57.795
    Aug 25 02:54:57.804: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1867" to be "running and ready"
    Aug 25 02:54:57.807: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.466729ms
    Aug 25 02:54:57.807: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 02:54:59.812: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008755718s
    Aug 25 02:54:59.812: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 25 02:54:59.813: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[pod1:[100]] 08/25/22 02:54:59.816
    Aug 25 02:54:59.827: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1867 08/25/22 02:54:59.827
    Aug 25 02:54:59.832: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1867" to be "running and ready"
    Aug 25 02:54:59.835: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86833ms
    Aug 25 02:54:59.835: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 02:55:01.840: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00741614s
    Aug 25 02:55:01.840: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 25 02:55:01.840: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[pod1:[100] pod2:[101]] 08/25/22 02:55:01.843
    Aug 25 02:55:01.856: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/25/22 02:55:01.856
    Aug 25 02:55:01.856: INFO: Creating new exec pod
    Aug 25 02:55:01.859: INFO: Waiting up to 5m0s for pod "execpodqxw7f" in namespace "services-1867" to be "running"
    Aug 25 02:55:01.862: INFO: Pod "execpodqxw7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.194182ms
    Aug 25 02:55:03.867: INFO: Pod "execpodqxw7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007320347s
    Aug 25 02:55:03.867: INFO: Pod "execpodqxw7f" satisfied condition "running"
    Aug 25 02:55:04.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Aug 25 02:55:05.068: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 25 02:55:05.068: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 02:55:05.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.144.224 80'
    Aug 25 02:55:05.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.144.224 80\nConnection to 10.101.144.224 80 port [tcp/http] succeeded!\n"
    Aug 25 02:55:05.238: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 02:55:05.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Aug 25 02:55:05.419: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 25 02:55:05.419: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 02:55:05.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1867 exec execpodqxw7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.144.224 81'
    Aug 25 02:55:05.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.144.224 81\nConnection to 10.101.144.224 81 port [tcp/*] succeeded!\n"
    Aug 25 02:55:05.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1867 08/25/22 02:55:05.605
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[pod2:[101]] 08/25/22 02:55:05.612
    Aug 25 02:55:05.623: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1867 08/25/22 02:55:05.623
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1867 to expose endpoints map[] 08/25/22 02:55:05.628
    Aug 25 02:55:06.646: INFO: successfully validated that service multi-endpoint-test in namespace services-1867 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 02:55:06.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1867" for this suite. 08/25/22 02:55:06.663
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:55:06.671
Aug 25 02:55:06.671: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename init-container 08/25/22 02:55:06.672
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:55:06.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:55:06.69
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 08/25/22 02:55:06.693
Aug 25 02:55:06.693: INFO: PodSpec: initContainers in spec.initContainers
Aug 25 02:55:55.279: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-278f3eaa-14c8-4c6d-8dfe-94688d02402d", GenerateName:"", Namespace:"init-container-9981", SelfLink:"", UID:"9ea7ef97-431e-432f-81c7-ecd088a91ef3", ResourceVersion:"7095", Generation:0, CreationTimestamp:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"693872145"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014b8168), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 25, 2, 55, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014b81b0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-q8swv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003aa60e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-q8swv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-q8swv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-q8swv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ba4330), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"bobymcbobs-c849-control-plane-p55pp", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0006f8150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ba43b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ba43d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002ba43d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002ba43dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00100e080), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"86.109.11.217", PodIP:"192.168.0.40", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.0.40"}}, StartTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f8230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f82a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://7018fba2f63c8ab6bd25c04427d26bd2265f9b16a5ee186c3532cb15b28e2906", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa6160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa6140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002ba445f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 02:55:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9981" for this suite. 08/25/22 02:55:55.283
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":13,"skipped":205,"failed":0}
------------------------------
• [SLOW TEST] [48.617 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:55:06.671
    Aug 25 02:55:06.671: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename init-container 08/25/22 02:55:06.672
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:55:06.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:55:06.69
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 08/25/22 02:55:06.693
    Aug 25 02:55:06.693: INFO: PodSpec: initContainers in spec.initContainers
    Aug 25 02:55:55.279: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-278f3eaa-14c8-4c6d-8dfe-94688d02402d", GenerateName:"", Namespace:"init-container-9981", SelfLink:"", UID:"9ea7ef97-431e-432f-81c7-ecd088a91ef3", ResourceVersion:"7095", Generation:0, CreationTimestamp:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"693872145"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014b8168), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 25, 2, 55, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014b81b0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-q8swv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003aa60e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-q8swv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-q8swv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-q8swv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ba4330), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"bobymcbobs-c849-control-plane-p55pp", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0006f8150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ba43b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ba43d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002ba43d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002ba43dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00100e080), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"86.109.11.217", PodIP:"192.168.0.40", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.0.40"}}, StartTime:time.Date(2022, time.August, 25, 2, 55, 6, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f8230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f82a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://7018fba2f63c8ab6bd25c04427d26bd2265f9b16a5ee186c3532cb15b28e2906", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa6160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa6140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002ba445f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 02:55:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9981" for this suite. 08/25/22 02:55:55.283
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:55:55.288
Aug 25 02:55:55.289: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename init-container 08/25/22 02:55:55.289
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:55:55.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:55:55.304
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 08/25/22 02:55:55.308
Aug 25 02:55:55.308: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 02:55:58.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5863" for this suite. 08/25/22 02:55:58.311
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":14,"skipped":214,"failed":0}
------------------------------
• [3.027 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:55:55.288
    Aug 25 02:55:55.289: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename init-container 08/25/22 02:55:55.289
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:55:55.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:55:55.304
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 08/25/22 02:55:55.308
    Aug 25 02:55:55.308: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 02:55:58.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5863" for this suite. 08/25/22 02:55:58.311
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:55:58.316
Aug 25 02:55:58.316: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 02:55:58.317
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:55:58.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:55:58.333
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Aug 25 02:55:58.337: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/25/22 02:56:02.414
Aug 25 02:56:02.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 create -f -'
Aug 25 02:56:03.172: INFO: stderr: ""
Aug 25 02:56:03.172: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 25 02:56:03.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 delete e2e-test-crd-publish-openapi-3021-crds test-cr'
Aug 25 02:56:03.264: INFO: stderr: ""
Aug 25 02:56:03.264: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 25 02:56:03.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 apply -f -'
Aug 25 02:56:03.523: INFO: stderr: ""
Aug 25 02:56:03.523: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 25 02:56:03.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 delete e2e-test-crd-publish-openapi-3021-crds test-cr'
Aug 25 02:56:03.596: INFO: stderr: ""
Aug 25 02:56:03.596: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/25/22 02:56:03.596
Aug 25 02:56:03.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 explain e2e-test-crd-publish-openapi-3021-crds'
Aug 25 02:56:03.851: INFO: stderr: ""
Aug 25 02:56:03.851: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3021-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 02:56:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5087" for this suite. 08/25/22 02:56:07.945
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":15,"skipped":214,"failed":0}
------------------------------
• [SLOW TEST] [9.635 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:55:58.316
    Aug 25 02:55:58.316: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 02:55:58.317
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:55:58.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:55:58.333
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Aug 25 02:55:58.337: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/25/22 02:56:02.414
    Aug 25 02:56:02.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 create -f -'
    Aug 25 02:56:03.172: INFO: stderr: ""
    Aug 25 02:56:03.172: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 25 02:56:03.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 delete e2e-test-crd-publish-openapi-3021-crds test-cr'
    Aug 25 02:56:03.264: INFO: stderr: ""
    Aug 25 02:56:03.264: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 25 02:56:03.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 apply -f -'
    Aug 25 02:56:03.523: INFO: stderr: ""
    Aug 25 02:56:03.523: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 25 02:56:03.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 --namespace=crd-publish-openapi-5087 delete e2e-test-crd-publish-openapi-3021-crds test-cr'
    Aug 25 02:56:03.596: INFO: stderr: ""
    Aug 25 02:56:03.596: INFO: stdout: "e2e-test-crd-publish-openapi-3021-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/25/22 02:56:03.596
    Aug 25 02:56:03.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5087 explain e2e-test-crd-publish-openapi-3021-crds'
    Aug 25 02:56:03.851: INFO: stderr: ""
    Aug 25 02:56:03.851: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3021-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 02:56:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5087" for this suite. 08/25/22 02:56:07.945
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:07.951
Aug 25 02:56:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 02:56:07.952
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:07.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:07.971
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 25 02:56:07.982: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 25 02:56:12.988: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/25/22 02:56:12.988
Aug 25 02:56:12.989: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/25/22 02:56:12.998
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 02:56:13.009: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8932  6505eb95-b42d-4105-abe8-42df9e20b9b1 7287 1 2022-08-25 02:56:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-08-25 02:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a101e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 25 02:56:13.012: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Aug 25 02:56:13.012: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 25 02:56:13.012: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8932  d47d3e52-6586-4dab-b2e0-ecf025777ce8 7288 1 2022-08-25 02:56:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 6505eb95-b42d-4105-abe8-42df9e20b9b1 0xc004a10637 0xc004a10638}] [] [{e2e.test Update apps/v1 2022-08-25 02:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 02:56:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-25 02:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"6505eb95-b42d-4105-abe8-42df9e20b9b1\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a10768 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 25 02:56:13.016: INFO: Pod "test-cleanup-controller-fhknx" is available:
&Pod{ObjectMeta:{test-cleanup-controller-fhknx test-cleanup-controller- deployment-8932  12cc1f31-3cbb-4dc7-9aec-ed74d8722e7e 7280 0 2022-08-25 02:56:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller d47d3e52-6586-4dab-b2e0-ecf025777ce8 0xc004a10b5f 0xc004a10b80}] [] [{kube-controller-manager Update v1 2022-08-25 02:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d47d3e52-6586-4dab-b2e0-ecf025777ce8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 02:56:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr66g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.39,StartTime:2022-08-25 02:56:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 02:56:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a6a4772b96bbfe026429f857c6b03475231a9c4e01a85695bcb134bb9bb33e6e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 02:56:13.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8932" for this suite. 08/25/22 02:56:13.02
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":16,"skipped":214,"failed":0}
------------------------------
• [SLOW TEST] [5.073 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:07.951
    Aug 25 02:56:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 02:56:07.952
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:07.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:07.971
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 25 02:56:07.982: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 25 02:56:12.988: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/25/22 02:56:12.988
    Aug 25 02:56:12.989: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/25/22 02:56:12.998
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 02:56:13.009: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8932  6505eb95-b42d-4105-abe8-42df9e20b9b1 7287 1 2022-08-25 02:56:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-08-25 02:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a101e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 25 02:56:13.012: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Aug 25 02:56:13.012: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Aug 25 02:56:13.012: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8932  d47d3e52-6586-4dab-b2e0-ecf025777ce8 7288 1 2022-08-25 02:56:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 6505eb95-b42d-4105-abe8-42df9e20b9b1 0xc004a10637 0xc004a10638}] [] [{e2e.test Update apps/v1 2022-08-25 02:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 02:56:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-25 02:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"6505eb95-b42d-4105-abe8-42df9e20b9b1\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a10768 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 02:56:13.016: INFO: Pod "test-cleanup-controller-fhknx" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-fhknx test-cleanup-controller- deployment-8932  12cc1f31-3cbb-4dc7-9aec-ed74d8722e7e 7280 0 2022-08-25 02:56:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller d47d3e52-6586-4dab-b2e0-ecf025777ce8 0xc004a10b5f 0xc004a10b80}] [] [{kube-controller-manager Update v1 2022-08-25 02:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d47d3e52-6586-4dab-b2e0-ecf025777ce8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 02:56:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr66g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 02:56:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.39,StartTime:2022-08-25 02:56:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 02:56:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a6a4772b96bbfe026429f857c6b03475231a9c4e01a85695bcb134bb9bb33e6e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 02:56:13.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8932" for this suite. 08/25/22 02:56:13.02
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:13.025
Aug 25 02:56:13.025: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 02:56:13.026
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:13.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:13.038
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7678 08/25/22 02:56:13.041
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7678 08/25/22 02:56:13.044
Aug 25 02:56:13.050: INFO: Found 0 stateful pods, waiting for 1
Aug 25 02:56:23.055: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/25/22 02:56:23.063
STEP: updating a scale subresource 08/25/22 02:56:23.066
STEP: verifying the statefulset Spec.Replicas was modified 08/25/22 02:56:23.072
STEP: Patch a scale subresource 08/25/22 02:56:23.077
STEP: verifying the statefulset Spec.Replicas was modified 08/25/22 02:56:23.085
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 02:56:23.089: INFO: Deleting all statefulset in ns statefulset-7678
Aug 25 02:56:23.092: INFO: Scaling statefulset ss to 0
Aug 25 02:56:33.111: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 02:56:33.115: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 02:56:33.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7678" for this suite. 08/25/22 02:56:33.132
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":17,"skipped":215,"failed":0}
------------------------------
• [SLOW TEST] [20.112 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:13.025
    Aug 25 02:56:13.025: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 02:56:13.026
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:13.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:13.038
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7678 08/25/22 02:56:13.041
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7678 08/25/22 02:56:13.044
    Aug 25 02:56:13.050: INFO: Found 0 stateful pods, waiting for 1
    Aug 25 02:56:23.055: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/25/22 02:56:23.063
    STEP: updating a scale subresource 08/25/22 02:56:23.066
    STEP: verifying the statefulset Spec.Replicas was modified 08/25/22 02:56:23.072
    STEP: Patch a scale subresource 08/25/22 02:56:23.077
    STEP: verifying the statefulset Spec.Replicas was modified 08/25/22 02:56:23.085
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 02:56:23.089: INFO: Deleting all statefulset in ns statefulset-7678
    Aug 25 02:56:23.092: INFO: Scaling statefulset ss to 0
    Aug 25 02:56:33.111: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 02:56:33.115: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 02:56:33.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7678" for this suite. 08/25/22 02:56:33.132
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:33.138
Aug 25 02:56:33.139: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 02:56:33.14
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:33.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:33.155
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 08/25/22 02:56:33.159
Aug 25 02:56:33.165: INFO: Waiting up to 5m0s for pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c" in namespace "emptydir-6536" to be "Succeeded or Failed"
Aug 25 02:56:33.167: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268763ms
Aug 25 02:56:35.172: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007487718s
Aug 25 02:56:37.173: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00786253s
STEP: Saw pod success 08/25/22 02:56:37.173
Aug 25 02:56:37.173: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c" satisfied condition "Succeeded or Failed"
Aug 25 02:56:37.177: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-05147593-ceeb-496c-bd68-9d5cca1e891c container test-container: <nil>
STEP: delete the pod 08/25/22 02:56:37.195
Aug 25 02:56:37.204: INFO: Waiting for pod pod-05147593-ceeb-496c-bd68-9d5cca1e891c to disappear
Aug 25 02:56:37.207: INFO: Pod pod-05147593-ceeb-496c-bd68-9d5cca1e891c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 02:56:37.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6536" for this suite. 08/25/22 02:56:37.211
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":18,"skipped":232,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:33.138
    Aug 25 02:56:33.139: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 02:56:33.14
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:33.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:33.155
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/25/22 02:56:33.159
    Aug 25 02:56:33.165: INFO: Waiting up to 5m0s for pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c" in namespace "emptydir-6536" to be "Succeeded or Failed"
    Aug 25 02:56:33.167: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268763ms
    Aug 25 02:56:35.172: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007487718s
    Aug 25 02:56:37.173: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00786253s
    STEP: Saw pod success 08/25/22 02:56:37.173
    Aug 25 02:56:37.173: INFO: Pod "pod-05147593-ceeb-496c-bd68-9d5cca1e891c" satisfied condition "Succeeded or Failed"
    Aug 25 02:56:37.177: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-05147593-ceeb-496c-bd68-9d5cca1e891c container test-container: <nil>
    STEP: delete the pod 08/25/22 02:56:37.195
    Aug 25 02:56:37.204: INFO: Waiting for pod pod-05147593-ceeb-496c-bd68-9d5cca1e891c to disappear
    Aug 25 02:56:37.207: INFO: Pod pod-05147593-ceeb-496c-bd68-9d5cca1e891c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 02:56:37.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6536" for this suite. 08/25/22 02:56:37.211
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:37.217
Aug 25 02:56:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 02:56:37.219
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:37.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:37.233
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 08/25/22 02:56:37.237
Aug 25 02:56:37.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-2046 cluster-info'
Aug 25 02:56:37.320: INFO: stderr: ""
Aug 25 02:56:37.320: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 02:56:37.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2046" for this suite. 08/25/22 02:56:37.325
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":19,"skipped":236,"failed":0}
------------------------------
• [0.113 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:37.217
    Aug 25 02:56:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 02:56:37.219
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:37.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:37.233
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 08/25/22 02:56:37.237
    Aug 25 02:56:37.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-2046 cluster-info'
    Aug 25 02:56:37.320: INFO: stderr: ""
    Aug 25 02:56:37.320: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 02:56:37.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2046" for this suite. 08/25/22 02:56:37.325
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:37.33
Aug 25 02:56:37.330: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir-wrapper 08/25/22 02:56:37.332
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:37.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:37.347
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 25 02:56:37.369: INFO: Waiting up to 5m0s for pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb" in namespace "emptydir-wrapper-45" to be "running and ready"
Aug 25 02:56:37.372: INFO: Pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.141191ms
Aug 25 02:56:37.372: INFO: The phase of Pod pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 02:56:39.378: INFO: Pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008475959s
Aug 25 02:56:39.378: INFO: The phase of Pod pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb is Running (Ready = true)
Aug 25 02:56:39.378: INFO: Pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/25/22 02:56:39.381
STEP: Cleaning up the configmap 08/25/22 02:56:39.385
STEP: Cleaning up the pod 08/25/22 02:56:39.391
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Aug 25 02:56:39.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-45" for this suite. 08/25/22 02:56:39.402
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":20,"skipped":239,"failed":0}
------------------------------
• [2.076 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:37.33
    Aug 25 02:56:37.330: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir-wrapper 08/25/22 02:56:37.332
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:37.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:37.347
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 25 02:56:37.369: INFO: Waiting up to 5m0s for pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb" in namespace "emptydir-wrapper-45" to be "running and ready"
    Aug 25 02:56:37.372: INFO: Pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.141191ms
    Aug 25 02:56:37.372: INFO: The phase of Pod pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 02:56:39.378: INFO: Pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008475959s
    Aug 25 02:56:39.378: INFO: The phase of Pod pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb is Running (Ready = true)
    Aug 25 02:56:39.378: INFO: Pod "pod-secrets-c8f48b18-590a-4723-be56-1e169dfe75eb" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/25/22 02:56:39.381
    STEP: Cleaning up the configmap 08/25/22 02:56:39.385
    STEP: Cleaning up the pod 08/25/22 02:56:39.391
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Aug 25 02:56:39.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-45" for this suite. 08/25/22 02:56:39.402
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:39.409
Aug 25 02:56:39.409: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename containers 08/25/22 02:56:39.411
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:39.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:39.426
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 08/25/22 02:56:39.429
Aug 25 02:56:39.435: INFO: Waiting up to 5m0s for pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251" in namespace "containers-3529" to be "Succeeded or Failed"
Aug 25 02:56:39.438: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692151ms
Aug 25 02:56:41.442: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007510801s
Aug 25 02:56:43.444: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008917266s
STEP: Saw pod success 08/25/22 02:56:43.444
Aug 25 02:56:43.444: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251" satisfied condition "Succeeded or Failed"
Aug 25 02:56:43.448: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-containers-f825c384-fa10-4b99-a047-aeb12c537251 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 02:56:43.453
Aug 25 02:56:43.459: INFO: Waiting for pod client-containers-f825c384-fa10-4b99-a047-aeb12c537251 to disappear
Aug 25 02:56:43.463: INFO: Pod client-containers-f825c384-fa10-4b99-a047-aeb12c537251 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 25 02:56:43.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3529" for this suite. 08/25/22 02:56:43.467
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":21,"skipped":268,"failed":0}
------------------------------
• [4.064 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:39.409
    Aug 25 02:56:39.409: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename containers 08/25/22 02:56:39.411
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:39.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:39.426
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 08/25/22 02:56:39.429
    Aug 25 02:56:39.435: INFO: Waiting up to 5m0s for pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251" in namespace "containers-3529" to be "Succeeded or Failed"
    Aug 25 02:56:39.438: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692151ms
    Aug 25 02:56:41.442: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007510801s
    Aug 25 02:56:43.444: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008917266s
    STEP: Saw pod success 08/25/22 02:56:43.444
    Aug 25 02:56:43.444: INFO: Pod "client-containers-f825c384-fa10-4b99-a047-aeb12c537251" satisfied condition "Succeeded or Failed"
    Aug 25 02:56:43.448: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-containers-f825c384-fa10-4b99-a047-aeb12c537251 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 02:56:43.453
    Aug 25 02:56:43.459: INFO: Waiting for pod client-containers-f825c384-fa10-4b99-a047-aeb12c537251 to disappear
    Aug 25 02:56:43.463: INFO: Pod client-containers-f825c384-fa10-4b99-a047-aeb12c537251 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 25 02:56:43.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3529" for this suite. 08/25/22 02:56:43.467
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:56:43.474
Aug 25 02:56:43.474: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename taint-multiple-pods 08/25/22 02:56:43.475
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:43.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:43.49
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Aug 25 02:56:43.494: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 25 02:57:43.587: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Aug 25 02:57:43.593: INFO: Starting informer...
STEP: Starting pods... 08/25/22 02:57:43.593
Aug 25 02:57:43.807: INFO: Pod1 is running on bobymcbobs-c849-control-plane-p55pp. Tainting Node
Aug 25 02:57:44.019: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-195" to be "running"
Aug 25 02:57:44.023: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954065ms
Aug 25 02:57:46.029: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010207108s
Aug 25 02:57:46.029: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 25 02:57:46.029: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-195" to be "running"
Aug 25 02:57:46.034: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.185239ms
Aug 25 02:57:46.034: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 25 02:57:46.034: INFO: Pod2 is running on bobymcbobs-c849-control-plane-p55pp. Tainting Node
STEP: Trying to apply a taint on the Node 08/25/22 02:57:46.034
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 02:57:46.05
STEP: Waiting for Pod1 and Pod2 to be deleted 08/25/22 02:57:46.055
Aug 25 02:58:19.271: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 25 02:59:36.056: FAIL: Failed to evict all Pods. 1 pod(s) is not evicted.

Full Stack Trace

STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 02:59:36.06
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Collecting events from namespace "taint-multiple-pods-195". 08/25/22 02:59:36.065
STEP: Found 11 events. 08/25/22 02:59:36.069
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:43 +0000 UTC - event for taint-eviction-b1: {default-scheduler } Scheduled: Successfully assigned taint-multiple-pods-195/taint-eviction-b1 to bobymcbobs-c849-control-plane-p55pp
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:43 +0000 UTC - event for taint-eviction-b2: {default-scheduler } Scheduled: Successfully assigned taint-multiple-pods-195/taint-eviction-b2 to bobymcbobs-c849-control-plane-p55pp
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Pulled: Container image "registry.k8s.io/pause:3.8" already present on machine
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Created: Created container pause
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Started: Started container pause
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b2: {kubelet bobymcbobs-c849-control-plane-p55pp} Pulled: Container image "registry.k8s.io/pause:3.8" already present on machine
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b2: {kubelet bobymcbobs-c849-control-plane-p55pp} Created: Created container pause
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b2: {kubelet bobymcbobs-c849-control-plane-p55pp} Started: Started container pause
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:51 +0000 UTC - event for taint-eviction-b1: {taint-controller } TaintManagerEviction: Marking for deletion Pod taint-multiple-pods-195/taint-eviction-b1
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:51 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Killing: Stopping container pause
Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:59 +0000 UTC - event for taint-eviction-b2: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod taint-multiple-pods-195/taint-eviction-b2
Aug 25 02:59:36.072: INFO: POD                NODE                                 PHASE    GRACE  CONDITIONS
Aug 25 02:59:36.072: INFO: taint-eviction-b2  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:43 +0000 UTC  }]
Aug 25 02:59:36.072: INFO: 
Aug 25 02:59:36.094: INFO: 
Logging node info for node bobymcbobs-c849-control-plane-p55pp
Aug 25 02:59:36.099: INFO: Node Info: &Node{ObjectMeta:{bobymcbobs-c849-control-plane-p55pp    c5dc1f57-bb1e-4562-b424-f31ab182c240 8638 0 2022-08-25 02:40:28 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:bobymcbobs-c849-control-plane-p55pp kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[cluster.x-k8s.io/cluster-name:bobymcbobs-c849 cluster.x-k8s.io/cluster-namespace:sharingio-pair cluster.x-k8s.io/machine:bobymcbobs-c849-control-plane-fmvtt cluster.x-k8s.io/owner-kind:KubeadmControlPlane cluster.x-k8s.io/owner-name:bobymcbobs-c849-control-plane kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2022-08-25 02:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}}} } {kubeadm Update v1 2022-08-25 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2022-08-25 02:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}} } {kube-utils Update v1 2022-08-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:spec":{"f:providerID":{}}} } {manager Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cluster.x-k8s.io/cluster-name":{},"f:cluster.x-k8s.io/cluster-namespace":{},"f:cluster.x-k8s.io/machine":{},"f:cluster.x-k8s.io/owner-kind":{},"f:cluster.x-k8s.io/owner-name":{}}}} } {kubelet Update v1 2022-08-25 02:56:41 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:ephemeral-storage":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{}}} status}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{234139500544 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404313366528 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{210725550141 0} {<nil>} 210725550141 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404208508928 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-08-25 02:40:57 +0000 UTC,LastTransitionTime:2022-08-25 02:40:57 +0000 UTC,Reason:WeaveIsUp,Message:Weave pod has set this,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:52 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:86.109.11.217,},NodeAddress{Type:Hostname,Address:bobymcbobs-c849-control-plane-p55pp,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4b0c7bda23ca4aa6a943cda55f9fa1e2,SystemUUID:4c4c4544-0053-4710-8038-b8c04f483033,BootID:217f066f-7295-498b-9b6e-db50783a6a97,KernelVersion:5.4.0-109-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:containerd://1.6.7,KubeletVersion:v1.25.0,KubeProxyVersion:v1.25.0,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.gitlab.com/sharingio/environment/environment@sha256:37f277951643b2b0e70c1a7c72b1fa03fd7adca706c6e10202ac6a9ea81d1a45 registry.gitlab.com/sharingio/environment/environment:2022.07.25.1600],SizeBytes:2667236715,},ContainerImage{Names:[registry.gitlab.com/ii/nz/reveal-multiplex@sha256:299c977cc6734bf88fa5bdc22f8f6d464f0435437baee92aef5b33534d5ccf1f registry.gitlab.com/ii/nz/reveal-multiplex:latest],SizeBytes:216223066,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:6f72b851544986cb0921b53ea655ec04c36131248f16d4ad110cb3ca0c369dc1 registry.k8s.io/etcd:3.5.4-0],SizeBytes:102157811,},ContainerImage{Names:[docker.io/linuxserver/mariadb@sha256:bceed8b46f875fd8c5fcf60d9201e1e8093f8ab181234d2bf3824c6621b897b6 docker.io/linuxserver/mariadb:alpine-version-10.5.12-r0],SizeBytes:89567136,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:8b45c5f7ab38f1ae9847e81f91aafa78d9905ae37cdae6fe3edaa43f3e6dfdfa registry.k8s.io/conformance:v1.25.0],SizeBytes:78887805,},ContainerImage{Names:[docker.io/fluxcd/helm-operator@sha256:8e63dcdbe0ce672215e1946bd64c88a362194dad901420632f529a295c2b8068 docker.io/fluxcd/helm-operator:1.4.0],SizeBytes:77258104,},ContainerImage{Names:[docker.io/envoyproxy/envoy@sha256:1f343072a58e74644b7adc8d2d877071f846fc77166295a6d2686aee6cf58162 docker.io/envoyproxy/envoy:v1.22.2],SizeBytes:51349232,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[quay.io/metallb/speaker@sha256:58cb99053bb33db1944f8b67c9d3335afe98c8ba810fca45f19cfde1442064c4 quay.io/metallb/speaker:v0.10.2],SizeBytes:39339636,},ContainerImage{Names:[docker.io/pschiffe/pdns-mysql@sha256:f9cabc76207919fcd82bcc1243df2ca510f89e61f6f5d2e2f54541e2b7d65d24 docker.io/pschiffe/pdns-mysql:4.3-alpine],SizeBytes:36903432,},ContainerImage{Names:[quay.io/metallb/controller@sha256:21164241c045443595439f2821a193ef823dbbd1ccc8959e1c363c44cc6d1e18 quay.io/metallb/controller:v0.10.2],SizeBytes:36020654,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:f6902791fb9aa6e283ed7d1d743417b3c425eec73151517813bef1539a66aefa registry.k8s.io/kube-apiserver:v1.25.0],SizeBytes:34227200,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:66ce7d460e53f942bb4729f656d66fe475ec3d41728de986b6d790eee6d8205d registry.k8s.io/kube-controller-manager:v1.25.0],SizeBytes:31259842,},ContainerImage{Names:[docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63 docker.io/weaveworks/weave-kube:2.8.1],SizeBytes:30924173,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[docker.io/appscode/kubed@sha256:d694910be47b07f941e44cf60514aa5284944b1271a456e51c45208fae296ee9 docker.io/appscode/kubed:v0.12.0],SizeBytes:25358728,},ContainerImage{Names:[k8s.gcr.io/external-dns/external-dns@sha256:d9569d17d224513f252520d3c92d6519aedc40524f4395a9185c7e66de3cdab7 k8s.gcr.io/external-dns/external-dns:v0.10.0],SizeBytes:21974457,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:1b1f3456bb19866aa1655c607514b85cd2b6efdfea4d93ea55e79475ff2765f9 registry.k8s.io/kube-proxy:v1.25.0],SizeBytes:20262263,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c],SizeBytes:18081586,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/operator/cmd/operator@sha256:2cbbbe9a7873fc9414a09c3ed558dde13a8105e70e4c83300e3a0a820534897f],SizeBytes:17654221,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:c2ac18db63de4982496e6c8650e20cecf4dd32172516b5934d051c7c084063da docker.io/sonobuoy/sonobuoy:v0.56.10],SizeBytes:17407841,},ContainerImage{Names:[quay.io/jetstack/cert-manager-controller@sha256:51027a4cc4d30e197e3506daf3a4fa2d2a0bc2826469f8a87848dfd279e031c0 quay.io/jetstack/cert-manager-controller:v1.7.1],SizeBytes:17112219,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2],SizeBytes:16259613,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7],SizeBytes:16056815,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:5526e650784165aa2ed2af1846e0e8bf37d5c52815ad6865bbe5d99d78eca32e],SizeBytes:16004457,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b],SizeBytes:15960524,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb],SizeBytes:15874009,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/controller@sha256:08409982c2dda0992cc16c88a9f16dab4efff54a29deb3072617fec6efcef6c4],SizeBytes:15830636,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:9330c53feca7b51b25e427fa96afd5d1460b3233e9fa92e20c895c067da56ac1 registry.k8s.io/kube-scheduler:v1.25.0],SizeBytes:15793363,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-contour/cmd/controller@sha256:baf500920abd51e33c7497799f5b241560783ad9a3bb6fec746770eb734d488a],SizeBytes:15760951,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0],SizeBytes:15432856,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/webhook@sha256:31da8a42d9f28999bdeeea4a0a312e0e6c52ab1f18c97c88a7e2c27627482835],SizeBytes:15312462,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a registry.k8s.io/coredns/coredns:v1.9.3],SizeBytes:14837849,},ContainerImage{Names:[quay.io/jetstack/cert-manager-webhook@sha256:a926d60b6f23553ca5d11ac9cd66bcc692136e838613c8bc0d60c6c35a3cbcfc quay.io/jetstack/cert-manager-webhook:v1.7.1],SizeBytes:13636913,},ContainerImage{Names:[docker.io/rancher/local-path-provisioner@sha256:9666b1635fec95d4e2251661e135c90678b8f45fd0f8324c55db99c80e2a958c docker.io/rancher/local-path-provisioner:v0.0.19],SizeBytes:13585626,},ContainerImage{Names:[ghcr.io/projectcontour/contour@sha256:c64e1f72fad4b495d6e0967c2b3063c06ba2d79c9098331ba0219094204023c7 ghcr.io/projectcontour/contour:v1.21.1],SizeBytes:13407229,},ContainerImage{Names:[docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c docker.io/weaveworks/weave-npc:2.8.1],SizeBytes:12814131,},ContainerImage{Names:[quay.io/jetstack/cert-manager-cainjector@sha256:985743eeed2b62f68ee06e583f1d5a371e1c35af4b1980a1b2571d29174cce47 quay.io/jetstack/cert-manager-cainjector:v1.7.1],SizeBytes:12094880,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/pkg/apiextensions/storageversion/cmd/migrate@sha256:b333601675dc5f8349b3133bd9fcc7a620db0a5076892ea00338b19fc1e073fb],SizeBytes:11564327,},ContainerImage{Names:[docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96 docker.io/library/registry:2.8.1],SizeBytes:9204128,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exposer@sha256:d5234c7aee6d1281a72b4b1f09124a662ca230ece854def9449c988390f29897 registry.gitlab.com/sharingio/environment/exposer:2022.07.25.1600],SizeBytes:8641265,},ContainerImage{Names:[registry.gitlab.com/safesurfer/go-http-server@sha256:5a092b5372f23ec18117411b63ffd60f73dce967204087b24cc4b65b97e2c0b4 registry.gitlab.com/safesurfer/go-http-server:1.6.0],SizeBytes:4512154,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exporter@sha256:d788024998ccce25d636debb5bdee4c489ba511f51b4298cb1be0da6f2feac5e registry.gitlab.com/sharingio/environment/exporter:2022.07.25.1600],SizeBytes:2441493,},ContainerImage{Names:[docker.io/library/busybox@sha256:ef320ff10026a50cf5f0213d35537ce0041ac1d96e9b7800bafd8bc9eff6c693 docker.io/library/busybox:latest],SizeBytes:777541,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf registry.k8s.io/e2e-test-images/busybox:1.29-2],SizeBytes:732424,},ContainerImage{Names:[registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d registry.k8s.io/pause:3.8],SizeBytes:311286,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db k8s.gcr.io/pause:3.6],SizeBytes:301773,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Aug 25 02:59:36.099: INFO: 
Logging kubelet events for node bobymcbobs-c849-control-plane-p55pp
Aug 25 02:59:36.104: INFO: 
Logging pods the kubelet thinks is on node bobymcbobs-c849-control-plane-p55pp
Aug 25 02:59:36.134: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 02:59:36.134: INFO: net-contour-controller-5c5fd89fdb-6mvbj started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
Aug 25 02:59:36.134: INFO: reveal-multiplex-6bbbf59d6-lh5vq started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 02:59:36.134: INFO: environment-0 started at 2022-08-25 02:58:14 +0000 UTC (0+2 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container environment ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 02:59:36.134: INFO: local-path-provisioner-56c55476f9-5hg9n started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 02:59:36.134: INFO: powerdns-5c6dbcf579-k5l4c started at 2022-08-25 02:57:59 +0000 UTC (1+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Init container powerdns-init-db ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 02:59:36.134: INFO: environment-exposer-5bb6d44465-cvdbh started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 02:59:36.134: INFO: activator-88b7df5c-gjfj9 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container activator ready: true, restart count 0
Aug 25 02:59:36.134: INFO: controller-77cc7ff558-n8vft started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
Aug 25 02:59:36.134: INFO: contour-588fc6cc6d-jspht started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
Aug 25 02:59:36.134: INFO: metrics-server-5cb46dccf6-swrff started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 02:59:36.134: INFO: distribution-7f9dff85c4-tjxf8 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container distribution ready: true, restart count 0
Aug 25 02:59:36.134: INFO: webhook-69d55f5549-chd9c started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container webhook ready: true, restart count 0
Aug 25 02:59:36.134: INFO: helm-operator-7959478576-dx96s started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 02:59:36.134: INFO: speaker-ghnxf started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container speaker ready: true, restart count 0
Aug 25 02:59:36.134: INFO: autoscaler-7bf8ff94db-2pl8r started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 02:59:36.134: INFO: cert-manager-66bd77df8f-ndf6l started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 02:59:36.134: INFO: contour-8597b78798-rvx58 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
Aug 25 02:59:36.134: INFO: taint-eviction-b2 started at 2022-08-25 02:57:43 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container pause ready: true, restart count 0
Aug 25 02:59:36.134: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 25 02:59:36.134: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 02:59:36.134: INFO: kube-proxy-b4lgh started at 2022-08-25 02:40:45 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 02:59:36.134: INFO: kubed-568bdbc6c4-5vhn9 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container kubed ready: true, restart count 0
Aug 25 02:59:36.134: INFO: contour-8597b78798-6xc8c started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
Aug 25 02:59:36.134: INFO: public-html-go-http-server-55b9f584d4-9dvd8 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 02:59:36.134: INFO: domain-mapping-5bcd85fbc6-ksvjk started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 02:59:36.134: INFO: autoscaler-hpa-776c44cc57-2rrjv started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 02:59:36.134: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 02:59:36.134: INFO: knative-operator-594876444d-ns75x started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 02:59:36.134: INFO: controller-7fd644cbc6-558wp started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
Aug 25 02:59:36.134: INFO: contour-588fc6cc6d-9gf27 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
Aug 25 02:59:36.134: INFO: domainmapping-webhook-77f466bbc9-g29sv started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 02:59:36.134: INFO: coredns-565d847f94-hv8kn started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container coredns ready: true, restart count 0
Aug 25 02:59:36.134: INFO: cert-manager-cainjector-6495667ff4-j7vbr started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 02:59:36.134: INFO: external-dns-67d79cbcd4-2t9mm started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 02:59:36.134: INFO: etcd-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container etcd ready: true, restart count 0
Aug 25 02:59:36.134: INFO: sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 02:59:36.134: INFO: sonobuoy-e2e-job-39386ef5d0694a49 started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container e2e ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 02:59:36.134: INFO: cert-manager-webhook-59d6cdfb6f-b6qpv started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 02:59:36.134: INFO: envoy-9vl8n started at 2022-08-25 02:58:47 +0000 UTC (1+2 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Init container envoy-initconfig ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container envoy ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 02:59:36.134: INFO: coredns-565d847f94-2jsfd started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container coredns ready: true, restart count 0
Aug 25 02:59:36.134: INFO: powerdns-db-7c897fddf4-899c6 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 02:59:36.134: INFO: storage-version-migration-serving-serving-1.6.0-vw27x started at 2022-08-25 02:58:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container migrate ready: false, restart count 0
Aug 25 02:59:36.134: INFO: envoy-w6h8q started at 2022-08-25 02:58:47 +0000 UTC (1+2 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Init container envoy-initconfig ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container envoy ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 02:59:36.134: INFO: weave-net-75fql started at 2022-08-25 02:40:45 +0000 UTC (1+2 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Init container weave-init ready: true, restart count 0
Aug 25 02:59:36.134: INFO: 	Container weave ready: true, restart count 1
Aug 25 02:59:36.134: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 02:59:36.134: INFO: net-certmanager-controller-6c8cb88879-4jbnf started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
Aug 25 02:59:36.134: INFO: net-certmanager-webhook-79cfb96f68-5nxmb started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 02:59:36.134: INFO: 	Container webhook ready: true, restart count 0
Aug 25 02:59:36.241: INFO: 
Latency metrics for node bobymcbobs-c849-control-plane-p55pp
Aug 25 02:59:36.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-195" for this suite. 08/25/22 02:59:36.246
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"FAILED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":21,"skipped":275,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [FAILED] [172.777 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:56:43.474
    Aug 25 02:56:43.474: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename taint-multiple-pods 08/25/22 02:56:43.475
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:56:43.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:56:43.49
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Aug 25 02:56:43.494: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 25 02:57:43.587: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Aug 25 02:57:43.593: INFO: Starting informer...
    STEP: Starting pods... 08/25/22 02:57:43.593
    Aug 25 02:57:43.807: INFO: Pod1 is running on bobymcbobs-c849-control-plane-p55pp. Tainting Node
    Aug 25 02:57:44.019: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-195" to be "running"
    Aug 25 02:57:44.023: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954065ms
    Aug 25 02:57:46.029: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010207108s
    Aug 25 02:57:46.029: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 25 02:57:46.029: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-195" to be "running"
    Aug 25 02:57:46.034: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.185239ms
    Aug 25 02:57:46.034: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 25 02:57:46.034: INFO: Pod2 is running on bobymcbobs-c849-control-plane-p55pp. Tainting Node
    STEP: Trying to apply a taint on the Node 08/25/22 02:57:46.034
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 02:57:46.05
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/25/22 02:57:46.055
    Aug 25 02:58:19.271: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 25 02:59:36.056: FAIL: Failed to evict all Pods. 1 pod(s) is not evicted.

    Full Stack Trace

    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 02:59:36.06
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    STEP: Collecting events from namespace "taint-multiple-pods-195". 08/25/22 02:59:36.065
    STEP: Found 11 events. 08/25/22 02:59:36.069
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:43 +0000 UTC - event for taint-eviction-b1: {default-scheduler } Scheduled: Successfully assigned taint-multiple-pods-195/taint-eviction-b1 to bobymcbobs-c849-control-plane-p55pp
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:43 +0000 UTC - event for taint-eviction-b2: {default-scheduler } Scheduled: Successfully assigned taint-multiple-pods-195/taint-eviction-b2 to bobymcbobs-c849-control-plane-p55pp
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Pulled: Container image "registry.k8s.io/pause:3.8" already present on machine
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Created: Created container pause
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Started: Started container pause
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b2: {kubelet bobymcbobs-c849-control-plane-p55pp} Pulled: Container image "registry.k8s.io/pause:3.8" already present on machine
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b2: {kubelet bobymcbobs-c849-control-plane-p55pp} Created: Created container pause
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:44 +0000 UTC - event for taint-eviction-b2: {kubelet bobymcbobs-c849-control-plane-p55pp} Started: Started container pause
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:51 +0000 UTC - event for taint-eviction-b1: {taint-controller } TaintManagerEviction: Marking for deletion Pod taint-multiple-pods-195/taint-eviction-b1
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:51 +0000 UTC - event for taint-eviction-b1: {kubelet bobymcbobs-c849-control-plane-p55pp} Killing: Stopping container pause
    Aug 25 02:59:36.069: INFO: At 2022-08-25 02:57:59 +0000 UTC - event for taint-eviction-b2: {taint-controller } TaintManagerEviction: Cancelling deletion of Pod taint-multiple-pods-195/taint-eviction-b2
    Aug 25 02:59:36.072: INFO: POD                NODE                                 PHASE    GRACE  CONDITIONS
    Aug 25 02:59:36.072: INFO: taint-eviction-b2  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 02:57:43 +0000 UTC  }]
    Aug 25 02:59:36.072: INFO: 
    Aug 25 02:59:36.094: INFO: 
    Logging node info for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 02:59:36.099: INFO: Node Info: &Node{ObjectMeta:{bobymcbobs-c849-control-plane-p55pp    c5dc1f57-bb1e-4562-b424-f31ab182c240 8638 0 2022-08-25 02:40:28 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:bobymcbobs-c849-control-plane-p55pp kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[cluster.x-k8s.io/cluster-name:bobymcbobs-c849 cluster.x-k8s.io/cluster-namespace:sharingio-pair cluster.x-k8s.io/machine:bobymcbobs-c849-control-plane-fmvtt cluster.x-k8s.io/owner-kind:KubeadmControlPlane cluster.x-k8s.io/owner-name:bobymcbobs-c849-control-plane kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2022-08-25 02:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}}} } {kubeadm Update v1 2022-08-25 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2022-08-25 02:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}} } {kube-utils Update v1 2022-08-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:spec":{"f:providerID":{}}} } {manager Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cluster.x-k8s.io/cluster-name":{},"f:cluster.x-k8s.io/cluster-namespace":{},"f:cluster.x-k8s.io/machine":{},"f:cluster.x-k8s.io/owner-kind":{},"f:cluster.x-k8s.io/owner-name":{}}}} } {kubelet Update v1 2022-08-25 02:56:41 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:ephemeral-storage":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{}}} status}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{234139500544 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404313366528 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{210725550141 0} {<nil>} 210725550141 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404208508928 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-08-25 02:40:57 +0000 UTC,LastTransitionTime:2022-08-25 02:40:57 +0000 UTC,Reason:WeaveIsUp,Message:Weave pod has set this,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-08-25 02:56:41 +0000 UTC,LastTransitionTime:2022-08-25 02:40:52 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:86.109.11.217,},NodeAddress{Type:Hostname,Address:bobymcbobs-c849-control-plane-p55pp,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4b0c7bda23ca4aa6a943cda55f9fa1e2,SystemUUID:4c4c4544-0053-4710-8038-b8c04f483033,BootID:217f066f-7295-498b-9b6e-db50783a6a97,KernelVersion:5.4.0-109-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:containerd://1.6.7,KubeletVersion:v1.25.0,KubeProxyVersion:v1.25.0,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.gitlab.com/sharingio/environment/environment@sha256:37f277951643b2b0e70c1a7c72b1fa03fd7adca706c6e10202ac6a9ea81d1a45 registry.gitlab.com/sharingio/environment/environment:2022.07.25.1600],SizeBytes:2667236715,},ContainerImage{Names:[registry.gitlab.com/ii/nz/reveal-multiplex@sha256:299c977cc6734bf88fa5bdc22f8f6d464f0435437baee92aef5b33534d5ccf1f registry.gitlab.com/ii/nz/reveal-multiplex:latest],SizeBytes:216223066,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:6f72b851544986cb0921b53ea655ec04c36131248f16d4ad110cb3ca0c369dc1 registry.k8s.io/etcd:3.5.4-0],SizeBytes:102157811,},ContainerImage{Names:[docker.io/linuxserver/mariadb@sha256:bceed8b46f875fd8c5fcf60d9201e1e8093f8ab181234d2bf3824c6621b897b6 docker.io/linuxserver/mariadb:alpine-version-10.5.12-r0],SizeBytes:89567136,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:8b45c5f7ab38f1ae9847e81f91aafa78d9905ae37cdae6fe3edaa43f3e6dfdfa registry.k8s.io/conformance:v1.25.0],SizeBytes:78887805,},ContainerImage{Names:[docker.io/fluxcd/helm-operator@sha256:8e63dcdbe0ce672215e1946bd64c88a362194dad901420632f529a295c2b8068 docker.io/fluxcd/helm-operator:1.4.0],SizeBytes:77258104,},ContainerImage{Names:[docker.io/envoyproxy/envoy@sha256:1f343072a58e74644b7adc8d2d877071f846fc77166295a6d2686aee6cf58162 docker.io/envoyproxy/envoy:v1.22.2],SizeBytes:51349232,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[quay.io/metallb/speaker@sha256:58cb99053bb33db1944f8b67c9d3335afe98c8ba810fca45f19cfde1442064c4 quay.io/metallb/speaker:v0.10.2],SizeBytes:39339636,},ContainerImage{Names:[docker.io/pschiffe/pdns-mysql@sha256:f9cabc76207919fcd82bcc1243df2ca510f89e61f6f5d2e2f54541e2b7d65d24 docker.io/pschiffe/pdns-mysql:4.3-alpine],SizeBytes:36903432,},ContainerImage{Names:[quay.io/metallb/controller@sha256:21164241c045443595439f2821a193ef823dbbd1ccc8959e1c363c44cc6d1e18 quay.io/metallb/controller:v0.10.2],SizeBytes:36020654,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:f6902791fb9aa6e283ed7d1d743417b3c425eec73151517813bef1539a66aefa registry.k8s.io/kube-apiserver:v1.25.0],SizeBytes:34227200,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:66ce7d460e53f942bb4729f656d66fe475ec3d41728de986b6d790eee6d8205d registry.k8s.io/kube-controller-manager:v1.25.0],SizeBytes:31259842,},ContainerImage{Names:[docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63 docker.io/weaveworks/weave-kube:2.8.1],SizeBytes:30924173,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[docker.io/appscode/kubed@sha256:d694910be47b07f941e44cf60514aa5284944b1271a456e51c45208fae296ee9 docker.io/appscode/kubed:v0.12.0],SizeBytes:25358728,},ContainerImage{Names:[k8s.gcr.io/external-dns/external-dns@sha256:d9569d17d224513f252520d3c92d6519aedc40524f4395a9185c7e66de3cdab7 k8s.gcr.io/external-dns/external-dns:v0.10.0],SizeBytes:21974457,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:1b1f3456bb19866aa1655c607514b85cd2b6efdfea4d93ea55e79475ff2765f9 registry.k8s.io/kube-proxy:v1.25.0],SizeBytes:20262263,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c],SizeBytes:18081586,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/operator/cmd/operator@sha256:2cbbbe9a7873fc9414a09c3ed558dde13a8105e70e4c83300e3a0a820534897f],SizeBytes:17654221,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:c2ac18db63de4982496e6c8650e20cecf4dd32172516b5934d051c7c084063da docker.io/sonobuoy/sonobuoy:v0.56.10],SizeBytes:17407841,},ContainerImage{Names:[quay.io/jetstack/cert-manager-controller@sha256:51027a4cc4d30e197e3506daf3a4fa2d2a0bc2826469f8a87848dfd279e031c0 quay.io/jetstack/cert-manager-controller:v1.7.1],SizeBytes:17112219,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2],SizeBytes:16259613,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7],SizeBytes:16056815,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:5526e650784165aa2ed2af1846e0e8bf37d5c52815ad6865bbe5d99d78eca32e],SizeBytes:16004457,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b],SizeBytes:15960524,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb],SizeBytes:15874009,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/controller@sha256:08409982c2dda0992cc16c88a9f16dab4efff54a29deb3072617fec6efcef6c4],SizeBytes:15830636,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:9330c53feca7b51b25e427fa96afd5d1460b3233e9fa92e20c895c067da56ac1 registry.k8s.io/kube-scheduler:v1.25.0],SizeBytes:15793363,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-contour/cmd/controller@sha256:baf500920abd51e33c7497799f5b241560783ad9a3bb6fec746770eb734d488a],SizeBytes:15760951,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0],SizeBytes:15432856,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/webhook@sha256:31da8a42d9f28999bdeeea4a0a312e0e6c52ab1f18c97c88a7e2c27627482835],SizeBytes:15312462,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a registry.k8s.io/coredns/coredns:v1.9.3],SizeBytes:14837849,},ContainerImage{Names:[quay.io/jetstack/cert-manager-webhook@sha256:a926d60b6f23553ca5d11ac9cd66bcc692136e838613c8bc0d60c6c35a3cbcfc quay.io/jetstack/cert-manager-webhook:v1.7.1],SizeBytes:13636913,},ContainerImage{Names:[docker.io/rancher/local-path-provisioner@sha256:9666b1635fec95d4e2251661e135c90678b8f45fd0f8324c55db99c80e2a958c docker.io/rancher/local-path-provisioner:v0.0.19],SizeBytes:13585626,},ContainerImage{Names:[ghcr.io/projectcontour/contour@sha256:c64e1f72fad4b495d6e0967c2b3063c06ba2d79c9098331ba0219094204023c7 ghcr.io/projectcontour/contour:v1.21.1],SizeBytes:13407229,},ContainerImage{Names:[docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c docker.io/weaveworks/weave-npc:2.8.1],SizeBytes:12814131,},ContainerImage{Names:[quay.io/jetstack/cert-manager-cainjector@sha256:985743eeed2b62f68ee06e583f1d5a371e1c35af4b1980a1b2571d29174cce47 quay.io/jetstack/cert-manager-cainjector:v1.7.1],SizeBytes:12094880,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/pkg/apiextensions/storageversion/cmd/migrate@sha256:b333601675dc5f8349b3133bd9fcc7a620db0a5076892ea00338b19fc1e073fb],SizeBytes:11564327,},ContainerImage{Names:[docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96 docker.io/library/registry:2.8.1],SizeBytes:9204128,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exposer@sha256:d5234c7aee6d1281a72b4b1f09124a662ca230ece854def9449c988390f29897 registry.gitlab.com/sharingio/environment/exposer:2022.07.25.1600],SizeBytes:8641265,},ContainerImage{Names:[registry.gitlab.com/safesurfer/go-http-server@sha256:5a092b5372f23ec18117411b63ffd60f73dce967204087b24cc4b65b97e2c0b4 registry.gitlab.com/safesurfer/go-http-server:1.6.0],SizeBytes:4512154,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exporter@sha256:d788024998ccce25d636debb5bdee4c489ba511f51b4298cb1be0da6f2feac5e registry.gitlab.com/sharingio/environment/exporter:2022.07.25.1600],SizeBytes:2441493,},ContainerImage{Names:[docker.io/library/busybox@sha256:ef320ff10026a50cf5f0213d35537ce0041ac1d96e9b7800bafd8bc9eff6c693 docker.io/library/busybox:latest],SizeBytes:777541,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf registry.k8s.io/e2e-test-images/busybox:1.29-2],SizeBytes:732424,},ContainerImage{Names:[registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d registry.k8s.io/pause:3.8],SizeBytes:311286,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db k8s.gcr.io/pause:3.6],SizeBytes:301773,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
    Aug 25 02:59:36.099: INFO: 
    Logging kubelet events for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 02:59:36.104: INFO: 
    Logging pods the kubelet thinks is on node bobymcbobs-c849-control-plane-p55pp
    Aug 25 02:59:36.134: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: net-contour-controller-5c5fd89fdb-6mvbj started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: reveal-multiplex-6bbbf59d6-lh5vq started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: environment-0 started at 2022-08-25 02:58:14 +0000 UTC (0+2 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container environment ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: local-path-provisioner-56c55476f9-5hg9n started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: powerdns-5c6dbcf579-k5l4c started at 2022-08-25 02:57:59 +0000 UTC (1+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Init container powerdns-init-db ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: environment-exposer-5bb6d44465-cvdbh started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: activator-88b7df5c-gjfj9 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container activator ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: controller-77cc7ff558-n8vft started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: contour-588fc6cc6d-jspht started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: metrics-server-5cb46dccf6-swrff started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: distribution-7f9dff85c4-tjxf8 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: webhook-69d55f5549-chd9c started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: helm-operator-7959478576-dx96s started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: speaker-ghnxf started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: autoscaler-7bf8ff94db-2pl8r started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: cert-manager-66bd77df8f-ndf6l started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: contour-8597b78798-rvx58 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: taint-eviction-b2 started at 2022-08-25 02:57:43 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container pause ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: kube-proxy-b4lgh started at 2022-08-25 02:40:45 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: kubed-568bdbc6c4-5vhn9 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: contour-8597b78798-6xc8c started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: public-html-go-http-server-55b9f584d4-9dvd8 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: domain-mapping-5bcd85fbc6-ksvjk started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: autoscaler-hpa-776c44cc57-2rrjv started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: knative-operator-594876444d-ns75x started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: controller-7fd644cbc6-558wp started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: contour-588fc6cc6d-9gf27 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container contour ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: domainmapping-webhook-77f466bbc9-g29sv started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: coredns-565d847f94-hv8kn started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: cert-manager-cainjector-6495667ff4-j7vbr started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: external-dns-67d79cbcd4-2t9mm started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: etcd-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: sonobuoy-e2e-job-39386ef5d0694a49 started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: cert-manager-webhook-59d6cdfb6f-b6qpv started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: envoy-9vl8n started at 2022-08-25 02:58:47 +0000 UTC (1+2 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Init container envoy-initconfig ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: coredns-565d847f94-2jsfd started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: powerdns-db-7c897fddf4-899c6 started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: storage-version-migration-serving-serving-1.6.0-vw27x started at 2022-08-25 02:58:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container migrate ready: false, restart count 0
    Aug 25 02:59:36.134: INFO: envoy-w6h8q started at 2022-08-25 02:58:47 +0000 UTC (1+2 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Init container envoy-initconfig ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: weave-net-75fql started at 2022-08-25 02:40:45 +0000 UTC (1+2 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Init container weave-init ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: 	Container weave ready: true, restart count 1
    Aug 25 02:59:36.134: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: net-certmanager-controller-6c8cb88879-4jbnf started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container controller ready: true, restart count 0
    Aug 25 02:59:36.134: INFO: net-certmanager-webhook-79cfb96f68-5nxmb started at 2022-08-25 02:57:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 02:59:36.134: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 02:59:36.241: INFO: 
    Latency metrics for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 02:59:36.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-195" for this suite. 08/25/22 02:59:36.246
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output

  Aug 25 02:59:36.056: Failed to evict all Pods. 1 pod(s) is not evicted.
  In [It] at: vendor/github.com/onsi/ginkgo/v2/internal/suite.go:596
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 02:59:36.253
Aug 25 02:59:36.253: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 02:59:36.253
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:59:36.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:59:36.27
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/25/22 02:59:36.276
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
 08/25/22 02:59:36.279
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
 08/25/22 02:59:36.279
STEP: creating a pod to probe DNS 08/25/22 02:59:36.279
STEP: submitting the pod to kubernetes 08/25/22 02:59:36.279
Aug 25 02:59:36.287: INFO: Waiting up to 15m0s for pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3" in namespace "dns-1334" to be "running"
Aug 25 02:59:36.291: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545149ms
Aug 25 02:59:38.315: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028444368s
Aug 25 02:59:40.297: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00955138s
Aug 25 02:59:42.296: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009195625s
Aug 25 02:59:44.296: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Running", Reason="", readiness=true. Elapsed: 8.008879521s
Aug 25 02:59:44.296: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3" satisfied condition "running"
STEP: retrieving the pod 08/25/22 02:59:44.296
STEP: looking for the results for each expected name from probers 08/25/22 02:59:44.301
Aug 25 02:59:44.308: INFO: DNS probes using dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3 succeeded

STEP: deleting the pod 08/25/22 02:59:44.308
STEP: changing the externalName to bar.example.com 08/25/22 02:59:44.317
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
 08/25/22 02:59:44.324
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
 08/25/22 02:59:44.324
STEP: creating a second pod to probe DNS 08/25/22 02:59:44.324
STEP: submitting the pod to kubernetes 08/25/22 02:59:44.325
Aug 25 02:59:44.330: INFO: Waiting up to 15m0s for pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d" in namespace "dns-1334" to be "running"
Aug 25 02:59:44.334: INFO: Pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942276ms
Aug 25 02:59:46.340: INFO: Pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009477815s
Aug 25 02:59:46.340: INFO: Pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d" satisfied condition "running"
STEP: retrieving the pod 08/25/22 02:59:46.34
STEP: looking for the results for each expected name from probers 08/25/22 02:59:46.345
Aug 25 02:59:46.349: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 02:59:46.352: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 02:59:46.352: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

Aug 25 02:59:51.357: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 02:59:51.360: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 02:59:51.360: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

Aug 25 02:59:56.358: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 02:59:56.361: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 02:59:56.361: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

Aug 25 03:00:01.357: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 03:00:01.360: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 03:00:01.360: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

Aug 25 03:00:06.358: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 03:00:06.362: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 25 03:00:06.362: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

Aug 25 03:00:11.361: INFO: DNS probes using dns-test-0799073f-eba0-466e-9f17-38a2f706e10d succeeded

STEP: deleting the pod 08/25/22 03:00:11.361
STEP: changing the service to type=ClusterIP 08/25/22 03:00:11.37
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
 08/25/22 03:00:11.387
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
 08/25/22 03:00:11.387
STEP: creating a third pod to probe DNS 08/25/22 03:00:11.388
STEP: submitting the pod to kubernetes 08/25/22 03:00:11.391
Aug 25 03:00:11.398: INFO: Waiting up to 15m0s for pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042" in namespace "dns-1334" to be "running"
Aug 25 03:00:11.402: INFO: Pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562259ms
Aug 25 03:00:13.408: INFO: Pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042": Phase="Running", Reason="", readiness=true. Elapsed: 2.009844435s
Aug 25 03:00:13.408: INFO: Pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042" satisfied condition "running"
STEP: retrieving the pod 08/25/22 03:00:13.408
STEP: looking for the results for each expected name from probers 08/25/22 03:00:13.413
Aug 25 03:00:13.420: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042 contains '' instead of '10.99.24.109'
Aug 25 03:00:13.420: INFO: Lookups using dns-1334/dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042 failed for: [jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

Aug 25 03:00:18.429: INFO: DNS probes using dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042 succeeded

STEP: deleting the pod 08/25/22 03:00:18.429
STEP: deleting the test externalName service 08/25/22 03:00:18.436
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 03:00:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1334" for this suite. 08/25/22 03:00:18.451
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":22,"skipped":309,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [42.202 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 02:59:36.253
    Aug 25 02:59:36.253: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 02:59:36.253
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 02:59:36.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 02:59:36.27
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/25/22 02:59:36.276
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
     08/25/22 02:59:36.279
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
     08/25/22 02:59:36.279
    STEP: creating a pod to probe DNS 08/25/22 02:59:36.279
    STEP: submitting the pod to kubernetes 08/25/22 02:59:36.279
    Aug 25 02:59:36.287: INFO: Waiting up to 15m0s for pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3" in namespace "dns-1334" to be "running"
    Aug 25 02:59:36.291: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545149ms
    Aug 25 02:59:38.315: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028444368s
    Aug 25 02:59:40.297: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00955138s
    Aug 25 02:59:42.296: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009195625s
    Aug 25 02:59:44.296: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3": Phase="Running", Reason="", readiness=true. Elapsed: 8.008879521s
    Aug 25 02:59:44.296: INFO: Pod "dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 02:59:44.296
    STEP: looking for the results for each expected name from probers 08/25/22 02:59:44.301
    Aug 25 02:59:44.308: INFO: DNS probes using dns-test-877bb404-ac4c-4f46-ad3b-69db682a2ce3 succeeded

    STEP: deleting the pod 08/25/22 02:59:44.308
    STEP: changing the externalName to bar.example.com 08/25/22 02:59:44.317
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
     08/25/22 02:59:44.324
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
     08/25/22 02:59:44.324
    STEP: creating a second pod to probe DNS 08/25/22 02:59:44.324
    STEP: submitting the pod to kubernetes 08/25/22 02:59:44.325
    Aug 25 02:59:44.330: INFO: Waiting up to 15m0s for pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d" in namespace "dns-1334" to be "running"
    Aug 25 02:59:44.334: INFO: Pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942276ms
    Aug 25 02:59:46.340: INFO: Pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009477815s
    Aug 25 02:59:46.340: INFO: Pod "dns-test-0799073f-eba0-466e-9f17-38a2f706e10d" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 02:59:46.34
    STEP: looking for the results for each expected name from probers 08/25/22 02:59:46.345
    Aug 25 02:59:46.349: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 02:59:46.352: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 02:59:46.352: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

    Aug 25 02:59:51.357: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 02:59:51.360: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 02:59:51.360: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

    Aug 25 02:59:56.358: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 02:59:56.361: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 02:59:56.361: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

    Aug 25 03:00:01.357: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 03:00:01.360: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 03:00:01.360: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

    Aug 25 03:00:06.358: INFO: File wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 03:00:06.362: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 25 03:00:06.362: INFO: Lookups using dns-1334/dns-test-0799073f-eba0-466e-9f17-38a2f706e10d failed for: [wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

    Aug 25 03:00:11.361: INFO: DNS probes using dns-test-0799073f-eba0-466e-9f17-38a2f706e10d succeeded

    STEP: deleting the pod 08/25/22 03:00:11.361
    STEP: changing the service to type=ClusterIP 08/25/22 03:00:11.37
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
     08/25/22 03:00:11.387
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1334.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local; sleep 1; done
     08/25/22 03:00:11.387
    STEP: creating a third pod to probe DNS 08/25/22 03:00:11.388
    STEP: submitting the pod to kubernetes 08/25/22 03:00:11.391
    Aug 25 03:00:11.398: INFO: Waiting up to 15m0s for pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042" in namespace "dns-1334" to be "running"
    Aug 25 03:00:11.402: INFO: Pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562259ms
    Aug 25 03:00:13.408: INFO: Pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042": Phase="Running", Reason="", readiness=true. Elapsed: 2.009844435s
    Aug 25 03:00:13.408: INFO: Pod "dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 03:00:13.408
    STEP: looking for the results for each expected name from probers 08/25/22 03:00:13.413
    Aug 25 03:00:13.420: INFO: File jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local from pod  dns-1334/dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042 contains '' instead of '10.99.24.109'
    Aug 25 03:00:13.420: INFO: Lookups using dns-1334/dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042 failed for: [jessie_udp@dns-test-service-3.dns-1334.svc.cluster.local]

    Aug 25 03:00:18.429: INFO: DNS probes using dns-test-579de110-e0e1-41c9-b6a3-dbba6dfc6042 succeeded

    STEP: deleting the pod 08/25/22 03:00:18.429
    STEP: deleting the test externalName service 08/25/22 03:00:18.436
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 03:00:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1334" for this suite. 08/25/22 03:00:18.451
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:00:18.455
Aug 25 03:00:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:00:18.457
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:18.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:18.471
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 08/25/22 03:00:18.475
STEP: Creating a ResourceQuota 08/25/22 03:00:23.479
STEP: Ensuring resource quota status is calculated 08/25/22 03:00:23.484
STEP: Creating a ReplicationController 08/25/22 03:00:25.488
STEP: Ensuring resource quota status captures replication controller creation 08/25/22 03:00:25.501
STEP: Deleting a ReplicationController 08/25/22 03:00:27.506
STEP: Ensuring resource quota status released usage 08/25/22 03:00:27.512
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:00:29.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9966" for this suite. 08/25/22 03:00:29.523
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":23,"skipped":309,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:00:18.455
    Aug 25 03:00:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:00:18.457
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:18.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:18.471
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 08/25/22 03:00:18.475
    STEP: Creating a ResourceQuota 08/25/22 03:00:23.479
    STEP: Ensuring resource quota status is calculated 08/25/22 03:00:23.484
    STEP: Creating a ReplicationController 08/25/22 03:00:25.488
    STEP: Ensuring resource quota status captures replication controller creation 08/25/22 03:00:25.501
    STEP: Deleting a ReplicationController 08/25/22 03:00:27.506
    STEP: Ensuring resource quota status released usage 08/25/22 03:00:27.512
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:00:29.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9966" for this suite. 08/25/22 03:00:29.523
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:00:29.529
Aug 25 03:00:29.529: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:00:29.531
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:29.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:29.548
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-32a2e985-8c81-4a9e-a6f3-a9891844a948 08/25/22 03:00:29.553
STEP: Creating a pod to test consume configMaps 08/25/22 03:00:29.556
Aug 25 03:00:29.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020" in namespace "projected-1705" to be "Succeeded or Failed"
Aug 25 03:00:29.565: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020": Phase="Pending", Reason="", readiness=false. Elapsed: 3.184599ms
Aug 25 03:00:31.570: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008039193s
Aug 25 03:00:33.571: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009109507s
STEP: Saw pod success 08/25/22 03:00:33.571
Aug 25 03:00:33.571: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020" satisfied condition "Succeeded or Failed"
Aug 25 03:00:33.575: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:00:33.582
Aug 25 03:00:33.588: INFO: Waiting for pod pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020 to disappear
Aug 25 03:00:33.591: INFO: Pod pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 03:00:33.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1705" for this suite. 08/25/22 03:00:33.595
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":24,"skipped":318,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.070 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:00:29.529
    Aug 25 03:00:29.529: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:00:29.531
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:29.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:29.548
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-32a2e985-8c81-4a9e-a6f3-a9891844a948 08/25/22 03:00:29.553
    STEP: Creating a pod to test consume configMaps 08/25/22 03:00:29.556
    Aug 25 03:00:29.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020" in namespace "projected-1705" to be "Succeeded or Failed"
    Aug 25 03:00:29.565: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020": Phase="Pending", Reason="", readiness=false. Elapsed: 3.184599ms
    Aug 25 03:00:31.570: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008039193s
    Aug 25 03:00:33.571: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009109507s
    STEP: Saw pod success 08/25/22 03:00:33.571
    Aug 25 03:00:33.571: INFO: Pod "pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020" satisfied condition "Succeeded or Failed"
    Aug 25 03:00:33.575: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:00:33.582
    Aug 25 03:00:33.588: INFO: Waiting for pod pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020 to disappear
    Aug 25 03:00:33.591: INFO: Pod pod-projected-configmaps-c1e46ffe-ebd3-4d71-baf8-80de3a414020 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 03:00:33.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1705" for this suite. 08/25/22 03:00:33.595
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:00:33.6
Aug 25 03:00:33.600: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pod-network-test 08/25/22 03:00:33.601
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:33.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:33.618
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-4861 08/25/22 03:00:33.621
STEP: creating a selector 08/25/22 03:00:33.622
STEP: Creating the service pods in kubernetes 08/25/22 03:00:33.622
Aug 25 03:00:33.622: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 25 03:00:33.633: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4861" to be "running and ready"
Aug 25 03:00:33.636: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78202ms
Aug 25 03:00:33.636: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:00:35.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007232788s
Aug 25 03:00:35.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:37.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008716927s
Aug 25 03:00:37.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:39.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008826818s
Aug 25 03:00:39.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:41.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008937546s
Aug 25 03:00:41.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:43.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007972466s
Aug 25 03:00:43.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:45.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008416992s
Aug 25 03:00:45.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:47.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.00785517s
Aug 25 03:00:47.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:49.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009906294s
Aug 25 03:00:49.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:51.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007518489s
Aug 25 03:00:51.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:53.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00871113s
Aug 25 03:00:53.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:00:55.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007344146s
Aug 25 03:00:55.640: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 25 03:00:55.640: INFO: Pod "netserver-0" satisfied condition "running and ready"
STEP: Creating test pods 08/25/22 03:00:55.644
Aug 25 03:00:55.649: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4861" to be "running"
Aug 25 03:00:55.652: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162305ms
Aug 25 03:00:57.657: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007997397s
Aug 25 03:00:57.657: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 25 03:00:57.661: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
Aug 25 03:00:57.661: INFO: Breadth first check of 192.168.0.13 on host 86.109.11.217...
Aug 25 03:00:57.664: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.0.20:9080/dial?request=hostname&protocol=http&host=192.168.0.13&port=8083&tries=1'] Namespace:pod-network-test-4861 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:00:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:00:57.665: INFO: ExecWithOptions: Clientset creation
Aug 25 03:00:57.666: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4861/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.0.13%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 25 03:00:57.783: INFO: Waiting for responses: map[]
Aug 25 03:00:57.783: INFO: reached 192.168.0.13 after 0/1 tries
Aug 25 03:00:57.783: INFO: Going to retry 0 out of 1 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 25 03:00:57.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4861" for this suite. 08/25/22 03:00:57.788
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":25,"skipped":319,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [24.194 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:00:33.6
    Aug 25 03:00:33.600: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pod-network-test 08/25/22 03:00:33.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:33.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:33.618
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-4861 08/25/22 03:00:33.621
    STEP: creating a selector 08/25/22 03:00:33.622
    STEP: Creating the service pods in kubernetes 08/25/22 03:00:33.622
    Aug 25 03:00:33.622: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 25 03:00:33.633: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4861" to be "running and ready"
    Aug 25 03:00:33.636: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78202ms
    Aug 25 03:00:33.636: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:00:35.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007232788s
    Aug 25 03:00:35.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:37.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008716927s
    Aug 25 03:00:37.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:39.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008826818s
    Aug 25 03:00:39.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:41.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008937546s
    Aug 25 03:00:41.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:43.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007972466s
    Aug 25 03:00:43.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:45.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008416992s
    Aug 25 03:00:45.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:47.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.00785517s
    Aug 25 03:00:47.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:49.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009906294s
    Aug 25 03:00:49.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:51.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007518489s
    Aug 25 03:00:51.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:53.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00871113s
    Aug 25 03:00:53.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:00:55.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007344146s
    Aug 25 03:00:55.640: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 25 03:00:55.640: INFO: Pod "netserver-0" satisfied condition "running and ready"
    STEP: Creating test pods 08/25/22 03:00:55.644
    Aug 25 03:00:55.649: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4861" to be "running"
    Aug 25 03:00:55.652: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162305ms
    Aug 25 03:00:57.657: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007997397s
    Aug 25 03:00:57.657: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 25 03:00:57.661: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
    Aug 25 03:00:57.661: INFO: Breadth first check of 192.168.0.13 on host 86.109.11.217...
    Aug 25 03:00:57.664: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.0.20:9080/dial?request=hostname&protocol=http&host=192.168.0.13&port=8083&tries=1'] Namespace:pod-network-test-4861 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:00:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:00:57.665: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:00:57.666: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4861/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.0.13%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 25 03:00:57.783: INFO: Waiting for responses: map[]
    Aug 25 03:00:57.783: INFO: reached 192.168.0.13 after 0/1 tries
    Aug 25 03:00:57.783: INFO: Going to retry 0 out of 1 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 25 03:00:57.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4861" for this suite. 08/25/22 03:00:57.788
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:00:57.795
Aug 25 03:00:57.795: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:00:57.796
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:57.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:57.812
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-8304 08/25/22 03:00:57.817
STEP: creating service affinity-clusterip in namespace services-8304 08/25/22 03:00:57.817
STEP: creating replication controller affinity-clusterip in namespace services-8304 08/25/22 03:00:57.825
I0825 03:00:57.829145      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8304, replica count: 3
I0825 03:01:00.880573      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 03:01:00.888: INFO: Creating new exec pod
Aug 25 03:01:00.893: INFO: Waiting up to 5m0s for pod "execpod-affinityzn4fc" in namespace "services-8304" to be "running"
Aug 25 03:01:00.896: INFO: Pod "execpod-affinityzn4fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095437ms
Aug 25 03:01:02.900: INFO: Pod "execpod-affinityzn4fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007003632s
Aug 25 03:01:02.900: INFO: Pod "execpod-affinityzn4fc" satisfied condition "running"
Aug 25 03:01:03.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8304 exec execpod-affinityzn4fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 25 03:01:04.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 25 03:01:04.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:01:04.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8304 exec execpod-affinityzn4fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.186.21 80'
Aug 25 03:01:04.251: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.186.21 80\nConnection to 10.102.186.21 80 port [tcp/http] succeeded!\n"
Aug 25 03:01:04.251: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:01:04.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8304 exec execpod-affinityzn4fc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.186.21:80/ ; done'
Aug 25 03:01:04.533: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n"
Aug 25 03:01:04.534: INFO: stdout: "\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5"
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
Aug 25 03:01:04.534: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8304, will wait for the garbage collector to delete the pods 08/25/22 03:01:04.541
Aug 25 03:01:04.601: INFO: Deleting ReplicationController affinity-clusterip took: 5.323557ms
Aug 25 03:01:04.701: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.202717ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:01:07.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8304" for this suite. 08/25/22 03:01:07.119
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":26,"skipped":329,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [9.329 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:00:57.795
    Aug 25 03:00:57.795: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:00:57.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:00:57.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:00:57.812
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-8304 08/25/22 03:00:57.817
    STEP: creating service affinity-clusterip in namespace services-8304 08/25/22 03:00:57.817
    STEP: creating replication controller affinity-clusterip in namespace services-8304 08/25/22 03:00:57.825
    I0825 03:00:57.829145      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8304, replica count: 3
    I0825 03:01:00.880573      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 03:01:00.888: INFO: Creating new exec pod
    Aug 25 03:01:00.893: INFO: Waiting up to 5m0s for pod "execpod-affinityzn4fc" in namespace "services-8304" to be "running"
    Aug 25 03:01:00.896: INFO: Pod "execpod-affinityzn4fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095437ms
    Aug 25 03:01:02.900: INFO: Pod "execpod-affinityzn4fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007003632s
    Aug 25 03:01:02.900: INFO: Pod "execpod-affinityzn4fc" satisfied condition "running"
    Aug 25 03:01:03.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8304 exec execpod-affinityzn4fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Aug 25 03:01:04.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 25 03:01:04.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:01:04.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8304 exec execpod-affinityzn4fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.186.21 80'
    Aug 25 03:01:04.251: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.186.21 80\nConnection to 10.102.186.21 80 port [tcp/http] succeeded!\n"
    Aug 25 03:01:04.251: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:01:04.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8304 exec execpod-affinityzn4fc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.186.21:80/ ; done'
    Aug 25 03:01:04.533: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.186.21:80/\n"
    Aug 25 03:01:04.534: INFO: stdout: "\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5\naffinity-clusterip-x45b5"
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Received response from host: affinity-clusterip-x45b5
    Aug 25 03:01:04.534: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8304, will wait for the garbage collector to delete the pods 08/25/22 03:01:04.541
    Aug 25 03:01:04.601: INFO: Deleting ReplicationController affinity-clusterip took: 5.323557ms
    Aug 25 03:01:04.701: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.202717ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:01:07.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8304" for this suite. 08/25/22 03:01:07.119
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:01:07.126
Aug 25 03:01:07.126: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:01:07.127
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:07.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:07.142
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 08/25/22 03:01:07.145
Aug 25 03:01:07.151: INFO: Waiting up to 5m0s for pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd" in namespace "emptydir-1011" to be "Succeeded or Failed"
Aug 25 03:01:07.154: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516343ms
Aug 25 03:01:09.158: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006759843s
Aug 25 03:01:11.157: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00596531s
STEP: Saw pod success 08/25/22 03:01:11.157
Aug 25 03:01:11.157: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd" satisfied condition "Succeeded or Failed"
Aug 25 03:01:11.160: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd container test-container: <nil>
STEP: delete the pod 08/25/22 03:01:11.166
Aug 25 03:01:11.172: INFO: Waiting for pod pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd to disappear
Aug 25 03:01:11.175: INFO: Pod pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:01:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1011" for this suite. 08/25/22 03:01:11.179
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":27,"skipped":375,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.057 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:01:07.126
    Aug 25 03:01:07.126: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:01:07.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:07.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:07.142
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 08/25/22 03:01:07.145
    Aug 25 03:01:07.151: INFO: Waiting up to 5m0s for pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd" in namespace "emptydir-1011" to be "Succeeded or Failed"
    Aug 25 03:01:07.154: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.516343ms
    Aug 25 03:01:09.158: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006759843s
    Aug 25 03:01:11.157: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00596531s
    STEP: Saw pod success 08/25/22 03:01:11.157
    Aug 25 03:01:11.157: INFO: Pod "pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd" satisfied condition "Succeeded or Failed"
    Aug 25 03:01:11.160: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd container test-container: <nil>
    STEP: delete the pod 08/25/22 03:01:11.166
    Aug 25 03:01:11.172: INFO: Waiting for pod pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd to disappear
    Aug 25 03:01:11.175: INFO: Pod pod-662e2d47-8ed8-4e0d-a26f-17a3d4154cdd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:01:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1011" for this suite. 08/25/22 03:01:11.179
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:01:11.184
Aug 25 03:01:11.184: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svcaccounts 08/25/22 03:01:11.185
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:11.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:11.2
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Aug 25 03:01:11.217: INFO: created pod pod-service-account-defaultsa
Aug 25 03:01:11.217: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 25 03:01:11.221: INFO: created pod pod-service-account-mountsa
Aug 25 03:01:11.221: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 25 03:01:11.226: INFO: created pod pod-service-account-nomountsa
Aug 25 03:01:11.226: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 25 03:01:11.231: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 25 03:01:11.231: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 25 03:01:11.234: INFO: created pod pod-service-account-mountsa-mountspec
Aug 25 03:01:11.234: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 25 03:01:11.238: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 25 03:01:11.238: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 25 03:01:11.242: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 25 03:01:11.242: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 25 03:01:11.245: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 25 03:01:11.245: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 25 03:01:11.247: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 25 03:01:11.247: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 25 03:01:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7412" for this suite. 08/25/22 03:01:11.249
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":28,"skipped":388,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.068 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:01:11.184
    Aug 25 03:01:11.184: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svcaccounts 08/25/22 03:01:11.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:11.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:11.2
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Aug 25 03:01:11.217: INFO: created pod pod-service-account-defaultsa
    Aug 25 03:01:11.217: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 25 03:01:11.221: INFO: created pod pod-service-account-mountsa
    Aug 25 03:01:11.221: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 25 03:01:11.226: INFO: created pod pod-service-account-nomountsa
    Aug 25 03:01:11.226: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 25 03:01:11.231: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 25 03:01:11.231: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 25 03:01:11.234: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 25 03:01:11.234: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 25 03:01:11.238: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 25 03:01:11.238: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 25 03:01:11.242: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 25 03:01:11.242: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 25 03:01:11.245: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 25 03:01:11.245: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 25 03:01:11.247: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 25 03:01:11.247: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 25 03:01:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7412" for this suite. 08/25/22 03:01:11.249
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:01:11.253
Aug 25 03:01:11.253: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 03:01:11.254
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:11.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:11.263
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9807 08/25/22 03:01:11.267
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 08/25/22 03:01:11.27
STEP: Creating pod with conflicting port in namespace statefulset-9807 08/25/22 03:01:11.273
STEP: Waiting until pod test-pod will start running in namespace statefulset-9807 08/25/22 03:01:11.278
Aug 25 03:01:11.278: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9807" to be "running"
Aug 25 03:01:11.281: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944107ms
Aug 25 03:01:13.286: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007365406s
Aug 25 03:01:15.287: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00872412s
Aug 25 03:01:15.287: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9807 08/25/22 03:01:15.287
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9807 08/25/22 03:01:15.293
Aug 25 03:01:15.303: INFO: Observed stateful pod in namespace: statefulset-9807, name: ss-0, uid: e8a3298a-e8d4-48cf-836a-dffda64485cc, status phase: Pending. Waiting for statefulset controller to delete.
Aug 25 03:01:19.024: INFO: Observed stateful pod in namespace: statefulset-9807, name: ss-0, uid: e8a3298a-e8d4-48cf-836a-dffda64485cc, status phase: Failed. Waiting for statefulset controller to delete.
Aug 25 03:01:19.031: INFO: Observed stateful pod in namespace: statefulset-9807, name: ss-0, uid: e8a3298a-e8d4-48cf-836a-dffda64485cc, status phase: Failed. Waiting for statefulset controller to delete.
Aug 25 03:01:19.032: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9807
STEP: Removing pod with conflicting port in namespace statefulset-9807 08/25/22 03:01:19.032
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9807 and will be in running state 08/25/22 03:01:19.041
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 03:01:27.065: INFO: Deleting all statefulset in ns statefulset-9807
Aug 25 03:01:27.070: INFO: Scaling statefulset ss to 0
Aug 25 03:01:37.088: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:01:37.092: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 03:01:37.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9807" for this suite. 08/25/22 03:01:37.111
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":29,"skipped":410,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [25.862 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:01:11.253
    Aug 25 03:01:11.253: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 03:01:11.254
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:11.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:11.263
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9807 08/25/22 03:01:11.267
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 08/25/22 03:01:11.27
    STEP: Creating pod with conflicting port in namespace statefulset-9807 08/25/22 03:01:11.273
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9807 08/25/22 03:01:11.278
    Aug 25 03:01:11.278: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9807" to be "running"
    Aug 25 03:01:11.281: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944107ms
    Aug 25 03:01:13.286: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007365406s
    Aug 25 03:01:15.287: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00872412s
    Aug 25 03:01:15.287: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9807 08/25/22 03:01:15.287
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9807 08/25/22 03:01:15.293
    Aug 25 03:01:15.303: INFO: Observed stateful pod in namespace: statefulset-9807, name: ss-0, uid: e8a3298a-e8d4-48cf-836a-dffda64485cc, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 25 03:01:19.024: INFO: Observed stateful pod in namespace: statefulset-9807, name: ss-0, uid: e8a3298a-e8d4-48cf-836a-dffda64485cc, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 25 03:01:19.031: INFO: Observed stateful pod in namespace: statefulset-9807, name: ss-0, uid: e8a3298a-e8d4-48cf-836a-dffda64485cc, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 25 03:01:19.032: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9807
    STEP: Removing pod with conflicting port in namespace statefulset-9807 08/25/22 03:01:19.032
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9807 and will be in running state 08/25/22 03:01:19.041
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 03:01:27.065: INFO: Deleting all statefulset in ns statefulset-9807
    Aug 25 03:01:27.070: INFO: Scaling statefulset ss to 0
    Aug 25 03:01:37.088: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:01:37.092: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 03:01:37.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9807" for this suite. 08/25/22 03:01:37.111
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:01:37.116
Aug 25 03:01:37.116: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 03:01:37.117
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:37.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:37.133
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 08/25/22 03:01:37.136
STEP: fetching the ConfigMap 08/25/22 03:01:37.14
STEP: patching the ConfigMap 08/25/22 03:01:37.142
STEP: listing all ConfigMaps in all namespaces with a label selector 08/25/22 03:01:37.145
STEP: deleting the ConfigMap by collection with a label selector 08/25/22 03:01:37.15
STEP: listing all ConfigMaps in test namespace 08/25/22 03:01:37.155
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 03:01:37.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4095" for this suite. 08/25/22 03:01:37.164
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":30,"skipped":422,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.053 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:01:37.116
    Aug 25 03:01:37.116: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 03:01:37.117
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:37.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:37.133
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 08/25/22 03:01:37.136
    STEP: fetching the ConfigMap 08/25/22 03:01:37.14
    STEP: patching the ConfigMap 08/25/22 03:01:37.142
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/25/22 03:01:37.145
    STEP: deleting the ConfigMap by collection with a label selector 08/25/22 03:01:37.15
    STEP: listing all ConfigMaps in test namespace 08/25/22 03:01:37.155
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 03:01:37.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4095" for this suite. 08/25/22 03:01:37.164
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:01:37.172
Aug 25 03:01:37.172: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 03:01:37.173
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:37.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:37.186
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8 in namespace container-probe-5946 08/25/22 03:01:37.19
Aug 25 03:01:37.196: INFO: Waiting up to 5m0s for pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8" in namespace "container-probe-5946" to be "not pending"
Aug 25 03:01:37.199: INFO: Pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049038ms
Aug 25 03:01:39.205: INFO: Pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008512934s
Aug 25 03:01:39.205: INFO: Pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8" satisfied condition "not pending"
Aug 25 03:01:39.205: INFO: Started pod liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8 in namespace container-probe-5946
STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:01:39.205
Aug 25 03:01:39.209: INFO: Initial restart count of pod liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8 is 0
STEP: deleting the pod 08/25/22 03:05:39.847
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 03:05:39.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5946" for this suite. 08/25/22 03:05:39.861
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":31,"skipped":464,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [242.694 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:01:37.172
    Aug 25 03:01:37.172: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 03:01:37.173
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:01:37.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:01:37.186
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8 in namespace container-probe-5946 08/25/22 03:01:37.19
    Aug 25 03:01:37.196: INFO: Waiting up to 5m0s for pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8" in namespace "container-probe-5946" to be "not pending"
    Aug 25 03:01:37.199: INFO: Pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049038ms
    Aug 25 03:01:39.205: INFO: Pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008512934s
    Aug 25 03:01:39.205: INFO: Pod "liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8" satisfied condition "not pending"
    Aug 25 03:01:39.205: INFO: Started pod liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8 in namespace container-probe-5946
    STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:01:39.205
    Aug 25 03:01:39.209: INFO: Initial restart count of pod liveness-c8454da0-dccd-4546-a2a7-4233fcc8a8f8 is 0
    STEP: deleting the pod 08/25/22 03:05:39.847
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 03:05:39.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5946" for this suite. 08/25/22 03:05:39.861
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:05:39.867
Aug 25 03:05:39.867: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:05:39.869
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:39.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:39.886
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 08/25/22 03:05:39.891
Aug 25 03:05:39.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-2709 create -f -'
Aug 25 03:05:40.297: INFO: stderr: ""
Aug 25 03:05:40.297: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/25/22 03:05:40.297
Aug 25 03:05:41.302: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:05:41.302: INFO: Found 1 / 1
Aug 25 03:05:41.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/25/22 03:05:41.302
Aug 25 03:05:41.306: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:05:41.306: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 25 03:05:41.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-2709 patch pod agnhost-primary-nkjv9 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 25 03:05:41.390: INFO: stderr: ""
Aug 25 03:05:41.390: INFO: stdout: "pod/agnhost-primary-nkjv9 patched\n"
STEP: checking annotations 08/25/22 03:05:41.39
Aug 25 03:05:41.394: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:05:41.394: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:05:41.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2709" for this suite. 08/25/22 03:05:41.398
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":32,"skipped":483,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [1.536 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:05:39.867
    Aug 25 03:05:39.867: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:05:39.869
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:39.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:39.886
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 08/25/22 03:05:39.891
    Aug 25 03:05:39.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-2709 create -f -'
    Aug 25 03:05:40.297: INFO: stderr: ""
    Aug 25 03:05:40.297: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/25/22 03:05:40.297
    Aug 25 03:05:41.302: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:05:41.302: INFO: Found 1 / 1
    Aug 25 03:05:41.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/25/22 03:05:41.302
    Aug 25 03:05:41.306: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:05:41.306: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 25 03:05:41.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-2709 patch pod agnhost-primary-nkjv9 -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 25 03:05:41.390: INFO: stderr: ""
    Aug 25 03:05:41.390: INFO: stdout: "pod/agnhost-primary-nkjv9 patched\n"
    STEP: checking annotations 08/25/22 03:05:41.39
    Aug 25 03:05:41.394: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:05:41.394: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:05:41.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2709" for this suite. 08/25/22 03:05:41.398
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:05:41.405
Aug 25 03:05:41.406: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:05:41.407
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:41.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:41.43
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:05:41.435
Aug 25 03:05:41.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4" in namespace "downward-api-3531" to be "Succeeded or Failed"
Aug 25 03:05:41.447: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01236ms
Aug 25 03:05:43.451: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010521767s
Aug 25 03:05:45.454: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012662709s
STEP: Saw pod success 08/25/22 03:05:45.454
Aug 25 03:05:45.454: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4" satisfied condition "Succeeded or Failed"
Aug 25 03:05:45.458: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4 container client-container: <nil>
STEP: delete the pod 08/25/22 03:05:45.481
Aug 25 03:05:45.488: INFO: Waiting for pod downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4 to disappear
Aug 25 03:05:45.491: INFO: Pod downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 03:05:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3531" for this suite. 08/25/22 03:05:45.495
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":33,"skipped":507,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:05:41.405
    Aug 25 03:05:41.406: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:05:41.407
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:41.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:41.43
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:05:41.435
    Aug 25 03:05:41.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4" in namespace "downward-api-3531" to be "Succeeded or Failed"
    Aug 25 03:05:41.447: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01236ms
    Aug 25 03:05:43.451: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010521767s
    Aug 25 03:05:45.454: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012662709s
    STEP: Saw pod success 08/25/22 03:05:45.454
    Aug 25 03:05:45.454: INFO: Pod "downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4" satisfied condition "Succeeded or Failed"
    Aug 25 03:05:45.458: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:05:45.481
    Aug 25 03:05:45.488: INFO: Waiting for pod downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4 to disappear
    Aug 25 03:05:45.491: INFO: Pod downwardapi-volume-76d73490-aed7-435f-abeb-b4d89903cef4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 03:05:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3531" for this suite. 08/25/22 03:05:45.495
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:05:45.502
Aug 25 03:05:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 03:05:45.503
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:45.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:45.518
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/25/22 03:05:45.525
STEP: create the rc2 08/25/22 03:05:45.529
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/25/22 03:05:50.537
STEP: delete the rc simpletest-rc-to-be-deleted 08/25/22 03:05:50.677
STEP: wait for the rc to be deleted 08/25/22 03:05:50.681
STEP: Gathering metrics 08/25/22 03:05:55.696
Aug 25 03:05:55.712: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
Aug 25 03:05:55.716: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.575015ms
Aug 25 03:05:55.716: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
Aug 25 03:05:55.716: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
Aug 25 03:05:55.871: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 25 03:05:55.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l97x" in namespace "gc-4684"
Aug 25 03:05:55.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qx2w" in namespace "gc-4684"
Aug 25 03:05:55.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zb6z" in namespace "gc-4684"
Aug 25 03:05:55.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-46rfw" in namespace "gc-4684"
Aug 25 03:05:55.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-49wjt" in namespace "gc-4684"
Aug 25 03:05:55.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gn6s" in namespace "gc-4684"
Aug 25 03:05:55.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vxpj" in namespace "gc-4684"
Aug 25 03:05:55.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w4kd" in namespace "gc-4684"
Aug 25 03:05:55.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wpzk" in namespace "gc-4684"
Aug 25 03:05:55.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f2kb" in namespace "gc-4684"
Aug 25 03:05:55.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xl89" in namespace "gc-4684"
Aug 25 03:05:55.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-68ppp" in namespace "gc-4684"
Aug 25 03:05:55.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-6g69m" in namespace "gc-4684"
Aug 25 03:05:55.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qj5b" in namespace "gc-4684"
Aug 25 03:05:55.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-766pd" in namespace "gc-4684"
Aug 25 03:05:55.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m7vp" in namespace "gc-4684"
Aug 25 03:05:55.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wz5f" in namespace "gc-4684"
Aug 25 03:05:55.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cmj8" in namespace "gc-4684"
Aug 25 03:05:55.981: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkr98" in namespace "gc-4684"
Aug 25 03:05:55.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvqzn" in namespace "gc-4684"
Aug 25 03:05:55.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc9q4" in namespace "gc-4684"
Aug 25 03:05:56.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-djwpk" in namespace "gc-4684"
Aug 25 03:05:56.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmtc6" in namespace "gc-4684"
Aug 25 03:05:56.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjprk" in namespace "gc-4684"
Aug 25 03:05:56.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn5lh" in namespace "gc-4684"
Aug 25 03:05:56.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-grfph" in namespace "gc-4684"
Aug 25 03:05:56.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9x79" in namespace "gc-4684"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 03:05:56.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4684" for this suite. 08/25/22 03:05:56.046
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":34,"skipped":523,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.548 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:05:45.502
    Aug 25 03:05:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 03:05:45.503
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:45.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:45.518
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/25/22 03:05:45.525
    STEP: create the rc2 08/25/22 03:05:45.529
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/25/22 03:05:50.537
    STEP: delete the rc simpletest-rc-to-be-deleted 08/25/22 03:05:50.677
    STEP: wait for the rc to be deleted 08/25/22 03:05:50.681
    STEP: Gathering metrics 08/25/22 03:05:55.696
    Aug 25 03:05:55.712: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
    Aug 25 03:05:55.716: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.575015ms
    Aug 25 03:05:55.716: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
    Aug 25 03:05:55.716: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
    Aug 25 03:05:55.871: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 25 03:05:55.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l97x" in namespace "gc-4684"
    Aug 25 03:05:55.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qx2w" in namespace "gc-4684"
    Aug 25 03:05:55.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zb6z" in namespace "gc-4684"
    Aug 25 03:05:55.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-46rfw" in namespace "gc-4684"
    Aug 25 03:05:55.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-49wjt" in namespace "gc-4684"
    Aug 25 03:05:55.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gn6s" in namespace "gc-4684"
    Aug 25 03:05:55.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vxpj" in namespace "gc-4684"
    Aug 25 03:05:55.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w4kd" in namespace "gc-4684"
    Aug 25 03:05:55.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wpzk" in namespace "gc-4684"
    Aug 25 03:05:55.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f2kb" in namespace "gc-4684"
    Aug 25 03:05:55.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xl89" in namespace "gc-4684"
    Aug 25 03:05:55.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-68ppp" in namespace "gc-4684"
    Aug 25 03:05:55.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-6g69m" in namespace "gc-4684"
    Aug 25 03:05:55.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qj5b" in namespace "gc-4684"
    Aug 25 03:05:55.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-766pd" in namespace "gc-4684"
    Aug 25 03:05:55.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m7vp" in namespace "gc-4684"
    Aug 25 03:05:55.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wz5f" in namespace "gc-4684"
    Aug 25 03:05:55.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cmj8" in namespace "gc-4684"
    Aug 25 03:05:55.981: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkr98" in namespace "gc-4684"
    Aug 25 03:05:55.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvqzn" in namespace "gc-4684"
    Aug 25 03:05:55.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc9q4" in namespace "gc-4684"
    Aug 25 03:05:56.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-djwpk" in namespace "gc-4684"
    Aug 25 03:05:56.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmtc6" in namespace "gc-4684"
    Aug 25 03:05:56.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjprk" in namespace "gc-4684"
    Aug 25 03:05:56.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn5lh" in namespace "gc-4684"
    Aug 25 03:05:56.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-grfph" in namespace "gc-4684"
    Aug 25 03:05:56.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9x79" in namespace "gc-4684"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 03:05:56.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4684" for this suite. 08/25/22 03:05:56.046
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:05:56.051
Aug 25 03:05:56.051: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename namespaces 08/25/22 03:05:56.051
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:56.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:56.063
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 08/25/22 03:05:56.067
STEP: patching the Namespace 08/25/22 03:05:56.076
STEP: get the Namespace and ensuring it has the label 08/25/22 03:05:56.08
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:05:56.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9027" for this suite. 08/25/22 03:05:56.089
STEP: Destroying namespace "nspatchtest-95a606c5-19fa-4a0f-a241-7aa02c2b54af-6265" for this suite. 08/25/22 03:05:56.093
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":35,"skipped":542,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.046 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:05:56.051
    Aug 25 03:05:56.051: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename namespaces 08/25/22 03:05:56.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:56.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:56.063
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 08/25/22 03:05:56.067
    STEP: patching the Namespace 08/25/22 03:05:56.076
    STEP: get the Namespace and ensuring it has the label 08/25/22 03:05:56.08
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:05:56.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9027" for this suite. 08/25/22 03:05:56.089
    STEP: Destroying namespace "nspatchtest-95a606c5-19fa-4a0f-a241-7aa02c2b54af-6265" for this suite. 08/25/22 03:05:56.093
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:05:56.098
Aug 25 03:05:56.098: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:05:56.099
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:56.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:56.11
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cb8d2b4b-a66a-4790-858e-30fb3bed149c 08/25/22 03:05:56.116
STEP: Creating the pod 08/25/22 03:05:56.119
Aug 25 03:05:56.125: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb" in namespace "projected-9797" to be "running and ready"
Aug 25 03:05:56.135: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.565314ms
Aug 25 03:05:56.135: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:05:58.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015918665s
Aug 25 03:05:58.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:00.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015642991s
Aug 25 03:06:00.140: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:02.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016644191s
Aug 25 03:06:02.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:04.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016814192s
Aug 25 03:06:04.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:06.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015846079s
Aug 25 03:06:06.140: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:08.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016322225s
Aug 25 03:06:08.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:10.142: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016983539s
Aug 25 03:06:10.142: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:12.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015871985s
Aug 25 03:06:12.140: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:06:14.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Running", Reason="", readiness=true. Elapsed: 18.016793619s
Aug 25 03:06:14.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Running (Ready = true)
Aug 25 03:06:14.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-cb8d2b4b-a66a-4790-858e-30fb3bed149c 08/25/22 03:06:14.151
STEP: waiting to observe update in volume 08/25/22 03:06:14.156
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 03:07:22.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9797" for this suite. 08/25/22 03:07:22.418
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":36,"skipped":570,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [86.325 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:05:56.098
    Aug 25 03:05:56.098: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:05:56.099
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:05:56.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:05:56.11
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-cb8d2b4b-a66a-4790-858e-30fb3bed149c 08/25/22 03:05:56.116
    STEP: Creating the pod 08/25/22 03:05:56.119
    Aug 25 03:05:56.125: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb" in namespace "projected-9797" to be "running and ready"
    Aug 25 03:05:56.135: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.565314ms
    Aug 25 03:05:56.135: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:05:58.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015918665s
    Aug 25 03:05:58.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:00.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015642991s
    Aug 25 03:06:00.140: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:02.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016644191s
    Aug 25 03:06:02.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:04.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016814192s
    Aug 25 03:06:04.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:06.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015846079s
    Aug 25 03:06:06.140: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:08.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016322225s
    Aug 25 03:06:08.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:10.142: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016983539s
    Aug 25 03:06:10.142: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:12.140: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015871985s
    Aug 25 03:06:12.140: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:06:14.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb": Phase="Running", Reason="", readiness=true. Elapsed: 18.016793619s
    Aug 25 03:06:14.141: INFO: The phase of Pod pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb is Running (Ready = true)
    Aug 25 03:06:14.141: INFO: Pod "pod-projected-configmaps-3f3bc454-0500-46db-abb0-6aac80fed7eb" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-cb8d2b4b-a66a-4790-858e-30fb3bed149c 08/25/22 03:06:14.151
    STEP: waiting to observe update in volume 08/25/22 03:06:14.156
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 03:07:22.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9797" for this suite. 08/25/22 03:07:22.418
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:22.425
Aug 25 03:07:22.425: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename server-version 08/25/22 03:07:22.427
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:22.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:22.442
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/25/22 03:07:22.447
STEP: Confirm major version 08/25/22 03:07:22.449
Aug 25 03:07:22.449: INFO: Major version: 1
STEP: Confirm minor version 08/25/22 03:07:22.449
Aug 25 03:07:22.449: INFO: cleanMinorVersion: 25
Aug 25 03:07:22.449: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Aug 25 03:07:22.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8182" for this suite. 08/25/22 03:07:22.453
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":37,"skipped":593,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.032 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:22.425
    Aug 25 03:07:22.425: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename server-version 08/25/22 03:07:22.427
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:22.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:22.442
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/25/22 03:07:22.447
    STEP: Confirm major version 08/25/22 03:07:22.449
    Aug 25 03:07:22.449: INFO: Major version: 1
    STEP: Confirm minor version 08/25/22 03:07:22.449
    Aug 25 03:07:22.449: INFO: cleanMinorVersion: 25
    Aug 25 03:07:22.449: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Aug 25 03:07:22.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8182" for this suite. 08/25/22 03:07:22.453
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:22.458
Aug 25 03:07:22.458: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:07:22.46
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:22.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:22.479
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-2532a497-a813-4ca7-b665-d270e2a5338a 08/25/22 03:07:22.483
STEP: Creating a pod to test consume configMaps 08/25/22 03:07:22.487
Aug 25 03:07:22.492: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722" in namespace "projected-1453" to be "Succeeded or Failed"
Aug 25 03:07:22.496: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749623ms
Aug 25 03:07:24.503: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010601099s
Aug 25 03:07:26.502: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009176365s
STEP: Saw pod success 08/25/22 03:07:26.502
Aug 25 03:07:26.502: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722" satisfied condition "Succeeded or Failed"
Aug 25 03:07:26.506: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:07:26.512
Aug 25 03:07:26.518: INFO: Waiting for pod pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722 to disappear
Aug 25 03:07:26.522: INFO: Pod pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 03:07:26.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1453" for this suite. 08/25/22 03:07:26.527
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":38,"skipped":599,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.074 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:22.458
    Aug 25 03:07:22.458: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:07:22.46
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:22.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:22.479
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-2532a497-a813-4ca7-b665-d270e2a5338a 08/25/22 03:07:22.483
    STEP: Creating a pod to test consume configMaps 08/25/22 03:07:22.487
    Aug 25 03:07:22.492: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722" in namespace "projected-1453" to be "Succeeded or Failed"
    Aug 25 03:07:22.496: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749623ms
    Aug 25 03:07:24.503: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010601099s
    Aug 25 03:07:26.502: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009176365s
    STEP: Saw pod success 08/25/22 03:07:26.502
    Aug 25 03:07:26.502: INFO: Pod "pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722" satisfied condition "Succeeded or Failed"
    Aug 25 03:07:26.506: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:07:26.512
    Aug 25 03:07:26.518: INFO: Waiting for pod pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722 to disappear
    Aug 25 03:07:26.522: INFO: Pod pod-projected-configmaps-1a7a0642-6f73-4b6e-9a54-ff29ba084722 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 03:07:26.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1453" for this suite. 08/25/22 03:07:26.527
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:26.533
Aug 25 03:07:26.533: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 03:07:26.534
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:26.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:26.55
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 08/25/22 03:07:26.567
STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 03:07:26.571
Aug 25 03:07:26.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 03:07:26.578: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 03:07:27.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 03:07:27.584: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 03:07:28.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 03:07:28.587: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Getting /status 08/25/22 03:07:28.592
Aug 25 03:07:28.600: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/25/22 03:07:28.6
Aug 25 03:07:28.609: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/25/22 03:07:28.609
Aug 25 03:07:28.612: INFO: Observed &DaemonSet event: ADDED
Aug 25 03:07:28.612: INFO: Observed &DaemonSet event: MODIFIED
Aug 25 03:07:28.612: INFO: Observed &DaemonSet event: MODIFIED
Aug 25 03:07:28.612: INFO: Found daemon set daemon-set in namespace daemonsets-7722 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 25 03:07:28.612: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/25/22 03:07:28.612
STEP: watching for the daemon set status to be patched 08/25/22 03:07:28.619
Aug 25 03:07:28.621: INFO: Observed &DaemonSet event: ADDED
Aug 25 03:07:28.622: INFO: Observed &DaemonSet event: MODIFIED
Aug 25 03:07:28.622: INFO: Observed &DaemonSet event: MODIFIED
Aug 25 03:07:28.622: INFO: Observed daemon set daemon-set in namespace daemonsets-7722 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 25 03:07:28.622: INFO: Observed &DaemonSet event: MODIFIED
Aug 25 03:07:28.622: INFO: Found daemon set daemon-set in namespace daemonsets-7722 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 25 03:07:28.622: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/25/22 03:07:28.626
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7722, will wait for the garbage collector to delete the pods 08/25/22 03:07:28.626
Aug 25 03:07:28.686: INFO: Deleting DaemonSet.extensions daemon-set took: 5.405602ms
Aug 25 03:07:28.787: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.063396ms
Aug 25 03:07:30.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 03:07:30.992: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 25 03:07:30.996: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13882"},"items":null}

Aug 25 03:07:31.000: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13882"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:07:31.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7722" for this suite. 08/25/22 03:07:31.011
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":39,"skipped":602,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.483 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:26.533
    Aug 25 03:07:26.533: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 03:07:26.534
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:26.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:26.55
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 08/25/22 03:07:26.567
    STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 03:07:26.571
    Aug 25 03:07:26.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 03:07:26.578: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 03:07:27.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 03:07:27.584: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 03:07:28.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 03:07:28.587: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Getting /status 08/25/22 03:07:28.592
    Aug 25 03:07:28.600: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/25/22 03:07:28.6
    Aug 25 03:07:28.609: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/25/22 03:07:28.609
    Aug 25 03:07:28.612: INFO: Observed &DaemonSet event: ADDED
    Aug 25 03:07:28.612: INFO: Observed &DaemonSet event: MODIFIED
    Aug 25 03:07:28.612: INFO: Observed &DaemonSet event: MODIFIED
    Aug 25 03:07:28.612: INFO: Found daemon set daemon-set in namespace daemonsets-7722 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 25 03:07:28.612: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/25/22 03:07:28.612
    STEP: watching for the daemon set status to be patched 08/25/22 03:07:28.619
    Aug 25 03:07:28.621: INFO: Observed &DaemonSet event: ADDED
    Aug 25 03:07:28.622: INFO: Observed &DaemonSet event: MODIFIED
    Aug 25 03:07:28.622: INFO: Observed &DaemonSet event: MODIFIED
    Aug 25 03:07:28.622: INFO: Observed daemon set daemon-set in namespace daemonsets-7722 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 25 03:07:28.622: INFO: Observed &DaemonSet event: MODIFIED
    Aug 25 03:07:28.622: INFO: Found daemon set daemon-set in namespace daemonsets-7722 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 25 03:07:28.622: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/25/22 03:07:28.626
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7722, will wait for the garbage collector to delete the pods 08/25/22 03:07:28.626
    Aug 25 03:07:28.686: INFO: Deleting DaemonSet.extensions daemon-set took: 5.405602ms
    Aug 25 03:07:28.787: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.063396ms
    Aug 25 03:07:30.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 03:07:30.992: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 25 03:07:30.996: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13882"},"items":null}

    Aug 25 03:07:31.000: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13882"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:07:31.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7722" for this suite. 08/25/22 03:07:31.011
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:31.016
Aug 25 03:07:31.016: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:07:31.017
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:31.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:31.031
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/25/22 03:07:31.035
Aug 25 03:07:31.035: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:07:35.103: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:07:49.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2888" for this suite. 08/25/22 03:07:49.385
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":40,"skipped":609,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [18.373 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:31.016
    Aug 25 03:07:31.016: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:07:31.017
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:31.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:31.031
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/25/22 03:07:31.035
    Aug 25 03:07:31.035: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:07:35.103: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:07:49.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2888" for this suite. 08/25/22 03:07:49.385
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:49.39
Aug 25 03:07:49.390: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:07:49.392
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:49.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:49.407
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-e2b8110e-2552-4037-95ef-6aa6ceb88212 08/25/22 03:07:49.411
STEP: Creating secret with name secret-projected-all-test-volume-2ffc0210-65b4-4621-a090-b7c5f9cc8aac 08/25/22 03:07:49.415
STEP: Creating a pod to test Check all projections for projected volume plugin 08/25/22 03:07:49.419
Aug 25 03:07:49.425: INFO: Waiting up to 5m0s for pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b" in namespace "projected-4401" to be "Succeeded or Failed"
Aug 25 03:07:49.428: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771398ms
Aug 25 03:07:51.433: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007329327s
Aug 25 03:07:53.434: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008688985s
STEP: Saw pod success 08/25/22 03:07:53.434
Aug 25 03:07:53.434: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b" satisfied condition "Succeeded or Failed"
Aug 25 03:07:53.438: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b container projected-all-volume-test: <nil>
STEP: delete the pod 08/25/22 03:07:53.444
Aug 25 03:07:53.451: INFO: Waiting for pod projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b to disappear
Aug 25 03:07:53.454: INFO: Pod projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Aug 25 03:07:53.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4401" for this suite. 08/25/22 03:07:53.459
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":41,"skipped":615,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.074 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:49.39
    Aug 25 03:07:49.390: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:07:49.392
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:49.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:49.407
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-e2b8110e-2552-4037-95ef-6aa6ceb88212 08/25/22 03:07:49.411
    STEP: Creating secret with name secret-projected-all-test-volume-2ffc0210-65b4-4621-a090-b7c5f9cc8aac 08/25/22 03:07:49.415
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/25/22 03:07:49.419
    Aug 25 03:07:49.425: INFO: Waiting up to 5m0s for pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b" in namespace "projected-4401" to be "Succeeded or Failed"
    Aug 25 03:07:49.428: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771398ms
    Aug 25 03:07:51.433: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007329327s
    Aug 25 03:07:53.434: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008688985s
    STEP: Saw pod success 08/25/22 03:07:53.434
    Aug 25 03:07:53.434: INFO: Pod "projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b" satisfied condition "Succeeded or Failed"
    Aug 25 03:07:53.438: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b container projected-all-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:07:53.444
    Aug 25 03:07:53.451: INFO: Waiting for pod projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b to disappear
    Aug 25 03:07:53.454: INFO: Pod projected-volume-fbfbc97b-a31a-4bb1-a91a-e6a40211720b no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Aug 25 03:07:53.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4401" for this suite. 08/25/22 03:07:53.459
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:53.464
Aug 25 03:07:53.464: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename endpointslice 08/25/22 03:07:53.466
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:53.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:53.481
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 25 03:07:55.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9291" for this suite. 08/25/22 03:07:55.527
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":42,"skipped":616,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.067 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:53.464
    Aug 25 03:07:53.464: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename endpointslice 08/25/22 03:07:53.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:53.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:53.481
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 25 03:07:55.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9291" for this suite. 08/25/22 03:07:55.527
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:07:55.532
Aug 25 03:07:55.532: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 03:07:55.533
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:55.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:55.545
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 25 03:07:55.566: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"24ff66dc-4116-477e-bfd7-3a02bfcb5937", Controller:(*bool)(0xc004243726), BlockOwnerDeletion:(*bool)(0xc004243727)}}
Aug 25 03:07:55.571: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b7009760-023b-4059-a7b9-e544c5813857", Controller:(*bool)(0xc009284d76), BlockOwnerDeletion:(*bool)(0xc009284d77)}}
Aug 25 03:07:55.577: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"42766a00-5f9d-4f6f-9f5f-8fedbf3cbb86", Controller:(*bool)(0xc00928516e), BlockOwnerDeletion:(*bool)(0xc00928516f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 03:08:00.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6741" for this suite. 08/25/22 03:08:00.588
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":43,"skipped":633,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.060 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:07:55.532
    Aug 25 03:07:55.532: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 03:07:55.533
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:07:55.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:07:55.545
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 25 03:07:55.566: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"24ff66dc-4116-477e-bfd7-3a02bfcb5937", Controller:(*bool)(0xc004243726), BlockOwnerDeletion:(*bool)(0xc004243727)}}
    Aug 25 03:07:55.571: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b7009760-023b-4059-a7b9-e544c5813857", Controller:(*bool)(0xc009284d76), BlockOwnerDeletion:(*bool)(0xc009284d77)}}
    Aug 25 03:07:55.577: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"42766a00-5f9d-4f6f-9f5f-8fedbf3cbb86", Controller:(*bool)(0xc00928516e), BlockOwnerDeletion:(*bool)(0xc00928516f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 03:08:00.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6741" for this suite. 08/25/22 03:08:00.588
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:00.594
Aug 25 03:08:00.594: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sysctl 08/25/22 03:08:00.595
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:00.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:00.609
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/25/22 03:08:00.612
STEP: Watching for error events or started pod 08/25/22 03:08:00.617
STEP: Waiting for pod completion 08/25/22 03:08:02.622
Aug 25 03:08:02.623: INFO: Waiting up to 3m0s for pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec" in namespace "sysctl-5178" to be "completed"
Aug 25 03:08:02.627: INFO: Pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219589ms
Aug 25 03:08:04.632: INFO: Pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009535708s
Aug 25 03:08:04.632: INFO: Pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/25/22 03:08:04.636
STEP: Getting logs from the pod 08/25/22 03:08:04.636
STEP: Checking that the sysctl is actually updated 08/25/22 03:08:04.642
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 03:08:04.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5178" for this suite. 08/25/22 03:08:04.646
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":44,"skipped":645,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.056 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:00.594
    Aug 25 03:08:00.594: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sysctl 08/25/22 03:08:00.595
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:00.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:00.609
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/25/22 03:08:00.612
    STEP: Watching for error events or started pod 08/25/22 03:08:00.617
    STEP: Waiting for pod completion 08/25/22 03:08:02.622
    Aug 25 03:08:02.623: INFO: Waiting up to 3m0s for pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec" in namespace "sysctl-5178" to be "completed"
    Aug 25 03:08:02.627: INFO: Pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219589ms
    Aug 25 03:08:04.632: INFO: Pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009535708s
    Aug 25 03:08:04.632: INFO: Pod "sysctl-40fcfb92-5df4-4639-9f94-f3a396fe0aec" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/25/22 03:08:04.636
    STEP: Getting logs from the pod 08/25/22 03:08:04.636
    STEP: Checking that the sysctl is actually updated 08/25/22 03:08:04.642
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 03:08:04.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5178" for this suite. 08/25/22 03:08:04.646
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:04.65
Aug 25 03:08:04.651: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:08:04.652
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:04.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:04.669
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 08/25/22 03:08:04.673
Aug 25 03:08:04.680: INFO: Waiting up to 5m0s for pod "pod-ec6d346b-145e-474c-b366-76360f35f959" in namespace "emptydir-2479" to be "Succeeded or Failed"
Aug 25 03:08:04.682: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449612ms
Aug 25 03:08:06.687: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007601661s
Aug 25 03:08:08.688: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008075951s
STEP: Saw pod success 08/25/22 03:08:08.688
Aug 25 03:08:08.688: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959" satisfied condition "Succeeded or Failed"
Aug 25 03:08:08.692: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-ec6d346b-145e-474c-b366-76360f35f959 container test-container: <nil>
STEP: delete the pod 08/25/22 03:08:08.698
Aug 25 03:08:08.705: INFO: Waiting for pod pod-ec6d346b-145e-474c-b366-76360f35f959 to disappear
Aug 25 03:08:08.709: INFO: Pod pod-ec6d346b-145e-474c-b366-76360f35f959 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:08:08.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2479" for this suite. 08/25/22 03:08:08.713
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":648,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:04.65
    Aug 25 03:08:04.651: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:08:04.652
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:04.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:04.669
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/25/22 03:08:04.673
    Aug 25 03:08:04.680: INFO: Waiting up to 5m0s for pod "pod-ec6d346b-145e-474c-b366-76360f35f959" in namespace "emptydir-2479" to be "Succeeded or Failed"
    Aug 25 03:08:04.682: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449612ms
    Aug 25 03:08:06.687: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007601661s
    Aug 25 03:08:08.688: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008075951s
    STEP: Saw pod success 08/25/22 03:08:08.688
    Aug 25 03:08:08.688: INFO: Pod "pod-ec6d346b-145e-474c-b366-76360f35f959" satisfied condition "Succeeded or Failed"
    Aug 25 03:08:08.692: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-ec6d346b-145e-474c-b366-76360f35f959 container test-container: <nil>
    STEP: delete the pod 08/25/22 03:08:08.698
    Aug 25 03:08:08.705: INFO: Waiting for pod pod-ec6d346b-145e-474c-b366-76360f35f959 to disappear
    Aug 25 03:08:08.709: INFO: Pod pod-ec6d346b-145e-474c-b366-76360f35f959 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:08:08.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2479" for this suite. 08/25/22 03:08:08.713
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:08.719
Aug 25 03:08:08.719: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:08:08.721
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:08.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:08.736
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 08/25/22 03:08:08.741
Aug 25 03:08:08.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 25 03:08:08.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
Aug 25 03:08:09.402: INFO: stderr: ""
Aug 25 03:08:09.402: INFO: stdout: "service/agnhost-replica created\n"
Aug 25 03:08:09.402: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 25 03:08:09.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
Aug 25 03:08:09.669: INFO: stderr: ""
Aug 25 03:08:09.669: INFO: stdout: "service/agnhost-primary created\n"
Aug 25 03:08:09.669: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 25 03:08:09.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
Aug 25 03:08:09.932: INFO: stderr: ""
Aug 25 03:08:09.932: INFO: stdout: "service/frontend created\n"
Aug 25 03:08:09.932: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 25 03:08:09.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
Aug 25 03:08:10.192: INFO: stderr: ""
Aug 25 03:08:10.192: INFO: stdout: "deployment.apps/frontend created\n"
Aug 25 03:08:10.192: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 25 03:08:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
Aug 25 03:08:10.441: INFO: stderr: ""
Aug 25 03:08:10.441: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 25 03:08:10.441: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 25 03:08:10.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
Aug 25 03:08:10.701: INFO: stderr: ""
Aug 25 03:08:10.701: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/25/22 03:08:10.701
Aug 25 03:08:10.701: INFO: Waiting for all frontend pods to be Running.
Aug 25 03:08:15.752: INFO: Waiting for frontend to serve content.
Aug 25 03:08:15.762: INFO: Trying to add a new entry to the guestbook.
Aug 25 03:08:15.770: INFO: Verifying that added entry can be retrieved.
Aug 25 03:08:15.777: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 08/25/22 03:08:20.786
Aug 25 03:08:20.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
Aug 25 03:08:20.876: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:08:20.876: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/25/22 03:08:20.876
Aug 25 03:08:20.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
Aug 25 03:08:20.951: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:08:20.951: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/25/22 03:08:20.951
Aug 25 03:08:20.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
Aug 25 03:08:21.033: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:08:21.033: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/25/22 03:08:21.033
Aug 25 03:08:21.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
Aug 25 03:08:21.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:08:21.121: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/25/22 03:08:21.121
Aug 25 03:08:21.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
Aug 25 03:08:21.283: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:08:21.283: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/25/22 03:08:21.283
Aug 25 03:08:21.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
Aug 25 03:08:21.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:08:21.364: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:08:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-492" for this suite. 08/25/22 03:08:21.367
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":46,"skipped":669,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [12.652 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:08.719
    Aug 25 03:08:08.719: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:08:08.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:08.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:08.736
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 08/25/22 03:08:08.741
    Aug 25 03:08:08.741: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 25 03:08:08.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
    Aug 25 03:08:09.402: INFO: stderr: ""
    Aug 25 03:08:09.402: INFO: stdout: "service/agnhost-replica created\n"
    Aug 25 03:08:09.402: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 25 03:08:09.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
    Aug 25 03:08:09.669: INFO: stderr: ""
    Aug 25 03:08:09.669: INFO: stdout: "service/agnhost-primary created\n"
    Aug 25 03:08:09.669: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 25 03:08:09.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
    Aug 25 03:08:09.932: INFO: stderr: ""
    Aug 25 03:08:09.932: INFO: stdout: "service/frontend created\n"
    Aug 25 03:08:09.932: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 25 03:08:09.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
    Aug 25 03:08:10.192: INFO: stderr: ""
    Aug 25 03:08:10.192: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 25 03:08:10.192: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 25 03:08:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
    Aug 25 03:08:10.441: INFO: stderr: ""
    Aug 25 03:08:10.441: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 25 03:08:10.441: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 25 03:08:10.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 create -f -'
    Aug 25 03:08:10.701: INFO: stderr: ""
    Aug 25 03:08:10.701: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/25/22 03:08:10.701
    Aug 25 03:08:10.701: INFO: Waiting for all frontend pods to be Running.
    Aug 25 03:08:15.752: INFO: Waiting for frontend to serve content.
    Aug 25 03:08:15.762: INFO: Trying to add a new entry to the guestbook.
    Aug 25 03:08:15.770: INFO: Verifying that added entry can be retrieved.
    Aug 25 03:08:15.777: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 08/25/22 03:08:20.786
    Aug 25 03:08:20.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    Aug 25 03:08:20.876: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:08:20.876: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/25/22 03:08:20.876
    Aug 25 03:08:20.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    Aug 25 03:08:20.951: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:08:20.951: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/25/22 03:08:20.951
    Aug 25 03:08:20.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    Aug 25 03:08:21.033: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:08:21.033: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/25/22 03:08:21.033
    Aug 25 03:08:21.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    Aug 25 03:08:21.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:08:21.121: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/25/22 03:08:21.121
    Aug 25 03:08:21.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    Aug 25 03:08:21.283: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:08:21.283: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/25/22 03:08:21.283
    Aug 25 03:08:21.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    Aug 25 03:08:21.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:08:21.364: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:08:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-492" for this suite. 08/25/22 03:08:21.367
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:21.373
Aug 25 03:08:21.373: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:08:21.374
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:21.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:21.387
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 08/25/22 03:08:21.391
STEP: setting up watch 08/25/22 03:08:21.391
STEP: submitting the pod to kubernetes 08/25/22 03:08:21.494
STEP: verifying the pod is in kubernetes 08/25/22 03:08:21.503
STEP: verifying pod creation was observed 08/25/22 03:08:21.506
Aug 25 03:08:21.506: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377" in namespace "pods-3498" to be "running"
Aug 25 03:08:21.509: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504041ms
Aug 25 03:08:23.516: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009849723s
Aug 25 03:08:25.515: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377": Phase="Running", Reason="", readiness=true. Elapsed: 4.008641153s
Aug 25 03:08:25.515: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377" satisfied condition "running"
STEP: deleting the pod gracefully 08/25/22 03:08:25.518
STEP: verifying pod deletion was observed 08/25/22 03:08:25.524
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:08:26.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3498" for this suite. 08/25/22 03:08:26.269
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":47,"skipped":698,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.901 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:21.373
    Aug 25 03:08:21.373: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:08:21.374
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:21.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:21.387
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 08/25/22 03:08:21.391
    STEP: setting up watch 08/25/22 03:08:21.391
    STEP: submitting the pod to kubernetes 08/25/22 03:08:21.494
    STEP: verifying the pod is in kubernetes 08/25/22 03:08:21.503
    STEP: verifying pod creation was observed 08/25/22 03:08:21.506
    Aug 25 03:08:21.506: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377" in namespace "pods-3498" to be "running"
    Aug 25 03:08:21.509: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504041ms
    Aug 25 03:08:23.516: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009849723s
    Aug 25 03:08:25.515: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377": Phase="Running", Reason="", readiness=true. Elapsed: 4.008641153s
    Aug 25 03:08:25.515: INFO: Pod "pod-submit-remove-5ea16a13-8ad3-4dc8-8c3a-b574b5187377" satisfied condition "running"
    STEP: deleting the pod gracefully 08/25/22 03:08:25.518
    STEP: verifying pod deletion was observed 08/25/22 03:08:25.524
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:08:26.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3498" for this suite. 08/25/22 03:08:26.269
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:26.274
Aug 25 03:08:26.274: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename endpointslice 08/25/22 03:08:26.275
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:26.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:26.291
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Aug 25 03:08:26.302: INFO: Endpoints addresses: [86.109.11.217] , ports: [6443]
Aug 25 03:08:26.302: INFO: EndpointSlices addresses: [86.109.11.217] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 25 03:08:26.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2472" for this suite. 08/25/22 03:08:26.305
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":48,"skipped":698,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.036 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:26.274
    Aug 25 03:08:26.274: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename endpointslice 08/25/22 03:08:26.275
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:26.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:26.291
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Aug 25 03:08:26.302: INFO: Endpoints addresses: [86.109.11.217] , ports: [6443]
    Aug 25 03:08:26.302: INFO: EndpointSlices addresses: [86.109.11.217] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 25 03:08:26.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2472" for this suite. 08/25/22 03:08:26.305
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:26.311
Aug 25 03:08:26.311: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 03:08:26.312
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:26.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:26.336
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-0d5a9c75-64b3-4873-8878-4891d216767a 08/25/22 03:08:26.339
STEP: Creating a pod to test consume configMaps 08/25/22 03:08:26.342
Aug 25 03:08:26.350: INFO: Waiting up to 5m0s for pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d" in namespace "configmap-6283" to be "Succeeded or Failed"
Aug 25 03:08:26.352: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843602ms
Aug 25 03:08:28.358: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008301837s
Aug 25 03:08:30.358: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00808316s
STEP: Saw pod success 08/25/22 03:08:30.358
Aug 25 03:08:30.358: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d" satisfied condition "Succeeded or Failed"
Aug 25 03:08:30.362: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:08:30.367
Aug 25 03:08:30.374: INFO: Waiting for pod pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d to disappear
Aug 25 03:08:30.377: INFO: Pod pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 03:08:30.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6283" for this suite. 08/25/22 03:08:30.381
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":49,"skipped":707,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.075 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:26.311
    Aug 25 03:08:26.311: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 03:08:26.312
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:26.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:26.336
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-0d5a9c75-64b3-4873-8878-4891d216767a 08/25/22 03:08:26.339
    STEP: Creating a pod to test consume configMaps 08/25/22 03:08:26.342
    Aug 25 03:08:26.350: INFO: Waiting up to 5m0s for pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d" in namespace "configmap-6283" to be "Succeeded or Failed"
    Aug 25 03:08:26.352: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843602ms
    Aug 25 03:08:28.358: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008301837s
    Aug 25 03:08:30.358: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00808316s
    STEP: Saw pod success 08/25/22 03:08:30.358
    Aug 25 03:08:30.358: INFO: Pod "pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d" satisfied condition "Succeeded or Failed"
    Aug 25 03:08:30.362: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:08:30.367
    Aug 25 03:08:30.374: INFO: Waiting for pod pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d to disappear
    Aug 25 03:08:30.377: INFO: Pod pod-configmaps-f12a94aa-6d3a-4d45-920e-0ce624d5828d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 03:08:30.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6283" for this suite. 08/25/22 03:08:30.381
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:30.389
Aug 25 03:08:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:08:30.391
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:30.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:30.407
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:08:30.414
Aug 25 03:08:30.419: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3" in namespace "projected-2336" to be "Succeeded or Failed"
Aug 25 03:08:30.421: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.959793ms
Aug 25 03:08:32.425: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006836388s
Aug 25 03:08:34.427: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008278248s
STEP: Saw pod success 08/25/22 03:08:34.427
Aug 25 03:08:34.427: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3" satisfied condition "Succeeded or Failed"
Aug 25 03:08:34.431: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3 container client-container: <nil>
STEP: delete the pod 08/25/22 03:08:34.437
Aug 25 03:08:34.444: INFO: Waiting for pod downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3 to disappear
Aug 25 03:08:34.448: INFO: Pod downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:08:34.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2336" for this suite. 08/25/22 03:08:34.452
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":50,"skipped":760,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:30.389
    Aug 25 03:08:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:08:30.391
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:30.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:30.407
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:08:30.414
    Aug 25 03:08:30.419: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3" in namespace "projected-2336" to be "Succeeded or Failed"
    Aug 25 03:08:30.421: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.959793ms
    Aug 25 03:08:32.425: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006836388s
    Aug 25 03:08:34.427: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008278248s
    STEP: Saw pod success 08/25/22 03:08:34.427
    Aug 25 03:08:34.427: INFO: Pod "downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3" satisfied condition "Succeeded or Failed"
    Aug 25 03:08:34.431: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:08:34.437
    Aug 25 03:08:34.444: INFO: Waiting for pod downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3 to disappear
    Aug 25 03:08:34.448: INFO: Pod downwardapi-volume-84a595c8-df79-4ce9-b6ef-d416df5df3b3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:08:34.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2336" for this suite. 08/25/22 03:08:34.452
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:34.457
Aug 25 03:08:34.457: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename security-context-test 08/25/22 03:08:34.458
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:34.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:34.473
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Aug 25 03:08:34.483: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa" in namespace "security-context-test-961" to be "Succeeded or Failed"
Aug 25 03:08:34.487: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435228ms
Aug 25 03:08:36.490: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": Phase="Running", Reason="", readiness=false. Elapsed: 2.007315656s
Aug 25 03:08:38.493: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009862099s
Aug 25 03:08:38.493: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa" satisfied condition "Succeeded or Failed"
Aug 25 03:08:38.499: INFO: Got logs for pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 25 03:08:38.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-961" for this suite. 08/25/22 03:08:38.503
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":51,"skipped":776,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.052 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:34.457
    Aug 25 03:08:34.457: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename security-context-test 08/25/22 03:08:34.458
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:34.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:34.473
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Aug 25 03:08:34.483: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa" in namespace "security-context-test-961" to be "Succeeded or Failed"
    Aug 25 03:08:34.487: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435228ms
    Aug 25 03:08:36.490: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": Phase="Running", Reason="", readiness=false. Elapsed: 2.007315656s
    Aug 25 03:08:38.493: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009862099s
    Aug 25 03:08:38.493: INFO: Pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa" satisfied condition "Succeeded or Failed"
    Aug 25 03:08:38.499: INFO: Got logs for pod "busybox-privileged-false-19c02622-0cbb-4e2e-a1e2-2da2a97f04aa": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 25 03:08:38.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-961" for this suite. 08/25/22 03:08:38.503
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:38.509
Aug 25 03:08:38.509: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-runtime 08/25/22 03:08:38.51
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:38.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:38.526
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 08/25/22 03:08:38.53
STEP: wait for the container to reach Succeeded 08/25/22 03:08:38.537
STEP: get the container status 08/25/22 03:08:42.558
STEP: the container should be terminated 08/25/22 03:08:42.562
STEP: the termination message should be set 08/25/22 03:08:42.563
Aug 25 03:08:42.563: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/25/22 03:08:42.563
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 25 03:08:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-498" for this suite. 08/25/22 03:08:42.58
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":52,"skipped":780,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.075 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:38.509
    Aug 25 03:08:38.509: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-runtime 08/25/22 03:08:38.51
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:38.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:38.526
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 08/25/22 03:08:38.53
    STEP: wait for the container to reach Succeeded 08/25/22 03:08:38.537
    STEP: get the container status 08/25/22 03:08:42.558
    STEP: the container should be terminated 08/25/22 03:08:42.562
    STEP: the termination message should be set 08/25/22 03:08:42.563
    Aug 25 03:08:42.563: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/25/22 03:08:42.563
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 25 03:08:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-498" for this suite. 08/25/22 03:08:42.58
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:42.585
Aug 25 03:08:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:08:42.586
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:42.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:42.6
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:08:42.628
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:08:42.973
STEP: Deploying the webhook pod 08/25/22 03:08:42.981
STEP: Wait for the deployment to be ready 08/25/22 03:08:42.991
Aug 25 03:08:42.998: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:08:45.012
STEP: Verifying the service has paired with the endpoint 08/25/22 03:08:45.022
Aug 25 03:08:46.023: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 08/25/22 03:08:46.027
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/25/22 03:08:46.05
STEP: Creating a configMap that should not be mutated 08/25/22 03:08:46.057
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/25/22 03:08:46.065
STEP: Creating a configMap that should be mutated 08/25/22 03:08:46.071
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:08:46.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7544" for this suite. 08/25/22 03:08:46.098
STEP: Destroying namespace "webhook-7544-markers" for this suite. 08/25/22 03:08:46.103
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":53,"skipped":786,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.545 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:42.585
    Aug 25 03:08:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:08:42.586
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:42.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:42.6
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:08:42.628
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:08:42.973
    STEP: Deploying the webhook pod 08/25/22 03:08:42.981
    STEP: Wait for the deployment to be ready 08/25/22 03:08:42.991
    Aug 25 03:08:42.998: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:08:45.012
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:08:45.022
    Aug 25 03:08:46.023: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 08/25/22 03:08:46.027
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/25/22 03:08:46.05
    STEP: Creating a configMap that should not be mutated 08/25/22 03:08:46.057
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/25/22 03:08:46.065
    STEP: Creating a configMap that should be mutated 08/25/22 03:08:46.071
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:08:46.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7544" for this suite. 08/25/22 03:08:46.098
    STEP: Destroying namespace "webhook-7544-markers" for this suite. 08/25/22 03:08:46.103
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:46.136
Aug 25 03:08:46.136: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 03:08:46.137
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:46.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:46.149
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/25/22 03:08:46.153
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/25/22 03:08:46.153
STEP: creating a pod to probe DNS 08/25/22 03:08:46.153
STEP: submitting the pod to kubernetes 08/25/22 03:08:46.153
Aug 25 03:08:46.159: INFO: Waiting up to 15m0s for pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3" in namespace "dns-5243" to be "running"
Aug 25 03:08:46.161: INFO: Pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.543435ms
Aug 25 03:08:48.167: INFO: Pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007727757s
Aug 25 03:08:48.167: INFO: Pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3" satisfied condition "running"
STEP: retrieving the pod 08/25/22 03:08:48.167
STEP: looking for the results for each expected name from probers 08/25/22 03:08:48.17
Aug 25 03:08:48.184: INFO: DNS probes using dns-5243/dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3 succeeded

STEP: deleting the pod 08/25/22 03:08:48.184
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 03:08:48.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5243" for this suite. 08/25/22 03:08:48.197
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":54,"skipped":852,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.065 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:46.136
    Aug 25 03:08:46.136: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 03:08:46.137
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:46.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:46.149
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/25/22 03:08:46.153
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/25/22 03:08:46.153
    STEP: creating a pod to probe DNS 08/25/22 03:08:46.153
    STEP: submitting the pod to kubernetes 08/25/22 03:08:46.153
    Aug 25 03:08:46.159: INFO: Waiting up to 15m0s for pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3" in namespace "dns-5243" to be "running"
    Aug 25 03:08:46.161: INFO: Pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.543435ms
    Aug 25 03:08:48.167: INFO: Pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007727757s
    Aug 25 03:08:48.167: INFO: Pod "dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 03:08:48.167
    STEP: looking for the results for each expected name from probers 08/25/22 03:08:48.17
    Aug 25 03:08:48.184: INFO: DNS probes using dns-5243/dns-test-bca4c967-ffa3-4f1d-8b99-eb54e8b9c4a3 succeeded

    STEP: deleting the pod 08/25/22 03:08:48.184
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 03:08:48.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5243" for this suite. 08/25/22 03:08:48.197
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:48.202
Aug 25 03:08:48.202: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:08:48.204
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:48.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:48.22
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 08/25/22 03:08:48.225
Aug 25 03:08:48.233: INFO: Waiting up to 5m0s for pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf" in namespace "emptydir-4106" to be "Succeeded or Failed"
Aug 25 03:08:48.235: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671358ms
Aug 25 03:08:50.241: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008167958s
Aug 25 03:08:52.241: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008080285s
STEP: Saw pod success 08/25/22 03:08:52.241
Aug 25 03:08:52.241: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf" satisfied condition "Succeeded or Failed"
Aug 25 03:08:52.245: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-4bc6d828-25d9-48b8-9203-10f53d2453bf container test-container: <nil>
STEP: delete the pod 08/25/22 03:08:52.25
Aug 25 03:08:52.257: INFO: Waiting for pod pod-4bc6d828-25d9-48b8-9203-10f53d2453bf to disappear
Aug 25 03:08:52.259: INFO: Pod pod-4bc6d828-25d9-48b8-9203-10f53d2453bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:08:52.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4106" for this suite. 08/25/22 03:08:52.264
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":55,"skipped":858,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:48.202
    Aug 25 03:08:48.202: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:08:48.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:48.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:48.22
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/25/22 03:08:48.225
    Aug 25 03:08:48.233: INFO: Waiting up to 5m0s for pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf" in namespace "emptydir-4106" to be "Succeeded or Failed"
    Aug 25 03:08:48.235: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671358ms
    Aug 25 03:08:50.241: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008167958s
    Aug 25 03:08:52.241: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008080285s
    STEP: Saw pod success 08/25/22 03:08:52.241
    Aug 25 03:08:52.241: INFO: Pod "pod-4bc6d828-25d9-48b8-9203-10f53d2453bf" satisfied condition "Succeeded or Failed"
    Aug 25 03:08:52.245: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-4bc6d828-25d9-48b8-9203-10f53d2453bf container test-container: <nil>
    STEP: delete the pod 08/25/22 03:08:52.25
    Aug 25 03:08:52.257: INFO: Waiting for pod pod-4bc6d828-25d9-48b8-9203-10f53d2453bf to disappear
    Aug 25 03:08:52.259: INFO: Pod pod-4bc6d828-25d9-48b8-9203-10f53d2453bf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:08:52.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4106" for this suite. 08/25/22 03:08:52.264
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:52.268
Aug 25 03:08:52.269: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:08:52.27
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:52.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:52.285
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:08:52.298
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:08:52.658
STEP: Deploying the webhook pod 08/25/22 03:08:52.664
STEP: Wait for the deployment to be ready 08/25/22 03:08:52.675
Aug 25 03:08:52.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:08:54.696
STEP: Verifying the service has paired with the endpoint 08/25/22 03:08:54.706
Aug 25 03:08:55.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 08/25/22 03:08:55.766
STEP: Creating a configMap that should be mutated 08/25/22 03:08:55.785
STEP: Deleting the collection of validation webhooks 08/25/22 03:08:55.816
STEP: Creating a configMap that should not be mutated 08/25/22 03:08:55.841
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:08:55.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2041" for this suite. 08/25/22 03:08:55.851
STEP: Destroying namespace "webhook-2041-markers" for this suite. 08/25/22 03:08:55.856
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":56,"skipped":858,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.615 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:52.268
    Aug 25 03:08:52.269: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:08:52.27
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:52.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:52.285
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:08:52.298
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:08:52.658
    STEP: Deploying the webhook pod 08/25/22 03:08:52.664
    STEP: Wait for the deployment to be ready 08/25/22 03:08:52.675
    Aug 25 03:08:52.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:08:54.696
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:08:54.706
    Aug 25 03:08:55.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 08/25/22 03:08:55.766
    STEP: Creating a configMap that should be mutated 08/25/22 03:08:55.785
    STEP: Deleting the collection of validation webhooks 08/25/22 03:08:55.816
    STEP: Creating a configMap that should not be mutated 08/25/22 03:08:55.841
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:08:55.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2041" for this suite. 08/25/22 03:08:55.851
    STEP: Destroying namespace "webhook-2041-markers" for this suite. 08/25/22 03:08:55.856
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:08:55.885
Aug 25 03:08:55.885: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename aggregator 08/25/22 03:08:55.886
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:55.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:55.899
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 25 03:08:55.902: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/25/22 03:08:55.903
Aug 25 03:08:56.226: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 25 03:08:58.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:00.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:02.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:04.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:06.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:08.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:10.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:12.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:14.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:16.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:18.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:09:20.403: INFO: Waited 124.687459ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/25/22 03:09:20.474
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/25/22 03:09:20.477
STEP: List APIServices 08/25/22 03:09:20.494
Aug 25 03:09:20.545: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Aug 25 03:09:21.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-868" for this suite. 08/25/22 03:09:21.388
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":57,"skipped":875,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [25.552 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:08:55.885
    Aug 25 03:08:55.885: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename aggregator 08/25/22 03:08:55.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:08:55.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:08:55.899
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 25 03:08:55.902: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/25/22 03:08:55.903
    Aug 25 03:08:56.226: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Aug 25 03:08:58.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:00.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:02.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:04.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:06.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:08.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:10.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:12.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:14.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:16.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:18.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:09:20.403: INFO: Waited 124.687459ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/25/22 03:09:20.474
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/25/22 03:09:20.477
    STEP: List APIServices 08/25/22 03:09:20.494
    Aug 25 03:09:20.545: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Aug 25 03:09:21.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-868" for this suite. 08/25/22 03:09:21.388
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:09:21.442
Aug 25 03:09:21.442: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replicaset 08/25/22 03:09:21.444
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:21.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:21.461
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/25/22 03:09:21.464
Aug 25 03:09:21.472: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 25 03:09:26.475: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/25/22 03:09:26.475
STEP: getting scale subresource 08/25/22 03:09:26.475
STEP: updating a scale subresource 08/25/22 03:09:26.478
STEP: verifying the replicaset Spec.Replicas was modified 08/25/22 03:09:26.483
STEP: Patch a scale subresource 08/25/22 03:09:26.486
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 25 03:09:26.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3833" for this suite. 08/25/22 03:09:26.498
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":58,"skipped":927,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.060 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:09:21.442
    Aug 25 03:09:21.442: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replicaset 08/25/22 03:09:21.444
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:21.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:21.461
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/25/22 03:09:21.464
    Aug 25 03:09:21.472: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 25 03:09:26.475: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/25/22 03:09:26.475
    STEP: getting scale subresource 08/25/22 03:09:26.475
    STEP: updating a scale subresource 08/25/22 03:09:26.478
    STEP: verifying the replicaset Spec.Replicas was modified 08/25/22 03:09:26.483
    STEP: Patch a scale subresource 08/25/22 03:09:26.486
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 25 03:09:26.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3833" for this suite. 08/25/22 03:09:26.498
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:09:26.504
Aug 25 03:09:26.504: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:09:26.506
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:26.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:26.519
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 08/25/22 03:09:26.523
STEP: Creating a ResourceQuota 08/25/22 03:09:31.527
STEP: Ensuring resource quota status is calculated 08/25/22 03:09:31.53
STEP: Creating a ReplicaSet 08/25/22 03:09:33.536
STEP: Ensuring resource quota status captures replicaset creation 08/25/22 03:09:33.546
STEP: Deleting a ReplicaSet 08/25/22 03:09:35.551
STEP: Ensuring resource quota status released usage 08/25/22 03:09:35.555
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:09:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-440" for this suite. 08/25/22 03:09:37.566
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":59,"skipped":946,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [11.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:09:26.504
    Aug 25 03:09:26.504: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:09:26.506
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:26.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:26.519
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 08/25/22 03:09:26.523
    STEP: Creating a ResourceQuota 08/25/22 03:09:31.527
    STEP: Ensuring resource quota status is calculated 08/25/22 03:09:31.53
    STEP: Creating a ReplicaSet 08/25/22 03:09:33.536
    STEP: Ensuring resource quota status captures replicaset creation 08/25/22 03:09:33.546
    STEP: Deleting a ReplicaSet 08/25/22 03:09:35.551
    STEP: Ensuring resource quota status released usage 08/25/22 03:09:35.555
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:09:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-440" for this suite. 08/25/22 03:09:37.566
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:09:37.571
Aug 25 03:09:37.571: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 03:09:37.573
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:37.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:37.59
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/25/22 03:09:37.594
Aug 25 03:09:37.601: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5773  7d564e80-c59b-491e-a9c0-acee1e8719b4 15476 0 2022-08-25 03:09:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-08-25 03:09:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hw957,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hw957,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:09:37.602: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5773" to be "running and ready"
Aug 25 03:09:37.604: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.431447ms
Aug 25 03:09:37.604: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:09:39.610: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.008044965s
Aug 25 03:09:39.610: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 25 03:09:39.610: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/25/22 03:09:39.61
Aug 25 03:09:39.610: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5773 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:09:39.610: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:09:39.611: INFO: ExecWithOptions: Clientset creation
Aug 25 03:09:39.611: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5773/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/25/22 03:09:39.752
Aug 25 03:09:39.752: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5773 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:09:39.752: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:09:39.754: INFO: ExecWithOptions: Clientset creation
Aug 25 03:09:39.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5773/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 25 03:09:39.870: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 03:09:39.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5773" for this suite. 08/25/22 03:09:39.883
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":60,"skipped":948,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.322 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:09:37.571
    Aug 25 03:09:37.571: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 03:09:37.573
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:37.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:37.59
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/25/22 03:09:37.594
    Aug 25 03:09:37.601: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5773  7d564e80-c59b-491e-a9c0-acee1e8719b4 15476 0 2022-08-25 03:09:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-08-25 03:09:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hw957,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hw957,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:09:37.602: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5773" to be "running and ready"
    Aug 25 03:09:37.604: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.431447ms
    Aug 25 03:09:37.604: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:09:39.610: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.008044965s
    Aug 25 03:09:39.610: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 25 03:09:39.610: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/25/22 03:09:39.61
    Aug 25 03:09:39.610: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5773 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:09:39.610: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:09:39.611: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:09:39.611: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5773/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/25/22 03:09:39.752
    Aug 25 03:09:39.752: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5773 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:09:39.752: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:09:39.754: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:09:39.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5773/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 25 03:09:39.870: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 03:09:39.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5773" for this suite. 08/25/22 03:09:39.883
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:09:39.896
Aug 25 03:09:39.896: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename lease-test 08/25/22 03:09:39.897
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:39.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:39.912
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Aug 25 03:09:39.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5858" for this suite. 08/25/22 03:09:39.961
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":61,"skipped":996,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.069 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:09:39.896
    Aug 25 03:09:39.896: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename lease-test 08/25/22 03:09:39.897
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:39.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:39.912
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Aug 25 03:09:39.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-5858" for this suite. 08/25/22 03:09:39.961
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:09:39.969
Aug 25 03:09:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 03:09:39.971
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:39.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:39.984
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/25/22 03:09:39.993
STEP: delete the rc 08/25/22 03:09:45.005
STEP: wait for the rc to be deleted 08/25/22 03:09:45.009
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/25/22 03:09:50.013
STEP: Gathering metrics 08/25/22 03:10:20.04
Aug 25 03:10:20.055: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
Aug 25 03:10:20.059: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.291888ms
Aug 25 03:10:20.059: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
Aug 25 03:10:20.059: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
Aug 25 03:10:20.151: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 25 03:10:20.151: INFO: Deleting pod "simpletest.rc-4jb6l" in namespace "gc-7893"
Aug 25 03:10:20.158: INFO: Deleting pod "simpletest.rc-64mvt" in namespace "gc-7893"
Aug 25 03:10:20.164: INFO: Deleting pod "simpletest.rc-7dlhv" in namespace "gc-7893"
Aug 25 03:10:20.171: INFO: Deleting pod "simpletest.rc-7tqpw" in namespace "gc-7893"
Aug 25 03:10:20.178: INFO: Deleting pod "simpletest.rc-8dzbm" in namespace "gc-7893"
Aug 25 03:10:20.183: INFO: Deleting pod "simpletest.rc-9lzrc" in namespace "gc-7893"
Aug 25 03:10:20.188: INFO: Deleting pod "simpletest.rc-9t6d9" in namespace "gc-7893"
Aug 25 03:10:20.193: INFO: Deleting pod "simpletest.rc-9zm4q" in namespace "gc-7893"
Aug 25 03:10:20.200: INFO: Deleting pod "simpletest.rc-b4lnv" in namespace "gc-7893"
Aug 25 03:10:20.206: INFO: Deleting pod "simpletest.rc-b9tl7" in namespace "gc-7893"
Aug 25 03:10:20.213: INFO: Deleting pod "simpletest.rc-bflvx" in namespace "gc-7893"
Aug 25 03:10:20.217: INFO: Deleting pod "simpletest.rc-bxjk6" in namespace "gc-7893"
Aug 25 03:10:20.222: INFO: Deleting pod "simpletest.rc-c6wgt" in namespace "gc-7893"
Aug 25 03:10:20.226: INFO: Deleting pod "simpletest.rc-dcswp" in namespace "gc-7893"
Aug 25 03:10:20.231: INFO: Deleting pod "simpletest.rc-djljk" in namespace "gc-7893"
Aug 25 03:10:20.236: INFO: Deleting pod "simpletest.rc-dqcqn" in namespace "gc-7893"
Aug 25 03:10:20.242: INFO: Deleting pod "simpletest.rc-dxn9d" in namespace "gc-7893"
Aug 25 03:10:20.249: INFO: Deleting pod "simpletest.rc-ftvb2" in namespace "gc-7893"
Aug 25 03:10:20.256: INFO: Deleting pod "simpletest.rc-gqk26" in namespace "gc-7893"
Aug 25 03:10:20.264: INFO: Deleting pod "simpletest.rc-hpn5f" in namespace "gc-7893"
Aug 25 03:10:20.270: INFO: Deleting pod "simpletest.rc-hw24h" in namespace "gc-7893"
Aug 25 03:10:20.276: INFO: Deleting pod "simpletest.rc-hxt2q" in namespace "gc-7893"
Aug 25 03:10:20.284: INFO: Deleting pod "simpletest.rc-jj4db" in namespace "gc-7893"
Aug 25 03:10:20.291: INFO: Deleting pod "simpletest.rc-k6skp" in namespace "gc-7893"
Aug 25 03:10:20.345: INFO: Deleting pod "simpletest.rc-ks6lc" in namespace "gc-7893"
Aug 25 03:10:20.447: INFO: Deleting pod "simpletest.rc-lb675" in namespace "gc-7893"
Aug 25 03:10:20.454: INFO: Deleting pod "simpletest.rc-lfch5" in namespace "gc-7893"
Aug 25 03:10:20.460: INFO: Deleting pod "simpletest.rc-ljjdb" in namespace "gc-7893"
Aug 25 03:10:20.465: INFO: Deleting pod "simpletest.rc-lsr9m" in namespace "gc-7893"
Aug 25 03:10:20.476: INFO: Deleting pod "simpletest.rc-m47tb" in namespace "gc-7893"
Aug 25 03:10:20.484: INFO: Deleting pod "simpletest.rc-n5lpb" in namespace "gc-7893"
Aug 25 03:10:20.490: INFO: Deleting pod "simpletest.rc-ngmnc" in namespace "gc-7893"
Aug 25 03:10:20.496: INFO: Deleting pod "simpletest.rc-nkvp5" in namespace "gc-7893"
Aug 25 03:10:20.507: INFO: Deleting pod "simpletest.rc-nlrkn" in namespace "gc-7893"
Aug 25 03:10:20.521: INFO: Deleting pod "simpletest.rc-npcds" in namespace "gc-7893"
Aug 25 03:10:20.527: INFO: Deleting pod "simpletest.rc-p4mvb" in namespace "gc-7893"
Aug 25 03:10:20.532: INFO: Deleting pod "simpletest.rc-pmbl2" in namespace "gc-7893"
Aug 25 03:10:20.538: INFO: Deleting pod "simpletest.rc-ptgkw" in namespace "gc-7893"
Aug 25 03:10:20.543: INFO: Deleting pod "simpletest.rc-ptqd6" in namespace "gc-7893"
Aug 25 03:10:20.552: INFO: Deleting pod "simpletest.rc-pwdrb" in namespace "gc-7893"
Aug 25 03:10:20.558: INFO: Deleting pod "simpletest.rc-r99dz" in namespace "gc-7893"
Aug 25 03:10:20.565: INFO: Deleting pod "simpletest.rc-s56kq" in namespace "gc-7893"
Aug 25 03:10:20.570: INFO: Deleting pod "simpletest.rc-sddr7" in namespace "gc-7893"
Aug 25 03:10:20.579: INFO: Deleting pod "simpletest.rc-tp6rq" in namespace "gc-7893"
Aug 25 03:10:20.586: INFO: Deleting pod "simpletest.rc-wlvp7" in namespace "gc-7893"
Aug 25 03:10:20.591: INFO: Deleting pod "simpletest.rc-wt56f" in namespace "gc-7893"
Aug 25 03:10:20.602: INFO: Deleting pod "simpletest.rc-x9ll8" in namespace "gc-7893"
Aug 25 03:10:20.608: INFO: Deleting pod "simpletest.rc-xpx2q" in namespace "gc-7893"
Aug 25 03:10:20.620: INFO: Deleting pod "simpletest.rc-xrznm" in namespace "gc-7893"
Aug 25 03:10:20.627: INFO: Deleting pod "simpletest.rc-z5srk" in namespace "gc-7893"
Aug 25 03:10:20.633: INFO: Deleting pod "simpletest.rc-z67pm" in namespace "gc-7893"
Aug 25 03:10:20.639: INFO: Deleting pod "simpletest.rc-z98g4" in namespace "gc-7893"
Aug 25 03:10:20.645: INFO: Deleting pod "simpletest.rc-zbd2b" in namespace "gc-7893"
Aug 25 03:10:20.650: INFO: Deleting pod "simpletest.rc-zbh5s" in namespace "gc-7893"
Aug 25 03:10:20.656: INFO: Deleting pod "simpletest.rc-zsrt6" in namespace "gc-7893"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 03:10:20.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7893" for this suite. 08/25/22 03:10:20.672
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":62,"skipped":1047,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [40.705 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:09:39.969
    Aug 25 03:09:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 03:09:39.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:09:39.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:09:39.984
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/25/22 03:09:39.993
    STEP: delete the rc 08/25/22 03:09:45.005
    STEP: wait for the rc to be deleted 08/25/22 03:09:45.009
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/25/22 03:09:50.013
    STEP: Gathering metrics 08/25/22 03:10:20.04
    Aug 25 03:10:20.055: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
    Aug 25 03:10:20.059: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.291888ms
    Aug 25 03:10:20.059: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
    Aug 25 03:10:20.059: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
    Aug 25 03:10:20.151: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 25 03:10:20.151: INFO: Deleting pod "simpletest.rc-4jb6l" in namespace "gc-7893"
    Aug 25 03:10:20.158: INFO: Deleting pod "simpletest.rc-64mvt" in namespace "gc-7893"
    Aug 25 03:10:20.164: INFO: Deleting pod "simpletest.rc-7dlhv" in namespace "gc-7893"
    Aug 25 03:10:20.171: INFO: Deleting pod "simpletest.rc-7tqpw" in namespace "gc-7893"
    Aug 25 03:10:20.178: INFO: Deleting pod "simpletest.rc-8dzbm" in namespace "gc-7893"
    Aug 25 03:10:20.183: INFO: Deleting pod "simpletest.rc-9lzrc" in namespace "gc-7893"
    Aug 25 03:10:20.188: INFO: Deleting pod "simpletest.rc-9t6d9" in namespace "gc-7893"
    Aug 25 03:10:20.193: INFO: Deleting pod "simpletest.rc-9zm4q" in namespace "gc-7893"
    Aug 25 03:10:20.200: INFO: Deleting pod "simpletest.rc-b4lnv" in namespace "gc-7893"
    Aug 25 03:10:20.206: INFO: Deleting pod "simpletest.rc-b9tl7" in namespace "gc-7893"
    Aug 25 03:10:20.213: INFO: Deleting pod "simpletest.rc-bflvx" in namespace "gc-7893"
    Aug 25 03:10:20.217: INFO: Deleting pod "simpletest.rc-bxjk6" in namespace "gc-7893"
    Aug 25 03:10:20.222: INFO: Deleting pod "simpletest.rc-c6wgt" in namespace "gc-7893"
    Aug 25 03:10:20.226: INFO: Deleting pod "simpletest.rc-dcswp" in namespace "gc-7893"
    Aug 25 03:10:20.231: INFO: Deleting pod "simpletest.rc-djljk" in namespace "gc-7893"
    Aug 25 03:10:20.236: INFO: Deleting pod "simpletest.rc-dqcqn" in namespace "gc-7893"
    Aug 25 03:10:20.242: INFO: Deleting pod "simpletest.rc-dxn9d" in namespace "gc-7893"
    Aug 25 03:10:20.249: INFO: Deleting pod "simpletest.rc-ftvb2" in namespace "gc-7893"
    Aug 25 03:10:20.256: INFO: Deleting pod "simpletest.rc-gqk26" in namespace "gc-7893"
    Aug 25 03:10:20.264: INFO: Deleting pod "simpletest.rc-hpn5f" in namespace "gc-7893"
    Aug 25 03:10:20.270: INFO: Deleting pod "simpletest.rc-hw24h" in namespace "gc-7893"
    Aug 25 03:10:20.276: INFO: Deleting pod "simpletest.rc-hxt2q" in namespace "gc-7893"
    Aug 25 03:10:20.284: INFO: Deleting pod "simpletest.rc-jj4db" in namespace "gc-7893"
    Aug 25 03:10:20.291: INFO: Deleting pod "simpletest.rc-k6skp" in namespace "gc-7893"
    Aug 25 03:10:20.345: INFO: Deleting pod "simpletest.rc-ks6lc" in namespace "gc-7893"
    Aug 25 03:10:20.447: INFO: Deleting pod "simpletest.rc-lb675" in namespace "gc-7893"
    Aug 25 03:10:20.454: INFO: Deleting pod "simpletest.rc-lfch5" in namespace "gc-7893"
    Aug 25 03:10:20.460: INFO: Deleting pod "simpletest.rc-ljjdb" in namespace "gc-7893"
    Aug 25 03:10:20.465: INFO: Deleting pod "simpletest.rc-lsr9m" in namespace "gc-7893"
    Aug 25 03:10:20.476: INFO: Deleting pod "simpletest.rc-m47tb" in namespace "gc-7893"
    Aug 25 03:10:20.484: INFO: Deleting pod "simpletest.rc-n5lpb" in namespace "gc-7893"
    Aug 25 03:10:20.490: INFO: Deleting pod "simpletest.rc-ngmnc" in namespace "gc-7893"
    Aug 25 03:10:20.496: INFO: Deleting pod "simpletest.rc-nkvp5" in namespace "gc-7893"
    Aug 25 03:10:20.507: INFO: Deleting pod "simpletest.rc-nlrkn" in namespace "gc-7893"
    Aug 25 03:10:20.521: INFO: Deleting pod "simpletest.rc-npcds" in namespace "gc-7893"
    Aug 25 03:10:20.527: INFO: Deleting pod "simpletest.rc-p4mvb" in namespace "gc-7893"
    Aug 25 03:10:20.532: INFO: Deleting pod "simpletest.rc-pmbl2" in namespace "gc-7893"
    Aug 25 03:10:20.538: INFO: Deleting pod "simpletest.rc-ptgkw" in namespace "gc-7893"
    Aug 25 03:10:20.543: INFO: Deleting pod "simpletest.rc-ptqd6" in namespace "gc-7893"
    Aug 25 03:10:20.552: INFO: Deleting pod "simpletest.rc-pwdrb" in namespace "gc-7893"
    Aug 25 03:10:20.558: INFO: Deleting pod "simpletest.rc-r99dz" in namespace "gc-7893"
    Aug 25 03:10:20.565: INFO: Deleting pod "simpletest.rc-s56kq" in namespace "gc-7893"
    Aug 25 03:10:20.570: INFO: Deleting pod "simpletest.rc-sddr7" in namespace "gc-7893"
    Aug 25 03:10:20.579: INFO: Deleting pod "simpletest.rc-tp6rq" in namespace "gc-7893"
    Aug 25 03:10:20.586: INFO: Deleting pod "simpletest.rc-wlvp7" in namespace "gc-7893"
    Aug 25 03:10:20.591: INFO: Deleting pod "simpletest.rc-wt56f" in namespace "gc-7893"
    Aug 25 03:10:20.602: INFO: Deleting pod "simpletest.rc-x9ll8" in namespace "gc-7893"
    Aug 25 03:10:20.608: INFO: Deleting pod "simpletest.rc-xpx2q" in namespace "gc-7893"
    Aug 25 03:10:20.620: INFO: Deleting pod "simpletest.rc-xrznm" in namespace "gc-7893"
    Aug 25 03:10:20.627: INFO: Deleting pod "simpletest.rc-z5srk" in namespace "gc-7893"
    Aug 25 03:10:20.633: INFO: Deleting pod "simpletest.rc-z67pm" in namespace "gc-7893"
    Aug 25 03:10:20.639: INFO: Deleting pod "simpletest.rc-z98g4" in namespace "gc-7893"
    Aug 25 03:10:20.645: INFO: Deleting pod "simpletest.rc-zbd2b" in namespace "gc-7893"
    Aug 25 03:10:20.650: INFO: Deleting pod "simpletest.rc-zbh5s" in namespace "gc-7893"
    Aug 25 03:10:20.656: INFO: Deleting pod "simpletest.rc-zsrt6" in namespace "gc-7893"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 03:10:20.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7893" for this suite. 08/25/22 03:10:20.672
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:10:20.675
Aug 25 03:10:20.675: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename init-container 08/25/22 03:10:20.676
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:10:20.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:10:20.688
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 08/25/22 03:10:20.691
Aug 25 03:10:20.691: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 03:10:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9695" for this suite. 08/25/22 03:10:32.377
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":63,"skipped":1059,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [11.706 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:10:20.675
    Aug 25 03:10:20.675: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename init-container 08/25/22 03:10:20.676
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:10:20.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:10:20.688
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 08/25/22 03:10:20.691
    Aug 25 03:10:20.691: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 03:10:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9695" for this suite. 08/25/22 03:10:32.377
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:10:32.383
Aug 25 03:10:32.383: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-preemption 08/25/22 03:10:32.385
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:10:32.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:10:32.4
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 25 03:10:32.413: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 25 03:11:32.514: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 08/25/22 03:11:32.517
Aug 25 03:11:32.532: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 25 03:11:32.536: INFO: Created pod: pod0-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/25/22 03:11:32.536
Aug 25 03:11:32.536: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1960" to be "running"
Aug 25 03:11:32.539: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474196ms
Aug 25 03:11:34.545: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008712331s
Aug 25 03:11:36.544: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007579995s
Aug 25 03:11:38.551: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014639161s
Aug 25 03:11:40.544: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007641258s
Aug 25 03:11:42.544: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.007591436s
Aug 25 03:11:42.544: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 25 03:11:42.544: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1960" to be "running"
Aug 25 03:11:42.547: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.3171ms
Aug 25 03:11:42.547: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/25/22 03:11:42.547
Aug 25 03:11:42.555: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 25 03:11:42.558: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622776ms
Aug 25 03:11:44.563: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007823752s
Aug 25 03:11:46.563: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007661496s
Aug 25 03:11:46.563: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:11:46.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1960" for this suite. 08/25/22 03:11:46.591
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":64,"skipped":1076,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [74.238 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:10:32.383
    Aug 25 03:10:32.383: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-preemption 08/25/22 03:10:32.385
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:10:32.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:10:32.4
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 25 03:10:32.413: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 25 03:11:32.514: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 08/25/22 03:11:32.517
    Aug 25 03:11:32.532: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 25 03:11:32.536: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/25/22 03:11:32.536
    Aug 25 03:11:32.536: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1960" to be "running"
    Aug 25 03:11:32.539: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474196ms
    Aug 25 03:11:34.545: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008712331s
    Aug 25 03:11:36.544: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007579995s
    Aug 25 03:11:38.551: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014639161s
    Aug 25 03:11:40.544: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007641258s
    Aug 25 03:11:42.544: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.007591436s
    Aug 25 03:11:42.544: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 25 03:11:42.544: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1960" to be "running"
    Aug 25 03:11:42.547: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.3171ms
    Aug 25 03:11:42.547: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/25/22 03:11:42.547
    Aug 25 03:11:42.555: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 25 03:11:42.558: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622776ms
    Aug 25 03:11:44.563: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007823752s
    Aug 25 03:11:46.563: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007661496s
    Aug 25 03:11:46.563: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:11:46.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1960" for this suite. 08/25/22 03:11:46.591
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:11:46.622
Aug 25 03:11:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 03:11:46.623
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:11:46.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:11:46.64
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-e0719eac-8206-4710-9dd5-d1a79ba7592b 08/25/22 03:11:46.646
STEP: Creating a pod to test consume configMaps 08/25/22 03:11:46.65
Aug 25 03:11:46.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7" in namespace "configmap-409" to be "Succeeded or Failed"
Aug 25 03:11:46.660: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632105ms
Aug 25 03:11:48.666: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00943854s
Aug 25 03:11:50.665: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008555931s
STEP: Saw pod success 08/25/22 03:11:50.665
Aug 25 03:11:50.665: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7" satisfied condition "Succeeded or Failed"
Aug 25 03:11:50.669: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:11:50.69
Aug 25 03:11:50.697: INFO: Waiting for pod pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7 to disappear
Aug 25 03:11:50.701: INFO: Pod pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 03:11:50.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-409" for this suite. 08/25/22 03:11:50.704
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":65,"skipped":1083,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:11:46.622
    Aug 25 03:11:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 03:11:46.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:11:46.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:11:46.64
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-e0719eac-8206-4710-9dd5-d1a79ba7592b 08/25/22 03:11:46.646
    STEP: Creating a pod to test consume configMaps 08/25/22 03:11:46.65
    Aug 25 03:11:46.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7" in namespace "configmap-409" to be "Succeeded or Failed"
    Aug 25 03:11:46.660: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632105ms
    Aug 25 03:11:48.666: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00943854s
    Aug 25 03:11:50.665: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008555931s
    STEP: Saw pod success 08/25/22 03:11:50.665
    Aug 25 03:11:50.665: INFO: Pod "pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7" satisfied condition "Succeeded or Failed"
    Aug 25 03:11:50.669: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:11:50.69
    Aug 25 03:11:50.697: INFO: Waiting for pod pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7 to disappear
    Aug 25 03:11:50.701: INFO: Pod pod-configmaps-43c3eac8-1171-4da7-a091-0df56043a7f7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 03:11:50.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-409" for this suite. 08/25/22 03:11:50.704
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:11:50.711
Aug 25 03:11:50.711: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:11:50.712
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:11:50.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:11:50.731
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:11:50.734
Aug 25 03:11:50.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a" in namespace "downward-api-9749" to be "Succeeded or Failed"
Aug 25 03:11:50.743: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37103ms
Aug 25 03:11:52.749: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008015042s
Aug 25 03:11:54.750: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009223932s
STEP: Saw pod success 08/25/22 03:11:54.75
Aug 25 03:11:54.750: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a" satisfied condition "Succeeded or Failed"
Aug 25 03:11:54.754: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a container client-container: <nil>
STEP: delete the pod 08/25/22 03:11:54.76
Aug 25 03:11:54.767: INFO: Waiting for pod downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a to disappear
Aug 25 03:11:54.770: INFO: Pod downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 03:11:54.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9749" for this suite. 08/25/22 03:11:54.773
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":66,"skipped":1107,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:11:50.711
    Aug 25 03:11:50.711: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:11:50.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:11:50.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:11:50.731
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:11:50.734
    Aug 25 03:11:50.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a" in namespace "downward-api-9749" to be "Succeeded or Failed"
    Aug 25 03:11:50.743: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37103ms
    Aug 25 03:11:52.749: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008015042s
    Aug 25 03:11:54.750: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009223932s
    STEP: Saw pod success 08/25/22 03:11:54.75
    Aug 25 03:11:54.750: INFO: Pod "downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a" satisfied condition "Succeeded or Failed"
    Aug 25 03:11:54.754: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a container client-container: <nil>
    STEP: delete the pod 08/25/22 03:11:54.76
    Aug 25 03:11:54.767: INFO: Waiting for pod downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a to disappear
    Aug 25 03:11:54.770: INFO: Pod downwardapi-volume-f1fbd667-15be-449e-9186-464435b0814a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 03:11:54.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9749" for this suite. 08/25/22 03:11:54.773
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:11:54.779
Aug 25 03:11:54.779: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replication-controller 08/25/22 03:11:54.781
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:11:54.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:11:54.797
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882 08/25/22 03:11:54.802
Aug 25 03:11:54.811: INFO: Pod name my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882: Found 0 pods out of 1
Aug 25 03:11:59.814: INFO: Pod name my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882: Found 1 pods out of 1
Aug 25 03:11:59.814: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882" are running
Aug 25 03:11:59.814: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt" in namespace "replication-controller-8824" to be "running"
Aug 25 03:11:59.816: INFO: Pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt": Phase="Running", Reason="", readiness=true. Elapsed: 2.068039ms
Aug 25 03:11:59.816: INFO: Pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt" satisfied condition "running"
Aug 25 03:11:59.816: INFO: Pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:54 +0000 UTC Reason: Message:}])
Aug 25 03:11:59.816: INFO: Trying to dial the pod
Aug 25 03:12:04.829: INFO: Controller my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882: Got expected result from replica 1 [my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt]: "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 25 03:12:04.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8824" for this suite. 08/25/22 03:12:04.832
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":67,"skipped":1116,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.058 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:11:54.779
    Aug 25 03:11:54.779: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replication-controller 08/25/22 03:11:54.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:11:54.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:11:54.797
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882 08/25/22 03:11:54.802
    Aug 25 03:11:54.811: INFO: Pod name my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882: Found 0 pods out of 1
    Aug 25 03:11:59.814: INFO: Pod name my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882: Found 1 pods out of 1
    Aug 25 03:11:59.814: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882" are running
    Aug 25 03:11:59.814: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt" in namespace "replication-controller-8824" to be "running"
    Aug 25 03:11:59.816: INFO: Pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt": Phase="Running", Reason="", readiness=true. Elapsed: 2.068039ms
    Aug 25 03:11:59.816: INFO: Pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt" satisfied condition "running"
    Aug 25 03:11:59.816: INFO: Pod "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:11:54 +0000 UTC Reason: Message:}])
    Aug 25 03:11:59.816: INFO: Trying to dial the pod
    Aug 25 03:12:04.829: INFO: Controller my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882: Got expected result from replica 1 [my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt]: "my-hostname-basic-b6dcbf67-2283-472d-b4e3-3cc7d5ac9882-xlmpt", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 25 03:12:04.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8824" for this suite. 08/25/22 03:12:04.832
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:12:04.838
Aug 25 03:12:04.838: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename subpath 08/25/22 03:12:04.84
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:04.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:04.858
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/25/22 03:12:04.862
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-zndp 08/25/22 03:12:04.869
STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:12:04.869
Aug 25 03:12:04.875: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zndp" in namespace "subpath-2396" to be "Succeeded or Failed"
Aug 25 03:12:04.878: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731326ms
Aug 25 03:12:06.885: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010053211s
Aug 25 03:12:08.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 4.008461459s
Aug 25 03:12:10.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 6.007942866s
Aug 25 03:12:12.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 8.008424677s
Aug 25 03:12:14.884: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008853606s
Aug 25 03:12:16.884: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008789906s
Aug 25 03:12:18.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 14.007906977s
Aug 25 03:12:20.882: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 16.007449182s
Aug 25 03:12:22.884: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 18.008806488s
Aug 25 03:12:24.885: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 20.009886336s
Aug 25 03:12:26.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=false. Elapsed: 22.008123559s
Aug 25 03:12:28.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008392038s
STEP: Saw pod success 08/25/22 03:12:28.883
Aug 25 03:12:28.884: INFO: Pod "pod-subpath-test-downwardapi-zndp" satisfied condition "Succeeded or Failed"
Aug 25 03:12:28.888: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-downwardapi-zndp container test-container-subpath-downwardapi-zndp: <nil>
STEP: delete the pod 08/25/22 03:12:28.894
Aug 25 03:12:28.902: INFO: Waiting for pod pod-subpath-test-downwardapi-zndp to disappear
Aug 25 03:12:28.905: INFO: Pod pod-subpath-test-downwardapi-zndp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zndp 08/25/22 03:12:28.905
Aug 25 03:12:28.905: INFO: Deleting pod "pod-subpath-test-downwardapi-zndp" in namespace "subpath-2396"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 25 03:12:28.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2396" for this suite. 08/25/22 03:12:28.912
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":68,"skipped":1134,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [24.077 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:12:04.838
    Aug 25 03:12:04.838: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename subpath 08/25/22 03:12:04.84
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:04.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:04.858
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/25/22 03:12:04.862
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-zndp 08/25/22 03:12:04.869
    STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:12:04.869
    Aug 25 03:12:04.875: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zndp" in namespace "subpath-2396" to be "Succeeded or Failed"
    Aug 25 03:12:04.878: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731326ms
    Aug 25 03:12:06.885: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010053211s
    Aug 25 03:12:08.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 4.008461459s
    Aug 25 03:12:10.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 6.007942866s
    Aug 25 03:12:12.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 8.008424677s
    Aug 25 03:12:14.884: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008853606s
    Aug 25 03:12:16.884: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008789906s
    Aug 25 03:12:18.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 14.007906977s
    Aug 25 03:12:20.882: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 16.007449182s
    Aug 25 03:12:22.884: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 18.008806488s
    Aug 25 03:12:24.885: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=true. Elapsed: 20.009886336s
    Aug 25 03:12:26.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Running", Reason="", readiness=false. Elapsed: 22.008123559s
    Aug 25 03:12:28.883: INFO: Pod "pod-subpath-test-downwardapi-zndp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008392038s
    STEP: Saw pod success 08/25/22 03:12:28.883
    Aug 25 03:12:28.884: INFO: Pod "pod-subpath-test-downwardapi-zndp" satisfied condition "Succeeded or Failed"
    Aug 25 03:12:28.888: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-downwardapi-zndp container test-container-subpath-downwardapi-zndp: <nil>
    STEP: delete the pod 08/25/22 03:12:28.894
    Aug 25 03:12:28.902: INFO: Waiting for pod pod-subpath-test-downwardapi-zndp to disappear
    Aug 25 03:12:28.905: INFO: Pod pod-subpath-test-downwardapi-zndp no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-zndp 08/25/22 03:12:28.905
    Aug 25 03:12:28.905: INFO: Deleting pod "pod-subpath-test-downwardapi-zndp" in namespace "subpath-2396"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 25 03:12:28.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2396" for this suite. 08/25/22 03:12:28.912
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:12:28.917
Aug 25 03:12:28.917: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename subpath 08/25/22 03:12:28.918
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:28.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:28.934
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/25/22 03:12:28.938
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-2qdc 08/25/22 03:12:28.944
STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:12:28.944
Aug 25 03:12:28.950: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2qdc" in namespace "subpath-5520" to be "Succeeded or Failed"
Aug 25 03:12:28.953: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795877ms
Aug 25 03:12:30.957: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007396521s
Aug 25 03:12:32.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007641981s
Aug 25 03:12:34.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 6.009043674s
Aug 25 03:12:36.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 8.008778177s
Aug 25 03:12:38.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 10.007677446s
Aug 25 03:12:40.956: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 12.006407552s
Aug 25 03:12:42.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 14.008799791s
Aug 25 03:12:44.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 16.009174601s
Aug 25 03:12:46.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 18.008007356s
Aug 25 03:12:48.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 20.007624989s
Aug 25 03:12:50.957: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=false. Elapsed: 22.006546665s
Aug 25 03:12:52.957: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007297839s
STEP: Saw pod success 08/25/22 03:12:52.957
Aug 25 03:12:52.958: INFO: Pod "pod-subpath-test-configmap-2qdc" satisfied condition "Succeeded or Failed"
Aug 25 03:12:52.962: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-configmap-2qdc container test-container-subpath-configmap-2qdc: <nil>
STEP: delete the pod 08/25/22 03:12:52.967
Aug 25 03:12:52.974: INFO: Waiting for pod pod-subpath-test-configmap-2qdc to disappear
Aug 25 03:12:52.977: INFO: Pod pod-subpath-test-configmap-2qdc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2qdc 08/25/22 03:12:52.977
Aug 25 03:12:52.977: INFO: Deleting pod "pod-subpath-test-configmap-2qdc" in namespace "subpath-5520"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 25 03:12:52.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5520" for this suite. 08/25/22 03:12:52.984
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":69,"skipped":1152,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [24.071 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:12:28.917
    Aug 25 03:12:28.917: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename subpath 08/25/22 03:12:28.918
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:28.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:28.934
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/25/22 03:12:28.938
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-2qdc 08/25/22 03:12:28.944
    STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:12:28.944
    Aug 25 03:12:28.950: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2qdc" in namespace "subpath-5520" to be "Succeeded or Failed"
    Aug 25 03:12:28.953: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795877ms
    Aug 25 03:12:30.957: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007396521s
    Aug 25 03:12:32.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007641981s
    Aug 25 03:12:34.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 6.009043674s
    Aug 25 03:12:36.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 8.008778177s
    Aug 25 03:12:38.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 10.007677446s
    Aug 25 03:12:40.956: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 12.006407552s
    Aug 25 03:12:42.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 14.008799791s
    Aug 25 03:12:44.959: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 16.009174601s
    Aug 25 03:12:46.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 18.008007356s
    Aug 25 03:12:48.958: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=true. Elapsed: 20.007624989s
    Aug 25 03:12:50.957: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Running", Reason="", readiness=false. Elapsed: 22.006546665s
    Aug 25 03:12:52.957: INFO: Pod "pod-subpath-test-configmap-2qdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007297839s
    STEP: Saw pod success 08/25/22 03:12:52.957
    Aug 25 03:12:52.958: INFO: Pod "pod-subpath-test-configmap-2qdc" satisfied condition "Succeeded or Failed"
    Aug 25 03:12:52.962: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-configmap-2qdc container test-container-subpath-configmap-2qdc: <nil>
    STEP: delete the pod 08/25/22 03:12:52.967
    Aug 25 03:12:52.974: INFO: Waiting for pod pod-subpath-test-configmap-2qdc to disappear
    Aug 25 03:12:52.977: INFO: Pod pod-subpath-test-configmap-2qdc no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-2qdc 08/25/22 03:12:52.977
    Aug 25 03:12:52.977: INFO: Deleting pod "pod-subpath-test-configmap-2qdc" in namespace "subpath-5520"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 25 03:12:52.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5520" for this suite. 08/25/22 03:12:52.984
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:12:52.99
Aug 25 03:12:52.990: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename containers 08/25/22 03:12:52.991
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:53.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:53.006
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 08/25/22 03:12:53.01
Aug 25 03:12:53.016: INFO: Waiting up to 5m0s for pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69" in namespace "containers-9764" to be "Succeeded or Failed"
Aug 25 03:12:53.020: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35488ms
Aug 25 03:12:55.026: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009333909s
Aug 25 03:12:57.025: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009026228s
STEP: Saw pod success 08/25/22 03:12:57.025
Aug 25 03:12:57.026: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69" satisfied condition "Succeeded or Failed"
Aug 25 03:12:57.030: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-containers-e63228fb-3d28-407b-98f2-5930f095bd69 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:12:57.035
Aug 25 03:12:57.043: INFO: Waiting for pod client-containers-e63228fb-3d28-407b-98f2-5930f095bd69 to disappear
Aug 25 03:12:57.047: INFO: Pod client-containers-e63228fb-3d28-407b-98f2-5930f095bd69 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 25 03:12:57.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9764" for this suite. 08/25/22 03:12:57.051
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":70,"skipped":1183,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:12:52.99
    Aug 25 03:12:52.990: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename containers 08/25/22 03:12:52.991
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:53.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:53.006
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 08/25/22 03:12:53.01
    Aug 25 03:12:53.016: INFO: Waiting up to 5m0s for pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69" in namespace "containers-9764" to be "Succeeded or Failed"
    Aug 25 03:12:53.020: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35488ms
    Aug 25 03:12:55.026: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009333909s
    Aug 25 03:12:57.025: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009026228s
    STEP: Saw pod success 08/25/22 03:12:57.025
    Aug 25 03:12:57.026: INFO: Pod "client-containers-e63228fb-3d28-407b-98f2-5930f095bd69" satisfied condition "Succeeded or Failed"
    Aug 25 03:12:57.030: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-containers-e63228fb-3d28-407b-98f2-5930f095bd69 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:12:57.035
    Aug 25 03:12:57.043: INFO: Waiting for pod client-containers-e63228fb-3d28-407b-98f2-5930f095bd69 to disappear
    Aug 25 03:12:57.047: INFO: Pod client-containers-e63228fb-3d28-407b-98f2-5930f095bd69 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 25 03:12:57.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9764" for this suite. 08/25/22 03:12:57.051
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:12:57.059
Aug 25 03:12:57.059: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename job 08/25/22 03:12:57.06
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:57.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:57.074
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 08/25/22 03:12:57.078
STEP: Ensure pods equal to paralellism count is attached to the job 08/25/22 03:12:57.082
STEP: patching /status 08/25/22 03:12:59.089
STEP: updating /status 08/25/22 03:12:59.097
STEP: get /status 08/25/22 03:12:59.105
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 25 03:12:59.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9967" for this suite. 08/25/22 03:12:59.111
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":71,"skipped":1213,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.056 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:12:57.059
    Aug 25 03:12:57.059: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename job 08/25/22 03:12:57.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:57.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:57.074
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 08/25/22 03:12:57.078
    STEP: Ensure pods equal to paralellism count is attached to the job 08/25/22 03:12:57.082
    STEP: patching /status 08/25/22 03:12:59.089
    STEP: updating /status 08/25/22 03:12:59.097
    STEP: get /status 08/25/22 03:12:59.105
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 25 03:12:59.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9967" for this suite. 08/25/22 03:12:59.111
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:12:59.116
Aug 25 03:12:59.116: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:12:59.117
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:59.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:59.133
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:12:59.146
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:12:59.643
STEP: Deploying the webhook pod 08/25/22 03:12:59.65
STEP: Wait for the deployment to be ready 08/25/22 03:12:59.66
Aug 25 03:12:59.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:13:01.69
STEP: Verifying the service has paired with the endpoint 08/25/22 03:13:01.697
Aug 25 03:13:02.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 08/25/22 03:13:02.702
STEP: create a pod 08/25/22 03:13:02.724
Aug 25 03:13:02.729: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4196" to be "running"
Aug 25 03:13:02.733: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06145ms
Aug 25 03:13:04.739: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009751186s
Aug 25 03:13:04.739: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/25/22 03:13:04.739
Aug 25 03:13:04.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=webhook-4196 attach --namespace=webhook-4196 to-be-attached-pod -i -c=container1'
Aug 25 03:13:04.832: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:13:04.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4196" for this suite. 08/25/22 03:13:04.84
STEP: Destroying namespace "webhook-4196-markers" for this suite. 08/25/22 03:13:04.845
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":72,"skipped":1218,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:12:59.116
    Aug 25 03:12:59.116: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:12:59.117
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:12:59.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:12:59.133
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:12:59.146
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:12:59.643
    STEP: Deploying the webhook pod 08/25/22 03:12:59.65
    STEP: Wait for the deployment to be ready 08/25/22 03:12:59.66
    Aug 25 03:12:59.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:13:01.69
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:13:01.697
    Aug 25 03:13:02.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 08/25/22 03:13:02.702
    STEP: create a pod 08/25/22 03:13:02.724
    Aug 25 03:13:02.729: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4196" to be "running"
    Aug 25 03:13:02.733: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06145ms
    Aug 25 03:13:04.739: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009751186s
    Aug 25 03:13:04.739: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/25/22 03:13:04.739
    Aug 25 03:13:04.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=webhook-4196 attach --namespace=webhook-4196 to-be-attached-pod -i -c=container1'
    Aug 25 03:13:04.832: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:13:04.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4196" for this suite. 08/25/22 03:13:04.84
    STEP: Destroying namespace "webhook-4196-markers" for this suite. 08/25/22 03:13:04.845
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:13:04.876
Aug 25 03:13:04.877: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename events 08/25/22 03:13:04.878
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:04.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:04.892
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/25/22 03:13:04.896
STEP: get a list of Events with a label in the current namespace 08/25/22 03:13:04.907
STEP: delete a list of events 08/25/22 03:13:04.91
Aug 25 03:13:04.910: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/25/22 03:13:04.921
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Aug 25 03:13:04.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5475" for this suite. 08/25/22 03:13:04.927
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":73,"skipped":1233,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.055 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:13:04.876
    Aug 25 03:13:04.877: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename events 08/25/22 03:13:04.878
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:04.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:04.892
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/25/22 03:13:04.896
    STEP: get a list of Events with a label in the current namespace 08/25/22 03:13:04.907
    STEP: delete a list of events 08/25/22 03:13:04.91
    Aug 25 03:13:04.910: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/25/22 03:13:04.921
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Aug 25 03:13:04.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5475" for this suite. 08/25/22 03:13:04.927
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:13:04.931
Aug 25 03:13:04.932: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:13:04.933
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:04.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:04.984
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5775 08/25/22 03:13:04.99
STEP: changing the ExternalName service to type=NodePort 08/25/22 03:13:05.01
STEP: creating replication controller externalname-service in namespace services-5775 08/25/22 03:13:05.087
I0825 03:13:05.093907      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5775, replica count: 2
I0825 03:13:08.145852      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 03:13:08.145: INFO: Creating new exec pod
Aug 25 03:13:08.151: INFO: Waiting up to 5m0s for pod "execpod6r9b5" in namespace "services-5775" to be "running"
Aug 25 03:13:08.154: INFO: Pod "execpod6r9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.443686ms
Aug 25 03:13:10.158: INFO: Pod "execpod6r9b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006545239s
Aug 25 03:13:10.158: INFO: Pod "execpod6r9b5" satisfied condition "running"
Aug 25 03:13:11.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-5775 exec execpod6r9b5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 25 03:13:11.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 25 03:13:11.340: INFO: stdout: "externalname-service-f6kxf"
Aug 25 03:13:11.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-5775 exec execpod6r9b5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.30.80 80'
Aug 25 03:13:11.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.30.80 80\nConnection to 10.109.30.80 80 port [tcp/http] succeeded!\n"
Aug 25 03:13:11.524: INFO: stdout: "externalname-service-2xtp2"
Aug 25 03:13:11.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-5775 exec execpod6r9b5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 31169'
Aug 25 03:13:11.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 31169\nConnection to 86.109.11.217 31169 port [tcp/*] succeeded!\n"
Aug 25 03:13:11.705: INFO: stdout: "externalname-service-2xtp2"
Aug 25 03:13:11.705: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:13:11.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5775" for this suite. 08/25/22 03:13:11.722
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":74,"skipped":1233,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.794 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:13:04.931
    Aug 25 03:13:04.932: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:13:04.933
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:04.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:04.984
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5775 08/25/22 03:13:04.99
    STEP: changing the ExternalName service to type=NodePort 08/25/22 03:13:05.01
    STEP: creating replication controller externalname-service in namespace services-5775 08/25/22 03:13:05.087
    I0825 03:13:05.093907      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5775, replica count: 2
    I0825 03:13:08.145852      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 03:13:08.145: INFO: Creating new exec pod
    Aug 25 03:13:08.151: INFO: Waiting up to 5m0s for pod "execpod6r9b5" in namespace "services-5775" to be "running"
    Aug 25 03:13:08.154: INFO: Pod "execpod6r9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.443686ms
    Aug 25 03:13:10.158: INFO: Pod "execpod6r9b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006545239s
    Aug 25 03:13:10.158: INFO: Pod "execpod6r9b5" satisfied condition "running"
    Aug 25 03:13:11.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-5775 exec execpod6r9b5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 25 03:13:11.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 25 03:13:11.340: INFO: stdout: "externalname-service-f6kxf"
    Aug 25 03:13:11.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-5775 exec execpod6r9b5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.30.80 80'
    Aug 25 03:13:11.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.30.80 80\nConnection to 10.109.30.80 80 port [tcp/http] succeeded!\n"
    Aug 25 03:13:11.524: INFO: stdout: "externalname-service-2xtp2"
    Aug 25 03:13:11.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-5775 exec execpod6r9b5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 31169'
    Aug 25 03:13:11.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 31169\nConnection to 86.109.11.217 31169 port [tcp/*] succeeded!\n"
    Aug 25 03:13:11.705: INFO: stdout: "externalname-service-2xtp2"
    Aug 25 03:13:11.705: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:13:11.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5775" for this suite. 08/25/22 03:13:11.722
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:13:11.728
Aug 25 03:13:11.728: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:13:11.729
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:11.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:11.744
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 08/25/22 03:13:11.748
STEP: Creating a ResourceQuota 08/25/22 03:13:16.751
STEP: Ensuring resource quota status is calculated 08/25/22 03:13:16.754
STEP: Creating a Pod that fits quota 08/25/22 03:13:18.76
STEP: Ensuring ResourceQuota status captures the pod usage 08/25/22 03:13:18.775
STEP: Not allowing a pod to be created that exceeds remaining quota 08/25/22 03:13:20.78
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/25/22 03:13:20.784
STEP: Ensuring a pod cannot update its resource requirements 08/25/22 03:13:20.788
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/25/22 03:13:20.792
STEP: Deleting the pod 08/25/22 03:13:22.797
STEP: Ensuring resource quota status released the pod usage 08/25/22 03:13:22.804
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:13:24.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5255" for this suite. 08/25/22 03:13:24.815
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":75,"skipped":1285,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [13.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:13:11.728
    Aug 25 03:13:11.728: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:13:11.729
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:11.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:11.744
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 08/25/22 03:13:11.748
    STEP: Creating a ResourceQuota 08/25/22 03:13:16.751
    STEP: Ensuring resource quota status is calculated 08/25/22 03:13:16.754
    STEP: Creating a Pod that fits quota 08/25/22 03:13:18.76
    STEP: Ensuring ResourceQuota status captures the pod usage 08/25/22 03:13:18.775
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/25/22 03:13:20.78
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/25/22 03:13:20.784
    STEP: Ensuring a pod cannot update its resource requirements 08/25/22 03:13:20.788
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/25/22 03:13:20.792
    STEP: Deleting the pod 08/25/22 03:13:22.797
    STEP: Ensuring resource quota status released the pod usage 08/25/22 03:13:22.804
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:13:24.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5255" for this suite. 08/25/22 03:13:24.815
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:13:24.821
Aug 25 03:13:24.821: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 03:13:24.822
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:24.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:24.84
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/25/22 03:13:24.843
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_tcp@PTR;sleep 1; done
 08/25/22 03:13:24.855
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_tcp@PTR;sleep 1; done
 08/25/22 03:13:24.855
STEP: creating a pod to probe DNS 08/25/22 03:13:24.856
STEP: submitting the pod to kubernetes 08/25/22 03:13:24.856
Aug 25 03:13:24.863: INFO: Waiting up to 15m0s for pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840" in namespace "dns-2028" to be "running"
Aug 25 03:13:24.866: INFO: Pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85684ms
Aug 25 03:13:26.871: INFO: Pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840": Phase="Running", Reason="", readiness=true. Elapsed: 2.007807424s
Aug 25 03:13:26.871: INFO: Pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840" satisfied condition "running"
STEP: retrieving the pod 08/25/22 03:13:26.871
STEP: looking for the results for each expected name from probers 08/25/22 03:13:26.876
Aug 25 03:13:26.880: INFO: Unable to read wheezy_udp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.886: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.889: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.905: INFO: Unable to read jessie_udp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.908: INFO: Unable to read jessie_tcp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.911: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.914: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:26.927: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@dns-test-service.dns-2028.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@dns-test-service.dns-2028.svc.cluster.local jessie_tcp@dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

Aug 25 03:13:31.938: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:31.947: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:31.967: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:31.969: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:31.981: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

Aug 25 03:13:36.939: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:36.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:36.961: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:36.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:36.975: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

Aug 25 03:13:41.945: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:41.949: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:41.968: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:41.971: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:41.982: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

Aug 25 03:13:46.938: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:46.947: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:46.968: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:46.971: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:46.983: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

Aug 25 03:13:51.939: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:51.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:51.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:51.963: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
Aug 25 03:13:51.974: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

Aug 25 03:13:56.977: INFO: DNS probes using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 succeeded

STEP: deleting the pod 08/25/22 03:13:56.977
STEP: deleting the test service 08/25/22 03:13:56.986
STEP: deleting the test headless service 08/25/22 03:13:56.997
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 03:13:57.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2028" for this suite. 08/25/22 03:13:57.007
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":76,"skipped":1303,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [32.189 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:13:24.821
    Aug 25 03:13:24.821: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 03:13:24.822
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:24.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:24.84
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/25/22 03:13:24.843
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_tcp@PTR;sleep 1; done
     08/25/22 03:13:24.855
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2028.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2028.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2028.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.191.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.191.252_tcp@PTR;sleep 1; done
     08/25/22 03:13:24.855
    STEP: creating a pod to probe DNS 08/25/22 03:13:24.856
    STEP: submitting the pod to kubernetes 08/25/22 03:13:24.856
    Aug 25 03:13:24.863: INFO: Waiting up to 15m0s for pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840" in namespace "dns-2028" to be "running"
    Aug 25 03:13:24.866: INFO: Pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85684ms
    Aug 25 03:13:26.871: INFO: Pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840": Phase="Running", Reason="", readiness=true. Elapsed: 2.007807424s
    Aug 25 03:13:26.871: INFO: Pod "dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 03:13:26.871
    STEP: looking for the results for each expected name from probers 08/25/22 03:13:26.876
    Aug 25 03:13:26.880: INFO: Unable to read wheezy_udp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.886: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.889: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.905: INFO: Unable to read jessie_udp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.908: INFO: Unable to read jessie_tcp@dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.911: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.914: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:26.927: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@dns-test-service.dns-2028.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@dns-test-service.dns-2028.svc.cluster.local jessie_tcp@dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

    Aug 25 03:13:31.938: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:31.947: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:31.967: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:31.969: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:31.981: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

    Aug 25 03:13:36.939: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:36.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:36.961: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:36.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:36.975: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

    Aug 25 03:13:41.945: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:41.949: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:41.968: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:41.971: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:41.982: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

    Aug 25 03:13:46.938: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:46.947: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:46.968: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:46.971: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:46.983: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

    Aug 25 03:13:51.939: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:51.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:51.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:51.963: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local from pod dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840: the server could not find the requested resource (get pods dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840)
    Aug 25 03:13:51.974: INFO: Lookups using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2028.svc.cluster.local]

    Aug 25 03:13:56.977: INFO: DNS probes using dns-2028/dns-test-95ab2b8b-a15e-4d25-99c0-f7c03664c840 succeeded

    STEP: deleting the pod 08/25/22 03:13:56.977
    STEP: deleting the test service 08/25/22 03:13:56.986
    STEP: deleting the test headless service 08/25/22 03:13:56.997
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 03:13:57.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2028" for this suite. 08/25/22 03:13:57.007
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:13:57.011
Aug 25 03:13:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:13:57.012
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:57.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:57.024
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/25/22 03:13:57.027
Aug 25 03:13:57.034: INFO: Waiting up to 5m0s for pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5" in namespace "emptydir-6223" to be "Succeeded or Failed"
Aug 25 03:13:57.042: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.354319ms
Aug 25 03:13:59.048: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013652351s
Aug 25 03:14:01.047: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013141078s
STEP: Saw pod success 08/25/22 03:14:01.047
Aug 25 03:14:01.047: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5" satisfied condition "Succeeded or Failed"
Aug 25 03:14:01.052: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5 container test-container: <nil>
STEP: delete the pod 08/25/22 03:14:01.057
Aug 25 03:14:01.065: INFO: Waiting for pod pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5 to disappear
Aug 25 03:14:01.068: INFO: Pod pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:14:01.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6223" for this suite. 08/25/22 03:14:01.073
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":77,"skipped":1311,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:13:57.011
    Aug 25 03:13:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:13:57.012
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:13:57.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:13:57.024
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/25/22 03:13:57.027
    Aug 25 03:13:57.034: INFO: Waiting up to 5m0s for pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5" in namespace "emptydir-6223" to be "Succeeded or Failed"
    Aug 25 03:13:57.042: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.354319ms
    Aug 25 03:13:59.048: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013652351s
    Aug 25 03:14:01.047: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013141078s
    STEP: Saw pod success 08/25/22 03:14:01.047
    Aug 25 03:14:01.047: INFO: Pod "pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5" satisfied condition "Succeeded or Failed"
    Aug 25 03:14:01.052: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5 container test-container: <nil>
    STEP: delete the pod 08/25/22 03:14:01.057
    Aug 25 03:14:01.065: INFO: Waiting for pod pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5 to disappear
    Aug 25 03:14:01.068: INFO: Pod pod-db7cea0f-75fb-4489-b7bd-c448c9cb7cc5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:14:01.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6223" for this suite. 08/25/22 03:14:01.073
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:01.079
Aug 25 03:14:01.079: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:01.08
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:01.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:01.098
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 08/25/22 03:14:01.102
STEP: Getting a ResourceQuota 08/25/22 03:14:01.106
STEP: Updating a ResourceQuota 08/25/22 03:14:01.109
STEP: Verifying a ResourceQuota was modified 08/25/22 03:14:01.114
STEP: Deleting a ResourceQuota 08/25/22 03:14:01.116
STEP: Verifying the deleted ResourceQuota 08/25/22 03:14:01.12
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:14:01.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5894" for this suite. 08/25/22 03:14:01.126
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":78,"skipped":1337,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:01.079
    Aug 25 03:14:01.079: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:01.08
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:01.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:01.098
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 08/25/22 03:14:01.102
    STEP: Getting a ResourceQuota 08/25/22 03:14:01.106
    STEP: Updating a ResourceQuota 08/25/22 03:14:01.109
    STEP: Verifying a ResourceQuota was modified 08/25/22 03:14:01.114
    STEP: Deleting a ResourceQuota 08/25/22 03:14:01.116
    STEP: Verifying the deleted ResourceQuota 08/25/22 03:14:01.12
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:14:01.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5894" for this suite. 08/25/22 03:14:01.126
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:01.133
Aug 25 03:14:01.133: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:14:01.135
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:01.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:01.148
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 25 03:14:01.152: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:14:04.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5064" for this suite. 08/25/22 03:14:04.305
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":79,"skipped":1380,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.177 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:01.133
    Aug 25 03:14:01.133: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:14:01.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:01.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:01.148
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 25 03:14:01.152: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:14:04.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5064" for this suite. 08/25/22 03:14:04.305
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:04.311
Aug 25 03:14:04.311: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:14:04.312
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:04.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:04.329
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 08/25/22 03:14:04.355
Aug 25 03:14:04.355: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-8103 proxy --unix-socket=/tmp/kubectl-proxy-unix4091282167/test'
STEP: retrieving proxy /api/ output 08/25/22 03:14:04.401
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:14:04.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8103" for this suite. 08/25/22 03:14:04.406
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":80,"skipped":1384,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.100 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:04.311
    Aug 25 03:14:04.311: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:14:04.312
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:04.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:04.329
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 08/25/22 03:14:04.355
    Aug 25 03:14:04.355: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-8103 proxy --unix-socket=/tmp/kubectl-proxy-unix4091282167/test'
    STEP: retrieving proxy /api/ output 08/25/22 03:14:04.401
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:14:04.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8103" for this suite. 08/25/22 03:14:04.406
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:04.411
Aug 25 03:14:04.412: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:04.413
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:04.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:04.426
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 08/25/22 03:14:04.429
STEP: Ensuring ResourceQuota status is calculated 08/25/22 03:14:04.432
STEP: Creating a ResourceQuota with not terminating scope 08/25/22 03:14:06.436
STEP: Ensuring ResourceQuota status is calculated 08/25/22 03:14:06.44
STEP: Creating a long running pod 08/25/22 03:14:08.443
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/25/22 03:14:08.451
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/25/22 03:14:10.456
STEP: Deleting the pod 08/25/22 03:14:12.46
STEP: Ensuring resource quota status released the pod usage 08/25/22 03:14:12.468
STEP: Creating a terminating pod 08/25/22 03:14:14.473
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/25/22 03:14:14.484
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/25/22 03:14:16.49
STEP: Deleting the pod 08/25/22 03:14:18.494
STEP: Ensuring resource quota status released the pod usage 08/25/22 03:14:18.501
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:14:20.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2518" for this suite. 08/25/22 03:14:20.511
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":81,"skipped":1389,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [16.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:04.411
    Aug 25 03:14:04.412: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:04.413
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:04.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:04.426
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 08/25/22 03:14:04.429
    STEP: Ensuring ResourceQuota status is calculated 08/25/22 03:14:04.432
    STEP: Creating a ResourceQuota with not terminating scope 08/25/22 03:14:06.436
    STEP: Ensuring ResourceQuota status is calculated 08/25/22 03:14:06.44
    STEP: Creating a long running pod 08/25/22 03:14:08.443
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/25/22 03:14:08.451
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/25/22 03:14:10.456
    STEP: Deleting the pod 08/25/22 03:14:12.46
    STEP: Ensuring resource quota status released the pod usage 08/25/22 03:14:12.468
    STEP: Creating a terminating pod 08/25/22 03:14:14.473
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/25/22 03:14:14.484
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/25/22 03:14:16.49
    STEP: Deleting the pod 08/25/22 03:14:18.494
    STEP: Ensuring resource quota status released the pod usage 08/25/22 03:14:18.501
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:14:20.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2518" for this suite. 08/25/22 03:14:20.511
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:20.516
Aug 25 03:14:20.516: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:14:20.518
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:20.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:20.534
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 08/25/22 03:14:20.538
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:14:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9431" for this suite. 08/25/22 03:14:20.547
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":82,"skipped":1389,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.035 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:20.516
    Aug 25 03:14:20.516: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:14:20.518
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:20.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:20.534
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 08/25/22 03:14:20.538
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:14:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9431" for this suite. 08/25/22 03:14:20.547
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:20.553
Aug 25 03:14:20.554: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:14:20.555
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:20.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:20.57
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/25/22 03:14:20.573
Aug 25 03:14:20.580: INFO: Waiting up to 5m0s for pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2" in namespace "emptydir-3196" to be "Succeeded or Failed"
Aug 25 03:14:20.582: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637358ms
Aug 25 03:14:22.588: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008534992s
Aug 25 03:14:24.589: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009526905s
STEP: Saw pod success 08/25/22 03:14:24.589
Aug 25 03:14:24.589: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2" satisfied condition "Succeeded or Failed"
Aug 25 03:14:24.594: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2 container test-container: <nil>
STEP: delete the pod 08/25/22 03:14:24.6
Aug 25 03:14:24.607: INFO: Waiting for pod pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2 to disappear
Aug 25 03:14:24.610: INFO: Pod pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:14:24.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3196" for this suite. 08/25/22 03:14:24.615
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":83,"skipped":1413,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.071 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:20.553
    Aug 25 03:14:20.554: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:14:20.555
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:20.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:20.57
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/25/22 03:14:20.573
    Aug 25 03:14:20.580: INFO: Waiting up to 5m0s for pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2" in namespace "emptydir-3196" to be "Succeeded or Failed"
    Aug 25 03:14:20.582: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637358ms
    Aug 25 03:14:22.588: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008534992s
    Aug 25 03:14:24.589: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009526905s
    STEP: Saw pod success 08/25/22 03:14:24.589
    Aug 25 03:14:24.589: INFO: Pod "pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2" satisfied condition "Succeeded or Failed"
    Aug 25 03:14:24.594: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2 container test-container: <nil>
    STEP: delete the pod 08/25/22 03:14:24.6
    Aug 25 03:14:24.607: INFO: Waiting for pod pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2 to disappear
    Aug 25 03:14:24.610: INFO: Pod pod-c223621f-94d7-4d69-a7b3-d247fc93f0b2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:14:24.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3196" for this suite. 08/25/22 03:14:24.615
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:24.625
Aug 25 03:14:24.625: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:14:24.627
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:24.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:24.643
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/25/22 03:14:24.648
Aug 25 03:14:24.655: INFO: Waiting up to 5m0s for pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393" in namespace "emptydir-4209" to be "Succeeded or Failed"
Aug 25 03:14:24.658: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174272ms
Aug 25 03:14:26.663: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008118873s
Aug 25 03:14:28.665: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009686294s
STEP: Saw pod success 08/25/22 03:14:28.665
Aug 25 03:14:28.665: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393" satisfied condition "Succeeded or Failed"
Aug 25 03:14:28.669: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-cf2188dd-da26-4897-88a3-fa08e06dd393 container test-container: <nil>
STEP: delete the pod 08/25/22 03:14:28.675
Aug 25 03:14:28.688: INFO: Waiting for pod pod-cf2188dd-da26-4897-88a3-fa08e06dd393 to disappear
Aug 25 03:14:28.691: INFO: Pod pod-cf2188dd-da26-4897-88a3-fa08e06dd393 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:14:28.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4209" for this suite. 08/25/22 03:14:28.694
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1417,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:24.625
    Aug 25 03:14:24.625: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:14:24.627
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:24.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:24.643
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/25/22 03:14:24.648
    Aug 25 03:14:24.655: INFO: Waiting up to 5m0s for pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393" in namespace "emptydir-4209" to be "Succeeded or Failed"
    Aug 25 03:14:24.658: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174272ms
    Aug 25 03:14:26.663: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008118873s
    Aug 25 03:14:28.665: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009686294s
    STEP: Saw pod success 08/25/22 03:14:28.665
    Aug 25 03:14:28.665: INFO: Pod "pod-cf2188dd-da26-4897-88a3-fa08e06dd393" satisfied condition "Succeeded or Failed"
    Aug 25 03:14:28.669: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-cf2188dd-da26-4897-88a3-fa08e06dd393 container test-container: <nil>
    STEP: delete the pod 08/25/22 03:14:28.675
    Aug 25 03:14:28.688: INFO: Waiting for pod pod-cf2188dd-da26-4897-88a3-fa08e06dd393 to disappear
    Aug 25 03:14:28.691: INFO: Pod pod-cf2188dd-da26-4897-88a3-fa08e06dd393 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:14:28.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4209" for this suite. 08/25/22 03:14:28.694
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:28.698
Aug 25 03:14:28.698: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:28.699
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:28.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:28.714
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 08/25/22 03:14:28.719
STEP: Counting existing ResourceQuota 08/25/22 03:14:33.723
STEP: Creating a ResourceQuota 08/25/22 03:14:38.727
STEP: Ensuring resource quota status is calculated 08/25/22 03:14:38.733
STEP: Creating a Secret 08/25/22 03:14:40.738
STEP: Ensuring resource quota status captures secret creation 08/25/22 03:14:40.749
STEP: Deleting a secret 08/25/22 03:14:42.755
STEP: Ensuring resource quota status released usage 08/25/22 03:14:42.76
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:14:44.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5342" for this suite. 08/25/22 03:14:44.77
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":85,"skipped":1427,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [16.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:28.698
    Aug 25 03:14:28.698: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:28.699
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:28.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:28.714
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 08/25/22 03:14:28.719
    STEP: Counting existing ResourceQuota 08/25/22 03:14:33.723
    STEP: Creating a ResourceQuota 08/25/22 03:14:38.727
    STEP: Ensuring resource quota status is calculated 08/25/22 03:14:38.733
    STEP: Creating a Secret 08/25/22 03:14:40.738
    STEP: Ensuring resource quota status captures secret creation 08/25/22 03:14:40.749
    STEP: Deleting a secret 08/25/22 03:14:42.755
    STEP: Ensuring resource quota status released usage 08/25/22 03:14:42.76
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:14:44.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5342" for this suite. 08/25/22 03:14:44.77
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:44.777
Aug 25 03:14:44.777: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename proxy 08/25/22 03:14:44.778
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:44.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:44.795
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 25 03:14:44.799: INFO: Creating pod...
Aug 25 03:14:44.805: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-650" to be "running"
Aug 25 03:14:44.808: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681437ms
Aug 25 03:14:46.813: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00806269s
Aug 25 03:14:46.813: INFO: Pod "agnhost" satisfied condition "running"
Aug 25 03:14:46.813: INFO: Creating service...
Aug 25 03:14:46.825: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/DELETE
Aug 25 03:14:46.829: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 25 03:14:46.829: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/GET
Aug 25 03:14:46.832: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 25 03:14:46.832: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/HEAD
Aug 25 03:14:46.835: INFO: http.Client request:HEAD | StatusCode:200
Aug 25 03:14:46.835: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 25 03:14:46.837: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 25 03:14:46.837: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/PATCH
Aug 25 03:14:46.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 25 03:14:46.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/POST
Aug 25 03:14:46.842: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 25 03:14:46.842: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/PUT
Aug 25 03:14:46.844: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 25 03:14:46.844: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/DELETE
Aug 25 03:14:46.847: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 25 03:14:46.847: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/GET
Aug 25 03:14:46.850: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 25 03:14:46.850: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/HEAD
Aug 25 03:14:46.854: INFO: http.Client request:HEAD | StatusCode:200
Aug 25 03:14:46.854: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/OPTIONS
Aug 25 03:14:46.857: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 25 03:14:46.857: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/PATCH
Aug 25 03:14:46.860: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 25 03:14:46.860: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/POST
Aug 25 03:14:46.863: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 25 03:14:46.863: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/PUT
Aug 25 03:14:46.866: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Aug 25 03:14:46.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-650" for this suite. 08/25/22 03:14:46.87
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":86,"skipped":1444,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.098 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:44.777
    Aug 25 03:14:44.777: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename proxy 08/25/22 03:14:44.778
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:44.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:44.795
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 25 03:14:44.799: INFO: Creating pod...
    Aug 25 03:14:44.805: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-650" to be "running"
    Aug 25 03:14:44.808: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681437ms
    Aug 25 03:14:46.813: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00806269s
    Aug 25 03:14:46.813: INFO: Pod "agnhost" satisfied condition "running"
    Aug 25 03:14:46.813: INFO: Creating service...
    Aug 25 03:14:46.825: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/DELETE
    Aug 25 03:14:46.829: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 25 03:14:46.829: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/GET
    Aug 25 03:14:46.832: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 25 03:14:46.832: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/HEAD
    Aug 25 03:14:46.835: INFO: http.Client request:HEAD | StatusCode:200
    Aug 25 03:14:46.835: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 25 03:14:46.837: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 25 03:14:46.837: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/PATCH
    Aug 25 03:14:46.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 25 03:14:46.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/POST
    Aug 25 03:14:46.842: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 25 03:14:46.842: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/pods/agnhost/proxy/some/path/with/PUT
    Aug 25 03:14:46.844: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 25 03:14:46.844: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/DELETE
    Aug 25 03:14:46.847: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 25 03:14:46.847: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/GET
    Aug 25 03:14:46.850: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 25 03:14:46.850: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/HEAD
    Aug 25 03:14:46.854: INFO: http.Client request:HEAD | StatusCode:200
    Aug 25 03:14:46.854: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/OPTIONS
    Aug 25 03:14:46.857: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 25 03:14:46.857: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/PATCH
    Aug 25 03:14:46.860: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 25 03:14:46.860: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/POST
    Aug 25 03:14:46.863: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 25 03:14:46.863: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-650/services/test-service/proxy/some/path/with/PUT
    Aug 25 03:14:46.866: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Aug 25 03:14:46.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-650" for this suite. 08/25/22 03:14:46.87
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:46.875
Aug 25 03:14:46.875: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:14:46.876
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:46.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:46.892
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8612 08/25/22 03:14:46.896
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/25/22 03:14:46.908
STEP: creating service externalsvc in namespace services-8612 08/25/22 03:14:46.908
STEP: creating replication controller externalsvc in namespace services-8612 08/25/22 03:14:46.92
I0825 03:14:46.925746      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8612, replica count: 2
I0825 03:14:49.977754      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/25/22 03:14:49.987
Aug 25 03:14:50.003: INFO: Creating new exec pod
Aug 25 03:14:50.008: INFO: Waiting up to 5m0s for pod "execpodmmzns" in namespace "services-8612" to be "running"
Aug 25 03:14:50.012: INFO: Pod "execpodmmzns": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777798ms
Aug 25 03:14:52.015: INFO: Pod "execpodmmzns": Phase="Running", Reason="", readiness=true. Elapsed: 2.007347279s
Aug 25 03:14:52.015: INFO: Pod "execpodmmzns" satisfied condition "running"
Aug 25 03:14:52.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8612 exec execpodmmzns -- /bin/sh -x -c nslookup nodeport-service.services-8612.svc.cluster.local'
Aug 25 03:14:52.245: INFO: stderr: "+ nslookup nodeport-service.services-8612.svc.cluster.local\n"
Aug 25 03:14:52.245: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8612.svc.cluster.local\tcanonical name = externalsvc.services-8612.svc.cluster.local.\nName:\texternalsvc.services-8612.svc.cluster.local\nAddress: 10.101.227.107\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8612, will wait for the garbage collector to delete the pods 08/25/22 03:14:52.246
Aug 25 03:14:52.304: INFO: Deleting ReplicationController externalsvc took: 4.83869ms
Aug 25 03:14:52.405: INFO: Terminating ReplicationController externalsvc pods took: 100.870783ms
Aug 25 03:14:54.615: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:14:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8612" for this suite. 08/25/22 03:14:54.626
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":87,"skipped":1448,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.755 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:46.875
    Aug 25 03:14:46.875: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:14:46.876
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:46.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:46.892
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8612 08/25/22 03:14:46.896
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/25/22 03:14:46.908
    STEP: creating service externalsvc in namespace services-8612 08/25/22 03:14:46.908
    STEP: creating replication controller externalsvc in namespace services-8612 08/25/22 03:14:46.92
    I0825 03:14:46.925746      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8612, replica count: 2
    I0825 03:14:49.977754      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/25/22 03:14:49.987
    Aug 25 03:14:50.003: INFO: Creating new exec pod
    Aug 25 03:14:50.008: INFO: Waiting up to 5m0s for pod "execpodmmzns" in namespace "services-8612" to be "running"
    Aug 25 03:14:50.012: INFO: Pod "execpodmmzns": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777798ms
    Aug 25 03:14:52.015: INFO: Pod "execpodmmzns": Phase="Running", Reason="", readiness=true. Elapsed: 2.007347279s
    Aug 25 03:14:52.015: INFO: Pod "execpodmmzns" satisfied condition "running"
    Aug 25 03:14:52.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8612 exec execpodmmzns -- /bin/sh -x -c nslookup nodeport-service.services-8612.svc.cluster.local'
    Aug 25 03:14:52.245: INFO: stderr: "+ nslookup nodeport-service.services-8612.svc.cluster.local\n"
    Aug 25 03:14:52.245: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8612.svc.cluster.local\tcanonical name = externalsvc.services-8612.svc.cluster.local.\nName:\texternalsvc.services-8612.svc.cluster.local\nAddress: 10.101.227.107\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8612, will wait for the garbage collector to delete the pods 08/25/22 03:14:52.246
    Aug 25 03:14:52.304: INFO: Deleting ReplicationController externalsvc took: 4.83869ms
    Aug 25 03:14:52.405: INFO: Terminating ReplicationController externalsvc pods took: 100.870783ms
    Aug 25 03:14:54.615: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:14:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8612" for this suite. 08/25/22 03:14:54.626
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:54.631
Aug 25 03:14:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:14:54.632
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:54.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:54.646
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 08/25/22 03:14:54.65
Aug 25 03:14:54.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1703 create -f -'
Aug 25 03:14:55.650: INFO: stderr: ""
Aug 25 03:14:55.650: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/25/22 03:14:55.65
Aug 25 03:14:55.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1703 diff -f -'
Aug 25 03:14:55.918: INFO: rc: 1
Aug 25 03:14:55.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1703 delete -f -'
Aug 25 03:14:56.002: INFO: stderr: ""
Aug 25 03:14:56.002: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:14:56.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1703" for this suite. 08/25/22 03:14:56.006
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":88,"skipped":1466,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [1.379 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:54.631
    Aug 25 03:14:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:14:54.632
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:54.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:54.646
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 08/25/22 03:14:54.65
    Aug 25 03:14:54.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1703 create -f -'
    Aug 25 03:14:55.650: INFO: stderr: ""
    Aug 25 03:14:55.650: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/25/22 03:14:55.65
    Aug 25 03:14:55.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1703 diff -f -'
    Aug 25 03:14:55.918: INFO: rc: 1
    Aug 25 03:14:55.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1703 delete -f -'
    Aug 25 03:14:56.002: INFO: stderr: ""
    Aug 25 03:14:56.002: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:14:56.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1703" for this suite. 08/25/22 03:14:56.006
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:14:56.012
Aug 25 03:14:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:56.013
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:56.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:56.031
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 08/25/22 03:15:13.039
STEP: Creating a ResourceQuota 08/25/22 03:15:18.043
STEP: Ensuring resource quota status is calculated 08/25/22 03:15:18.048
STEP: Creating a ConfigMap 08/25/22 03:15:20.054
STEP: Ensuring resource quota status captures configMap creation 08/25/22 03:15:20.067
STEP: Deleting a ConfigMap 08/25/22 03:15:22.073
STEP: Ensuring resource quota status released usage 08/25/22 03:15:22.078
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:15:24.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2803" for this suite. 08/25/22 03:15:24.086
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":89,"skipped":1491,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [28.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:14:56.012
    Aug 25 03:14:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:14:56.013
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:14:56.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:14:56.031
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 08/25/22 03:15:13.039
    STEP: Creating a ResourceQuota 08/25/22 03:15:18.043
    STEP: Ensuring resource quota status is calculated 08/25/22 03:15:18.048
    STEP: Creating a ConfigMap 08/25/22 03:15:20.054
    STEP: Ensuring resource quota status captures configMap creation 08/25/22 03:15:20.067
    STEP: Deleting a ConfigMap 08/25/22 03:15:22.073
    STEP: Ensuring resource quota status released usage 08/25/22 03:15:22.078
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:15:24.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2803" for this suite. 08/25/22 03:15:24.086
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:15:24.09
Aug 25 03:15:24.091: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename disruption 08/25/22 03:15:24.091
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:24.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:24.102
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:15:24.105
Aug 25 03:15:24.106: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename disruption-2 08/25/22 03:15:24.106
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:24.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:24.118
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 08/25/22 03:15:24.126
STEP: Waiting for the pdb to be processed 08/25/22 03:15:26.138
STEP: Waiting for the pdb to be processed 08/25/22 03:15:28.152
STEP: listing a collection of PDBs across all namespaces 08/25/22 03:15:30.16
STEP: listing a collection of PDBs in namespace disruption-7165 08/25/22 03:15:30.165
STEP: deleting a collection of PDBs 08/25/22 03:15:30.169
STEP: Waiting for the PDB collection to be deleted 08/25/22 03:15:30.178
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Aug 25 03:15:30.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7199" for this suite. 08/25/22 03:15:30.185
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 25 03:15:30.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7165" for this suite. 08/25/22 03:15:30.193
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":90,"skipped":1536,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:15:24.09
    Aug 25 03:15:24.091: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename disruption 08/25/22 03:15:24.091
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:24.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:24.102
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:15:24.105
    Aug 25 03:15:24.106: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename disruption-2 08/25/22 03:15:24.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:24.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:24.118
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 08/25/22 03:15:24.126
    STEP: Waiting for the pdb to be processed 08/25/22 03:15:26.138
    STEP: Waiting for the pdb to be processed 08/25/22 03:15:28.152
    STEP: listing a collection of PDBs across all namespaces 08/25/22 03:15:30.16
    STEP: listing a collection of PDBs in namespace disruption-7165 08/25/22 03:15:30.165
    STEP: deleting a collection of PDBs 08/25/22 03:15:30.169
    STEP: Waiting for the PDB collection to be deleted 08/25/22 03:15:30.178
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Aug 25 03:15:30.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-7199" for this suite. 08/25/22 03:15:30.185
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 25 03:15:30.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7165" for this suite. 08/25/22 03:15:30.193
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:15:30.199
Aug 25 03:15:30.199: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:15:30.201
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:30.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:30.22
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 08/25/22 03:15:30.224
Aug 25 03:15:30.229: INFO: Waiting up to 5m0s for pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa" in namespace "downward-api-5459" to be "Succeeded or Failed"
Aug 25 03:15:30.233: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252804ms
Aug 25 03:15:32.239: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009122745s
Aug 25 03:15:34.238: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009031173s
STEP: Saw pod success 08/25/22 03:15:34.239
Aug 25 03:15:34.239: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa" satisfied condition "Succeeded or Failed"
Aug 25 03:15:34.243: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa container dapi-container: <nil>
STEP: delete the pod 08/25/22 03:15:34.25
Aug 25 03:15:34.257: INFO: Waiting for pod downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa to disappear
Aug 25 03:15:34.259: INFO: Pod downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 25 03:15:34.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5459" for this suite. 08/25/22 03:15:34.263
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":91,"skipped":1541,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:15:30.199
    Aug 25 03:15:30.199: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:15:30.201
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:30.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:30.22
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 08/25/22 03:15:30.224
    Aug 25 03:15:30.229: INFO: Waiting up to 5m0s for pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa" in namespace "downward-api-5459" to be "Succeeded or Failed"
    Aug 25 03:15:30.233: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252804ms
    Aug 25 03:15:32.239: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009122745s
    Aug 25 03:15:34.238: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009031173s
    STEP: Saw pod success 08/25/22 03:15:34.239
    Aug 25 03:15:34.239: INFO: Pod "downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa" satisfied condition "Succeeded or Failed"
    Aug 25 03:15:34.243: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa container dapi-container: <nil>
    STEP: delete the pod 08/25/22 03:15:34.25
    Aug 25 03:15:34.257: INFO: Waiting for pod downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa to disappear
    Aug 25 03:15:34.259: INFO: Pod downward-api-c8c900fe-76b8-462e-9fa0-51b2d2299dfa no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 25 03:15:34.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5459" for this suite. 08/25/22 03:15:34.263
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:15:34.269
Aug 25 03:15:34.269: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:15:34.27
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:34.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:34.288
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7733 08/25/22 03:15:34.292
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/25/22 03:15:34.3
STEP: creating service externalsvc in namespace services-7733 08/25/22 03:15:34.3
STEP: creating replication controller externalsvc in namespace services-7733 08/25/22 03:15:34.31
I0825 03:15:34.319473      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7733, replica count: 2
I0825 03:15:37.371383      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/25/22 03:15:37.376
Aug 25 03:15:37.407: INFO: Creating new exec pod
Aug 25 03:15:37.413: INFO: Waiting up to 5m0s for pod "execpod2lj9l" in namespace "services-7733" to be "running"
Aug 25 03:15:37.415: INFO: Pod "execpod2lj9l": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860653ms
Aug 25 03:15:39.419: INFO: Pod "execpod2lj9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.006217851s
Aug 25 03:15:39.419: INFO: Pod "execpod2lj9l" satisfied condition "running"
Aug 25 03:15:39.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7733 exec execpod2lj9l -- /bin/sh -x -c nslookup clusterip-service.services-7733.svc.cluster.local'
Aug 25 03:15:39.637: INFO: stderr: "+ nslookup clusterip-service.services-7733.svc.cluster.local\n"
Aug 25 03:15:39.637: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7733.svc.cluster.local\tcanonical name = externalsvc.services-7733.svc.cluster.local.\nName:\texternalsvc.services-7733.svc.cluster.local\nAddress: 10.99.213.64\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7733, will wait for the garbage collector to delete the pods 08/25/22 03:15:39.637
Aug 25 03:15:39.695: INFO: Deleting ReplicationController externalsvc took: 5.443198ms
Aug 25 03:15:39.796: INFO: Terminating ReplicationController externalsvc pods took: 100.161613ms
Aug 25 03:15:41.909: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:15:41.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7733" for this suite. 08/25/22 03:15:41.92
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":92,"skipped":1562,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.654 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:15:34.269
    Aug 25 03:15:34.269: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:15:34.27
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:34.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:34.288
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7733 08/25/22 03:15:34.292
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/25/22 03:15:34.3
    STEP: creating service externalsvc in namespace services-7733 08/25/22 03:15:34.3
    STEP: creating replication controller externalsvc in namespace services-7733 08/25/22 03:15:34.31
    I0825 03:15:34.319473      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7733, replica count: 2
    I0825 03:15:37.371383      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/25/22 03:15:37.376
    Aug 25 03:15:37.407: INFO: Creating new exec pod
    Aug 25 03:15:37.413: INFO: Waiting up to 5m0s for pod "execpod2lj9l" in namespace "services-7733" to be "running"
    Aug 25 03:15:37.415: INFO: Pod "execpod2lj9l": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860653ms
    Aug 25 03:15:39.419: INFO: Pod "execpod2lj9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.006217851s
    Aug 25 03:15:39.419: INFO: Pod "execpod2lj9l" satisfied condition "running"
    Aug 25 03:15:39.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7733 exec execpod2lj9l -- /bin/sh -x -c nslookup clusterip-service.services-7733.svc.cluster.local'
    Aug 25 03:15:39.637: INFO: stderr: "+ nslookup clusterip-service.services-7733.svc.cluster.local\n"
    Aug 25 03:15:39.637: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7733.svc.cluster.local\tcanonical name = externalsvc.services-7733.svc.cluster.local.\nName:\texternalsvc.services-7733.svc.cluster.local\nAddress: 10.99.213.64\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7733, will wait for the garbage collector to delete the pods 08/25/22 03:15:39.637
    Aug 25 03:15:39.695: INFO: Deleting ReplicationController externalsvc took: 5.443198ms
    Aug 25 03:15:39.796: INFO: Terminating ReplicationController externalsvc pods took: 100.161613ms
    Aug 25 03:15:41.909: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:15:41.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7733" for this suite. 08/25/22 03:15:41.92
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:15:41.925
Aug 25 03:15:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:15:41.926
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:41.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:41.937
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:15:41.952
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:15:42.153
STEP: Deploying the webhook pod 08/25/22 03:15:42.161
STEP: Wait for the deployment to be ready 08/25/22 03:15:42.171
Aug 25 03:15:42.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 03:15:44.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:15:46.199
STEP: Verifying the service has paired with the endpoint 08/25/22 03:15:46.21
Aug 25 03:15:47.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/25/22 03:15:47.213
STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:47.214
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/25/22 03:15:47.233
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/25/22 03:15:48.245
STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:48.245
STEP: Having no error when timeout is longer than webhook latency 08/25/22 03:15:49.271
STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:49.272
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/25/22 03:15:54.305
STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:54.306
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:15:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5789" for this suite. 08/25/22 03:15:59.337
STEP: Destroying namespace "webhook-5789-markers" for this suite. 08/25/22 03:15:59.342
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":93,"skipped":1585,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [17.445 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:15:41.925
    Aug 25 03:15:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:15:41.926
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:41.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:41.937
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:15:41.952
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:15:42.153
    STEP: Deploying the webhook pod 08/25/22 03:15:42.161
    STEP: Wait for the deployment to be ready 08/25/22 03:15:42.171
    Aug 25 03:15:42.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 03:15:44.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 15, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:15:46.199
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:15:46.21
    Aug 25 03:15:47.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/25/22 03:15:47.213
    STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:47.214
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/25/22 03:15:47.233
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/25/22 03:15:48.245
    STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:48.245
    STEP: Having no error when timeout is longer than webhook latency 08/25/22 03:15:49.271
    STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:49.272
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/25/22 03:15:54.305
    STEP: Registering slow webhook via the AdmissionRegistration API 08/25/22 03:15:54.306
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:15:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5789" for this suite. 08/25/22 03:15:59.337
    STEP: Destroying namespace "webhook-5789-markers" for this suite. 08/25/22 03:15:59.342
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:15:59.371
Aug 25 03:15:59.371: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:15:59.371
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:59.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:59.388
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 08/25/22 03:15:59.391
Aug 25 03:15:59.397: INFO: Waiting up to 5m0s for pod "pod-4k4l9" in namespace "pods-5157" to be "running"
Aug 25 03:15:59.400: INFO: Pod "pod-4k4l9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580421ms
Aug 25 03:16:01.404: INFO: Pod "pod-4k4l9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007656001s
Aug 25 03:16:01.404: INFO: Pod "pod-4k4l9" satisfied condition "running"
STEP: patching /status 08/25/22 03:16:01.404
Aug 25 03:16:01.410: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:16:01.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5157" for this suite. 08/25/22 03:16:01.414
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":94,"skipped":1585,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.048 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:15:59.371
    Aug 25 03:15:59.371: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:15:59.371
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:15:59.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:15:59.388
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 08/25/22 03:15:59.391
    Aug 25 03:15:59.397: INFO: Waiting up to 5m0s for pod "pod-4k4l9" in namespace "pods-5157" to be "running"
    Aug 25 03:15:59.400: INFO: Pod "pod-4k4l9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580421ms
    Aug 25 03:16:01.404: INFO: Pod "pod-4k4l9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007656001s
    Aug 25 03:16:01.404: INFO: Pod "pod-4k4l9" satisfied condition "running"
    STEP: patching /status 08/25/22 03:16:01.404
    Aug 25 03:16:01.410: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:16:01.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5157" for this suite. 08/25/22 03:16:01.414
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:16:01.42
Aug 25 03:16:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:16:01.421
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:16:01.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:16:01.436
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:16:01.442
Aug 25 03:16:01.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac" in namespace "downward-api-9704" to be "Succeeded or Failed"
Aug 25 03:16:01.452: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410734ms
Aug 25 03:16:03.458: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008906078s
Aug 25 03:16:05.457: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007991153s
STEP: Saw pod success 08/25/22 03:16:05.457
Aug 25 03:16:05.457: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac" satisfied condition "Succeeded or Failed"
Aug 25 03:16:05.461: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac container client-container: <nil>
STEP: delete the pod 08/25/22 03:16:05.466
Aug 25 03:16:05.474: INFO: Waiting for pod downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac to disappear
Aug 25 03:16:05.479: INFO: Pod downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 03:16:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9704" for this suite. 08/25/22 03:16:05.484
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":95,"skipped":1606,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.069 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:16:01.42
    Aug 25 03:16:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:16:01.421
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:16:01.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:16:01.436
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:16:01.442
    Aug 25 03:16:01.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac" in namespace "downward-api-9704" to be "Succeeded or Failed"
    Aug 25 03:16:01.452: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410734ms
    Aug 25 03:16:03.458: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008906078s
    Aug 25 03:16:05.457: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007991153s
    STEP: Saw pod success 08/25/22 03:16:05.457
    Aug 25 03:16:05.457: INFO: Pod "downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac" satisfied condition "Succeeded or Failed"
    Aug 25 03:16:05.461: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac container client-container: <nil>
    STEP: delete the pod 08/25/22 03:16:05.466
    Aug 25 03:16:05.474: INFO: Waiting for pod downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac to disappear
    Aug 25 03:16:05.479: INFO: Pod downwardapi-volume-d674230a-3b8f-478d-b4a5-fd405c338bac no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 03:16:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9704" for this suite. 08/25/22 03:16:05.484
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:16:05.49
Aug 25 03:16:05.490: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:16:05.492
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:16:05.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:16:05.51
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:16:05.514
Aug 25 03:16:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 25 03:16:05.590: INFO: stderr: ""
Aug 25 03:16:05.590: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/25/22 03:16:05.59
STEP: verifying the pod e2e-test-httpd-pod was created 08/25/22 03:16:10.642
Aug 25 03:16:10.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 get pod e2e-test-httpd-pod -o json'
Aug 25 03:16:10.711: INFO: stderr: ""
Aug 25 03:16:10.711: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-08-25T03:16:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6760\",\n        \"resourceVersion\": \"19814\",\n        \"uid\": \"fd589d18-42b9-4e3d-832b-aedcd44233ac\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6cfwt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"bobymcbobs-c849-control-plane-p55pp\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6cfwt\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0b167d7929045b42f47c7f8b136988e0f13f62e0dc52e4f987dc897a07235a5f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-08-25T03:16:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"86.109.11.217\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.0.13\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.0.13\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-08-25T03:16:05Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/25/22 03:16:10.711
Aug 25 03:16:10.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 replace -f -'
Aug 25 03:16:10.973: INFO: stderr: ""
Aug 25 03:16:10.973: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 08/25/22 03:16:10.973
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Aug 25 03:16:10.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 delete pods e2e-test-httpd-pod'
Aug 25 03:16:12.988: INFO: stderr: ""
Aug 25 03:16:12.988: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:16:12.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6760" for this suite. 08/25/22 03:16:12.992
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":96,"skipped":1618,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.506 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:16:05.49
    Aug 25 03:16:05.490: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:16:05.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:16:05.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:16:05.51
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:16:05.514
    Aug 25 03:16:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 25 03:16:05.590: INFO: stderr: ""
    Aug 25 03:16:05.590: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/25/22 03:16:05.59
    STEP: verifying the pod e2e-test-httpd-pod was created 08/25/22 03:16:10.642
    Aug 25 03:16:10.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 get pod e2e-test-httpd-pod -o json'
    Aug 25 03:16:10.711: INFO: stderr: ""
    Aug 25 03:16:10.711: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-08-25T03:16:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6760\",\n        \"resourceVersion\": \"19814\",\n        \"uid\": \"fd589d18-42b9-4e3d-832b-aedcd44233ac\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6cfwt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"bobymcbobs-c849-control-plane-p55pp\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6cfwt\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-25T03:16:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0b167d7929045b42f47c7f8b136988e0f13f62e0dc52e4f987dc897a07235a5f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-08-25T03:16:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"86.109.11.217\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.0.13\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.0.13\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-08-25T03:16:05Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/25/22 03:16:10.711
    Aug 25 03:16:10.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 replace -f -'
    Aug 25 03:16:10.973: INFO: stderr: ""
    Aug 25 03:16:10.973: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 08/25/22 03:16:10.973
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Aug 25 03:16:10.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6760 delete pods e2e-test-httpd-pod'
    Aug 25 03:16:12.988: INFO: stderr: ""
    Aug 25 03:16:12.988: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:16:12.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6760" for this suite. 08/25/22 03:16:12.992
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:16:13
Aug 25 03:16:13.000: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir-wrapper 08/25/22 03:16:13.001
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:16:13.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:16:13.017
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/25/22 03:16:13.02
STEP: Creating RC which spawns configmap-volume pods 08/25/22 03:16:13.256
Aug 25 03:16:13.358: INFO: Pod name wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/25/22 03:16:13.358
Aug 25 03:16:13.359: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:13.406: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 47.305271ms
Aug 25 03:16:15.413: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054284142s
Aug 25 03:16:17.411: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052304995s
Aug 25 03:16:19.414: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055373576s
Aug 25 03:16:21.412: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053479734s
Aug 25 03:16:23.415: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 10.056533889s
Aug 25 03:16:25.414: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.055201023s
Aug 25 03:16:27.412: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Running", Reason="", readiness=true. Elapsed: 14.05359474s
Aug 25 03:16:27.412: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t" satisfied condition "running"
Aug 25 03:16:27.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-9xmgw" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:27.418: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-9xmgw": Phase="Running", Reason="", readiness=true. Elapsed: 5.589795ms
Aug 25 03:16:27.418: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-9xmgw" satisfied condition "running"
Aug 25 03:16:27.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-hvngr" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:27.422: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-hvngr": Phase="Running", Reason="", readiness=true. Elapsed: 4.463556ms
Aug 25 03:16:27.422: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-hvngr" satisfied condition "running"
Aug 25 03:16:27.422: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-mjj4h" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:27.427: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-mjj4h": Phase="Running", Reason="", readiness=true. Elapsed: 4.525629ms
Aug 25 03:16:27.427: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-mjj4h" satisfied condition "running"
Aug 25 03:16:27.427: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-rlz64" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:27.432: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-rlz64": Phase="Running", Reason="", readiness=true. Elapsed: 5.337718ms
Aug 25 03:16:27.432: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-rlz64" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e in namespace emptydir-wrapper-4036, will wait for the garbage collector to delete the pods 08/25/22 03:16:27.432
Aug 25 03:16:27.495: INFO: Deleting ReplicationController wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e took: 6.250422ms
Aug 25 03:16:27.596: INFO: Terminating ReplicationController wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e pods took: 101.1049ms
STEP: Creating RC which spawns configmap-volume pods 08/25/22 03:16:32.202
Aug 25 03:16:32.218: INFO: Pod name wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb: Found 0 pods out of 5
Aug 25 03:16:37.232: INFO: Pod name wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/25/22 03:16:37.232
Aug 25 03:16:37.232: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:37.238: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.517502ms
Aug 25 03:16:39.243: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01060944s
Aug 25 03:16:41.244: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011776275s
Aug 25 03:16:43.246: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013936265s
Aug 25 03:16:45.244: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012302157s
Aug 25 03:16:47.244: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Running", Reason="", readiness=true. Elapsed: 10.012411307s
Aug 25 03:16:47.245: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d" satisfied condition "running"
Aug 25 03:16:47.245: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:47.250: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511412ms
Aug 25 03:16:49.257: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012626022s
Aug 25 03:16:49.257: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz" satisfied condition "running"
Aug 25 03:16:49.257: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-hmthp" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:49.263: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-hmthp": Phase="Running", Reason="", readiness=true. Elapsed: 5.53376ms
Aug 25 03:16:49.263: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-hmthp" satisfied condition "running"
Aug 25 03:16:49.263: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-rvbd8" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:49.267: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-rvbd8": Phase="Running", Reason="", readiness=true. Elapsed: 4.193732ms
Aug 25 03:16:49.267: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-rvbd8" satisfied condition "running"
Aug 25 03:16:49.267: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-wmd78" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:49.270: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-wmd78": Phase="Running", Reason="", readiness=true. Elapsed: 3.354561ms
Aug 25 03:16:49.270: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-wmd78" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb in namespace emptydir-wrapper-4036, will wait for the garbage collector to delete the pods 08/25/22 03:16:49.27
Aug 25 03:16:49.333: INFO: Deleting ReplicationController wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb took: 6.080619ms
Aug 25 03:16:49.434: INFO: Terminating ReplicationController wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb pods took: 100.738727ms
STEP: Creating RC which spawns configmap-volume pods 08/25/22 03:16:52.841
Aug 25 03:16:52.859: INFO: Pod name wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd: Found 0 pods out of 5
Aug 25 03:16:57.872: INFO: Pod name wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/25/22 03:16:57.872
Aug 25 03:16:57.872: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:16:57.878: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132336ms
Aug 25 03:16:59.885: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034298s
Aug 25 03:17:01.885: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013120429s
Aug 25 03:17:03.886: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014074683s
Aug 25 03:17:05.885: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012732169s
Aug 25 03:17:07.886: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Running", Reason="", readiness=true. Elapsed: 10.013278619s
Aug 25 03:17:07.886: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l" satisfied condition "running"
Aug 25 03:17:07.886: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-4vhpr" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:17:07.889: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-4vhpr": Phase="Running", Reason="", readiness=true. Elapsed: 3.78285ms
Aug 25 03:17:07.889: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-4vhpr" satisfied condition "running"
Aug 25 03:17:07.889: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-hc5lf" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:17:07.892: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-hc5lf": Phase="Running", Reason="", readiness=true. Elapsed: 2.489283ms
Aug 25 03:17:07.892: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-hc5lf" satisfied condition "running"
Aug 25 03:17:07.892: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:17:07.895: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609059ms
Aug 25 03:17:09.908: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7": Phase="Running", Reason="", readiness=true. Elapsed: 2.016369383s
Aug 25 03:17:09.908: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7" satisfied condition "running"
Aug 25 03:17:09.908: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-z9k6v" in namespace "emptydir-wrapper-4036" to be "running"
Aug 25 03:17:09.914: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-z9k6v": Phase="Running", Reason="", readiness=true. Elapsed: 5.860569ms
Aug 25 03:17:09.914: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-z9k6v" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd in namespace emptydir-wrapper-4036, will wait for the garbage collector to delete the pods 08/25/22 03:17:09.914
Aug 25 03:17:09.975: INFO: Deleting ReplicationController wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd took: 5.922071ms
Aug 25 03:17:10.076: INFO: Terminating ReplicationController wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd pods took: 100.752647ms
STEP: Cleaning up the configMaps 08/25/22 03:17:13.477
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Aug 25 03:17:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4036" for this suite. 08/25/22 03:17:13.677
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":97,"skipped":1646,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [60.681 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:16:13
    Aug 25 03:16:13.000: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir-wrapper 08/25/22 03:16:13.001
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:16:13.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:16:13.017
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/25/22 03:16:13.02
    STEP: Creating RC which spawns configmap-volume pods 08/25/22 03:16:13.256
    Aug 25 03:16:13.358: INFO: Pod name wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/25/22 03:16:13.358
    Aug 25 03:16:13.359: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:13.406: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 47.305271ms
    Aug 25 03:16:15.413: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054284142s
    Aug 25 03:16:17.411: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052304995s
    Aug 25 03:16:19.414: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055373576s
    Aug 25 03:16:21.412: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053479734s
    Aug 25 03:16:23.415: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 10.056533889s
    Aug 25 03:16:25.414: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.055201023s
    Aug 25 03:16:27.412: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t": Phase="Running", Reason="", readiness=true. Elapsed: 14.05359474s
    Aug 25 03:16:27.412: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-4pp8t" satisfied condition "running"
    Aug 25 03:16:27.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-9xmgw" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:27.418: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-9xmgw": Phase="Running", Reason="", readiness=true. Elapsed: 5.589795ms
    Aug 25 03:16:27.418: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-9xmgw" satisfied condition "running"
    Aug 25 03:16:27.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-hvngr" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:27.422: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-hvngr": Phase="Running", Reason="", readiness=true. Elapsed: 4.463556ms
    Aug 25 03:16:27.422: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-hvngr" satisfied condition "running"
    Aug 25 03:16:27.422: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-mjj4h" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:27.427: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-mjj4h": Phase="Running", Reason="", readiness=true. Elapsed: 4.525629ms
    Aug 25 03:16:27.427: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-mjj4h" satisfied condition "running"
    Aug 25 03:16:27.427: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-rlz64" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:27.432: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-rlz64": Phase="Running", Reason="", readiness=true. Elapsed: 5.337718ms
    Aug 25 03:16:27.432: INFO: Pod "wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e-rlz64" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e in namespace emptydir-wrapper-4036, will wait for the garbage collector to delete the pods 08/25/22 03:16:27.432
    Aug 25 03:16:27.495: INFO: Deleting ReplicationController wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e took: 6.250422ms
    Aug 25 03:16:27.596: INFO: Terminating ReplicationController wrapped-volume-race-aef8d0aa-d6db-4c1b-82de-a6cf7d3adc1e pods took: 101.1049ms
    STEP: Creating RC which spawns configmap-volume pods 08/25/22 03:16:32.202
    Aug 25 03:16:32.218: INFO: Pod name wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb: Found 0 pods out of 5
    Aug 25 03:16:37.232: INFO: Pod name wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/25/22 03:16:37.232
    Aug 25 03:16:37.232: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:37.238: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.517502ms
    Aug 25 03:16:39.243: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01060944s
    Aug 25 03:16:41.244: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011776275s
    Aug 25 03:16:43.246: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013936265s
    Aug 25 03:16:45.244: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012302157s
    Aug 25 03:16:47.244: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d": Phase="Running", Reason="", readiness=true. Elapsed: 10.012411307s
    Aug 25 03:16:47.245: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-4rc6d" satisfied condition "running"
    Aug 25 03:16:47.245: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:47.250: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511412ms
    Aug 25 03:16:49.257: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012626022s
    Aug 25 03:16:49.257: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-gvbnz" satisfied condition "running"
    Aug 25 03:16:49.257: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-hmthp" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:49.263: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-hmthp": Phase="Running", Reason="", readiness=true. Elapsed: 5.53376ms
    Aug 25 03:16:49.263: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-hmthp" satisfied condition "running"
    Aug 25 03:16:49.263: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-rvbd8" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:49.267: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-rvbd8": Phase="Running", Reason="", readiness=true. Elapsed: 4.193732ms
    Aug 25 03:16:49.267: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-rvbd8" satisfied condition "running"
    Aug 25 03:16:49.267: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-wmd78" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:49.270: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-wmd78": Phase="Running", Reason="", readiness=true. Elapsed: 3.354561ms
    Aug 25 03:16:49.270: INFO: Pod "wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb-wmd78" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb in namespace emptydir-wrapper-4036, will wait for the garbage collector to delete the pods 08/25/22 03:16:49.27
    Aug 25 03:16:49.333: INFO: Deleting ReplicationController wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb took: 6.080619ms
    Aug 25 03:16:49.434: INFO: Terminating ReplicationController wrapped-volume-race-c82a7e4d-ab38-46a0-921e-471a42a5c1bb pods took: 100.738727ms
    STEP: Creating RC which spawns configmap-volume pods 08/25/22 03:16:52.841
    Aug 25 03:16:52.859: INFO: Pod name wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd: Found 0 pods out of 5
    Aug 25 03:16:57.872: INFO: Pod name wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/25/22 03:16:57.872
    Aug 25 03:16:57.872: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:16:57.878: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132336ms
    Aug 25 03:16:59.885: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034298s
    Aug 25 03:17:01.885: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013120429s
    Aug 25 03:17:03.886: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014074683s
    Aug 25 03:17:05.885: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012732169s
    Aug 25 03:17:07.886: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l": Phase="Running", Reason="", readiness=true. Elapsed: 10.013278619s
    Aug 25 03:17:07.886: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-2qk8l" satisfied condition "running"
    Aug 25 03:17:07.886: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-4vhpr" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:17:07.889: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-4vhpr": Phase="Running", Reason="", readiness=true. Elapsed: 3.78285ms
    Aug 25 03:17:07.889: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-4vhpr" satisfied condition "running"
    Aug 25 03:17:07.889: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-hc5lf" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:17:07.892: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-hc5lf": Phase="Running", Reason="", readiness=true. Elapsed: 2.489283ms
    Aug 25 03:17:07.892: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-hc5lf" satisfied condition "running"
    Aug 25 03:17:07.892: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:17:07.895: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609059ms
    Aug 25 03:17:09.908: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7": Phase="Running", Reason="", readiness=true. Elapsed: 2.016369383s
    Aug 25 03:17:09.908: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-m6kx7" satisfied condition "running"
    Aug 25 03:17:09.908: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-z9k6v" in namespace "emptydir-wrapper-4036" to be "running"
    Aug 25 03:17:09.914: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-z9k6v": Phase="Running", Reason="", readiness=true. Elapsed: 5.860569ms
    Aug 25 03:17:09.914: INFO: Pod "wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd-z9k6v" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd in namespace emptydir-wrapper-4036, will wait for the garbage collector to delete the pods 08/25/22 03:17:09.914
    Aug 25 03:17:09.975: INFO: Deleting ReplicationController wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd took: 5.922071ms
    Aug 25 03:17:10.076: INFO: Terminating ReplicationController wrapped-volume-race-c40a13c7-a1d5-4042-b438-78d008b7ecfd pods took: 100.752647ms
    STEP: Cleaning up the configMaps 08/25/22 03:17:13.477
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:17:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-4036" for this suite. 08/25/22 03:17:13.677
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:13.683
Aug 25 03:17:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 03:17:13.684
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:13.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:13.697
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/25/22 03:17:13.7
STEP: Wait for the Deployment to create new ReplicaSet 08/25/22 03:17:13.705
STEP: delete the deployment 08/25/22 03:17:14.213
STEP: wait for all rs to be garbage collected 08/25/22 03:17:14.217
STEP: expected 0 rs, got 1 rs 08/25/22 03:17:14.224
STEP: expected 0 pods, got 2 pods 08/25/22 03:17:14.228
STEP: Gathering metrics 08/25/22 03:17:14.74
Aug 25 03:17:15.172: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
Aug 25 03:17:15.177: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.989326ms
Aug 25 03:17:15.177: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
Aug 25 03:17:15.177: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
Aug 25 03:17:15.255: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 03:17:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9369" for this suite. 08/25/22 03:17:15.259
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":98,"skipped":1680,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [1.580 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:13.683
    Aug 25 03:17:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 03:17:13.684
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:13.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:13.697
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/25/22 03:17:13.7
    STEP: Wait for the Deployment to create new ReplicaSet 08/25/22 03:17:13.705
    STEP: delete the deployment 08/25/22 03:17:14.213
    STEP: wait for all rs to be garbage collected 08/25/22 03:17:14.217
    STEP: expected 0 rs, got 1 rs 08/25/22 03:17:14.224
    STEP: expected 0 pods, got 2 pods 08/25/22 03:17:14.228
    STEP: Gathering metrics 08/25/22 03:17:14.74
    Aug 25 03:17:15.172: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
    Aug 25 03:17:15.177: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.989326ms
    Aug 25 03:17:15.177: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
    Aug 25 03:17:15.177: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
    Aug 25 03:17:15.255: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 03:17:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9369" for this suite. 08/25/22 03:17:15.259
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:15.265
Aug 25 03:17:15.265: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:17:15.266
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:15.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:15.282
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 08/25/22 03:17:15.287
Aug 25 03:17:15.287: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3179 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/25/22 03:17:15.344
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:17:15.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3179" for this suite. 08/25/22 03:17:15.359
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":99,"skipped":1698,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.099 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:15.265
    Aug 25 03:17:15.265: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:17:15.266
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:15.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:15.282
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 08/25/22 03:17:15.287
    Aug 25 03:17:15.287: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3179 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/25/22 03:17:15.344
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:17:15.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3179" for this suite. 08/25/22 03:17:15.359
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:15.364
Aug 25 03:17:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 03:17:15.366
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:15.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:15.38
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-d6f3fc10-48fa-40e2-b4db-1881da951801 08/25/22 03:17:15.384
STEP: Creating a pod to test consume secrets 08/25/22 03:17:15.388
Aug 25 03:17:15.395: INFO: Waiting up to 5m0s for pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656" in namespace "secrets-8279" to be "Succeeded or Failed"
Aug 25 03:17:15.398: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656": Phase="Pending", Reason="", readiness=false. Elapsed: 2.806439ms
Aug 25 03:17:17.404: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009460911s
Aug 25 03:17:19.403: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007655108s
STEP: Saw pod success 08/25/22 03:17:19.403
Aug 25 03:17:19.403: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656" satisfied condition "Succeeded or Failed"
Aug 25 03:17:19.406: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:17:19.411
Aug 25 03:17:19.417: INFO: Waiting for pod pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656 to disappear
Aug 25 03:17:19.420: INFO: Pod pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 03:17:19.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8279" for this suite. 08/25/22 03:17:19.424
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":100,"skipped":1700,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.063 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:15.364
    Aug 25 03:17:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 03:17:15.366
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:15.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:15.38
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-d6f3fc10-48fa-40e2-b4db-1881da951801 08/25/22 03:17:15.384
    STEP: Creating a pod to test consume secrets 08/25/22 03:17:15.388
    Aug 25 03:17:15.395: INFO: Waiting up to 5m0s for pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656" in namespace "secrets-8279" to be "Succeeded or Failed"
    Aug 25 03:17:15.398: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656": Phase="Pending", Reason="", readiness=false. Elapsed: 2.806439ms
    Aug 25 03:17:17.404: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009460911s
    Aug 25 03:17:19.403: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007655108s
    STEP: Saw pod success 08/25/22 03:17:19.403
    Aug 25 03:17:19.403: INFO: Pod "pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656" satisfied condition "Succeeded or Failed"
    Aug 25 03:17:19.406: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:17:19.411
    Aug 25 03:17:19.417: INFO: Waiting for pod pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656 to disappear
    Aug 25 03:17:19.420: INFO: Pod pod-secrets-e9271a5b-9deb-4349-be78-13467edb6656 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 03:17:19.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8279" for this suite. 08/25/22 03:17:19.424
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:19.431
Aug 25 03:17:19.432: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:17:19.433
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:19.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:19.445
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 08/25/22 03:17:19.449
Aug 25 03:17:19.454: INFO: Waiting up to 5m0s for pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526" in namespace "downward-api-2862" to be "Succeeded or Failed"
Aug 25 03:17:19.456: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126641ms
Aug 25 03:17:21.461: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007811724s
Aug 25 03:17:23.461: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007304168s
STEP: Saw pod success 08/25/22 03:17:23.461
Aug 25 03:17:23.461: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526" satisfied condition "Succeeded or Failed"
Aug 25 03:17:23.465: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-b2eede08-c431-4b0e-98d3-8375addb3526 container dapi-container: <nil>
STEP: delete the pod 08/25/22 03:17:23.471
Aug 25 03:17:23.477: INFO: Waiting for pod downward-api-b2eede08-c431-4b0e-98d3-8375addb3526 to disappear
Aug 25 03:17:23.480: INFO: Pod downward-api-b2eede08-c431-4b0e-98d3-8375addb3526 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 25 03:17:23.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2862" for this suite. 08/25/22 03:17:23.484
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":101,"skipped":1731,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.057 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:19.431
    Aug 25 03:17:19.432: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:17:19.433
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:19.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:19.445
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 08/25/22 03:17:19.449
    Aug 25 03:17:19.454: INFO: Waiting up to 5m0s for pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526" in namespace "downward-api-2862" to be "Succeeded or Failed"
    Aug 25 03:17:19.456: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126641ms
    Aug 25 03:17:21.461: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007811724s
    Aug 25 03:17:23.461: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007304168s
    STEP: Saw pod success 08/25/22 03:17:23.461
    Aug 25 03:17:23.461: INFO: Pod "downward-api-b2eede08-c431-4b0e-98d3-8375addb3526" satisfied condition "Succeeded or Failed"
    Aug 25 03:17:23.465: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-b2eede08-c431-4b0e-98d3-8375addb3526 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 03:17:23.471
    Aug 25 03:17:23.477: INFO: Waiting for pod downward-api-b2eede08-c431-4b0e-98d3-8375addb3526 to disappear
    Aug 25 03:17:23.480: INFO: Pod downward-api-b2eede08-c431-4b0e-98d3-8375addb3526 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 25 03:17:23.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2862" for this suite. 08/25/22 03:17:23.484
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:23.49
Aug 25 03:17:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:17:23.491
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:23.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:23.507
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 08/25/22 03:17:23.511
Aug 25 03:17:23.512: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: mark a version not serverd 08/25/22 03:17:32.995
STEP: check the unserved version gets removed 08/25/22 03:17:33.013
STEP: check the other version is not changed 08/25/22 03:17:36.557
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:17:44.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8564" for this suite. 08/25/22 03:17:44.121
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":102,"skipped":1741,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [20.635 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:23.49
    Aug 25 03:17:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:17:23.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:23.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:23.507
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 08/25/22 03:17:23.511
    Aug 25 03:17:23.512: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: mark a version not serverd 08/25/22 03:17:32.995
    STEP: check the unserved version gets removed 08/25/22 03:17:33.013
    STEP: check the other version is not changed 08/25/22 03:17:36.557
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:17:44.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8564" for this suite. 08/25/22 03:17:44.121
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:44.128
Aug 25 03:17:44.128: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:17:44.13
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:44.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:44.145
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 08/25/22 03:17:44.148
Aug 25 03:17:44.148: INFO: namespace kubectl-3652
Aug 25 03:17:44.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 create -f -'
Aug 25 03:17:44.862: INFO: stderr: ""
Aug 25 03:17:44.862: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/25/22 03:17:44.862
Aug 25 03:17:45.867: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:17:45.867: INFO: Found 0 / 1
Aug 25 03:17:46.866: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:17:46.866: INFO: Found 1 / 1
Aug 25 03:17:46.866: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 25 03:17:46.870: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:17:46.870: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 25 03:17:46.870: INFO: wait on agnhost-primary startup in kubectl-3652 
Aug 25 03:17:46.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 logs agnhost-primary-2t4gq agnhost-primary'
Aug 25 03:17:46.956: INFO: stderr: ""
Aug 25 03:17:46.956: INFO: stdout: "Paused\n"
STEP: exposing RC 08/25/22 03:17:46.956
Aug 25 03:17:46.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 25 03:17:47.052: INFO: stderr: ""
Aug 25 03:17:47.052: INFO: stdout: "service/rm2 exposed\n"
Aug 25 03:17:47.056: INFO: Service rm2 in namespace kubectl-3652 found.
STEP: exposing service 08/25/22 03:17:49.065
Aug 25 03:17:49.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 25 03:17:49.158: INFO: stderr: ""
Aug 25 03:17:49.158: INFO: stdout: "service/rm3 exposed\n"
Aug 25 03:17:49.161: INFO: Service rm3 in namespace kubectl-3652 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:17:51.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3652" for this suite. 08/25/22 03:17:51.175
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":103,"skipped":1775,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.052 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:44.128
    Aug 25 03:17:44.128: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:17:44.13
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:44.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:44.145
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 08/25/22 03:17:44.148
    Aug 25 03:17:44.148: INFO: namespace kubectl-3652
    Aug 25 03:17:44.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 create -f -'
    Aug 25 03:17:44.862: INFO: stderr: ""
    Aug 25 03:17:44.862: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/25/22 03:17:44.862
    Aug 25 03:17:45.867: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:17:45.867: INFO: Found 0 / 1
    Aug 25 03:17:46.866: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:17:46.866: INFO: Found 1 / 1
    Aug 25 03:17:46.866: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 25 03:17:46.870: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:17:46.870: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 25 03:17:46.870: INFO: wait on agnhost-primary startup in kubectl-3652 
    Aug 25 03:17:46.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 logs agnhost-primary-2t4gq agnhost-primary'
    Aug 25 03:17:46.956: INFO: stderr: ""
    Aug 25 03:17:46.956: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/25/22 03:17:46.956
    Aug 25 03:17:46.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 25 03:17:47.052: INFO: stderr: ""
    Aug 25 03:17:47.052: INFO: stdout: "service/rm2 exposed\n"
    Aug 25 03:17:47.056: INFO: Service rm2 in namespace kubectl-3652 found.
    STEP: exposing service 08/25/22 03:17:49.065
    Aug 25 03:17:49.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3652 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 25 03:17:49.158: INFO: stderr: ""
    Aug 25 03:17:49.158: INFO: stdout: "service/rm3 exposed\n"
    Aug 25 03:17:49.161: INFO: Service rm3 in namespace kubectl-3652 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:17:51.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3652" for this suite. 08/25/22 03:17:51.175
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:51.181
Aug 25 03:17:51.181: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replicaset 08/25/22 03:17:51.182
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:51.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:51.202
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 25 03:17:51.221: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 25 03:17:56.224: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/25/22 03:17:56.224
STEP: Scaling up "test-rs" replicaset  08/25/22 03:17:56.224
Aug 25 03:17:56.232: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/25/22 03:17:56.232
W0825 03:17:56.236773      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 25 03:17:56.239: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
Aug 25 03:17:56.247: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
Aug 25 03:17:56.256: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
Aug 25 03:17:56.261: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
Aug 25 03:17:57.514: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 2, AvailableReplicas 2
Aug 25 03:17:57.521: INFO: observed Replicaset test-rs in namespace replicaset-7181 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 25 03:17:57.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7181" for this suite. 08/25/22 03:17:57.525
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":104,"skipped":1777,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.348 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:51.181
    Aug 25 03:17:51.181: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replicaset 08/25/22 03:17:51.182
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:51.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:51.202
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 25 03:17:51.221: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 25 03:17:56.224: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/25/22 03:17:56.224
    STEP: Scaling up "test-rs" replicaset  08/25/22 03:17:56.224
    Aug 25 03:17:56.232: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/25/22 03:17:56.232
    W0825 03:17:56.236773      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 25 03:17:56.239: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
    Aug 25 03:17:56.247: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
    Aug 25 03:17:56.256: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
    Aug 25 03:17:56.261: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 1, AvailableReplicas 1
    Aug 25 03:17:57.514: INFO: observed ReplicaSet test-rs in namespace replicaset-7181 with ReadyReplicas 2, AvailableReplicas 2
    Aug 25 03:17:57.521: INFO: observed Replicaset test-rs in namespace replicaset-7181 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 25 03:17:57.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7181" for this suite. 08/25/22 03:17:57.525
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:57.53
Aug 25 03:17:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:17:57.531
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:57.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:57.547
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 08/25/22 03:17:57.551
Aug 25 03:17:57.557: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614" in namespace "emptydir-4639" to be "running"
Aug 25 03:17:57.561: INFO: Pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614": Phase="Pending", Reason="", readiness=false. Elapsed: 4.216834ms
Aug 25 03:17:59.567: INFO: Pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614": Phase="Running", Reason="", readiness=false. Elapsed: 2.010094296s
Aug 25 03:17:59.567: INFO: Pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/25/22 03:17:59.567
Aug 25 03:17:59.567: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4639 PodName:pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:17:59.567: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:17:59.568: INFO: ExecWithOptions: Clientset creation
Aug 25 03:17:59.568: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4639/pods/pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 25 03:17:59.680: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:17:59.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4639" for this suite. 08/25/22 03:17:59.686
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":105,"skipped":1780,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.161 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:57.53
    Aug 25 03:17:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:17:57.531
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:57.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:57.547
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 08/25/22 03:17:57.551
    Aug 25 03:17:57.557: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614" in namespace "emptydir-4639" to be "running"
    Aug 25 03:17:57.561: INFO: Pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614": Phase="Pending", Reason="", readiness=false. Elapsed: 4.216834ms
    Aug 25 03:17:59.567: INFO: Pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614": Phase="Running", Reason="", readiness=false. Elapsed: 2.010094296s
    Aug 25 03:17:59.567: INFO: Pod "pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/25/22 03:17:59.567
    Aug 25 03:17:59.567: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4639 PodName:pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:17:59.567: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:17:59.568: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:17:59.568: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4639/pods/pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 25 03:17:59.680: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:17:59.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4639" for this suite. 08/25/22 03:17:59.686
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:59.693
Aug 25 03:17:59.693: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:17:59.694
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:59.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:59.711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 08/25/22 03:17:59.716
Aug 25 03:17:59.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-7830 api-versions'
Aug 25 03:17:59.795: INFO: stderr: ""
Aug 25 03:17:59.795: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.internal.knative.dev/v1alpha1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncaching.internal.knative.dev/v1alpha1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nexternaldns.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.fluxcd.io/v1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.internal.knative.dev/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.knative.dev/v1beta1\npolicy/v1\nprojectcontour.io/v1\nprojectcontour.io/v1alpha1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nserving.knative.dev/v1\nserving.knative.dev/v1alpha1\nserving.knative.dev/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:17:59.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7830" for this suite. 08/25/22 03:17:59.799
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":106,"skipped":1809,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.110 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:59.693
    Aug 25 03:17:59.693: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:17:59.694
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:59.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:59.711
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 08/25/22 03:17:59.716
    Aug 25 03:17:59.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-7830 api-versions'
    Aug 25 03:17:59.795: INFO: stderr: ""
    Aug 25 03:17:59.795: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.internal.knative.dev/v1alpha1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncaching.internal.knative.dev/v1alpha1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nexternaldns.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.fluxcd.io/v1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.internal.knative.dev/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.knative.dev/v1beta1\npolicy/v1\nprojectcontour.io/v1\nprojectcontour.io/v1alpha1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nserving.knative.dev/v1\nserving.knative.dev/v1alpha1\nserving.knative.dev/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:17:59.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7830" for this suite. 08/25/22 03:17:59.799
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:17:59.804
Aug 25 03:17:59.805: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-pred 08/25/22 03:17:59.806
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:59.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:59.822
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 25 03:17:59.826: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 25 03:17:59.832: INFO: Waiting for terminating namespaces to be deleted...
Aug 25 03:17:59.837: INFO: 
Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
Aug 25 03:17:59.867: INFO: environment-0 from bobymcbobs started at 2022-08-25 02:58:14 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.867: INFO: 	Container environment ready: true, restart count 0
Aug 25 03:17:59.867: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 03:17:59.867: INFO: cert-manager-66bd77df8f-ndf6l from cert-manager started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.867: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 03:17:59.867: INFO: cert-manager-cainjector-6495667ff4-j7vbr from cert-manager started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.867: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 03:17:59.868: INFO: cert-manager-webhook-59d6cdfb6f-b6qpv from cert-manager started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 03:17:59.868: INFO: contour-588fc6cc6d-9gf27 from contour-external started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:17:59.868: INFO: contour-588fc6cc6d-jspht from contour-external started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:17:59.868: INFO: envoy-w6h8q from contour-external started at 2022-08-25 02:58:47 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container envoy ready: true, restart count 0
Aug 25 03:17:59.868: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 03:17:59.868: INFO: contour-8597b78798-6xc8c from contour-internal started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:17:59.868: INFO: contour-8597b78798-rvx58 from contour-internal started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:17:59.868: INFO: envoy-9vl8n from contour-internal started at 2022-08-25 02:58:47 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container envoy ready: true, restart count 0
Aug 25 03:17:59.868: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 03:17:59.868: INFO: pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614 from emptydir-4639 started at 2022-08-25 03:17:57 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container busybox-main-container ready: true, restart count 0
Aug 25 03:17:59.868: INFO: 	Container busybox-sub-container ready: false, restart count 0
Aug 25 03:17:59.868: INFO: external-dns-67d79cbcd4-2t9mm from external-dns started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 03:17:59.868: INFO: helm-operator-7959478576-dx96s from helm-operator started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 03:17:59.868: INFO: knative-operator-594876444d-ns75x from knative-operator started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 03:17:59.868: INFO: activator-88b7df5c-gjfj9 from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container activator ready: true, restart count 0
Aug 25 03:17:59.868: INFO: autoscaler-7bf8ff94db-2pl8r from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 03:17:59.868: INFO: autoscaler-hpa-776c44cc57-2rrjv from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 03:17:59.868: INFO: controller-7fd644cbc6-558wp from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:17:59.868: INFO: domain-mapping-5bcd85fbc6-ksvjk from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 03:17:59.868: INFO: domainmapping-webhook-77f466bbc9-g29sv from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 03:17:59.868: INFO: net-certmanager-controller-6c8cb88879-4jbnf from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:17:59.868: INFO: net-certmanager-webhook-79cfb96f68-5nxmb from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container webhook ready: true, restart count 0
Aug 25 03:17:59.868: INFO: net-contour-controller-5c5fd89fdb-6mvbj from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:17:59.868: INFO: webhook-69d55f5549-chd9c from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container webhook ready: true, restart count 0
Aug 25 03:17:59.868: INFO: coredns-565d847f94-2jsfd from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container coredns ready: true, restart count 0
Aug 25 03:17:59.868: INFO: coredns-565d847f94-hv8kn from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container coredns ready: true, restart count 0
Aug 25 03:17:59.868: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container etcd ready: true, restart count 0
Aug 25 03:17:59.868: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 03:17:59.868: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 03:17:59.868: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 03:17:59.868: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 03:17:59.868: INFO: kubed-568bdbc6c4-5vhn9 from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container kubed ready: true, restart count 0
Aug 25 03:17:59.868: INFO: metrics-server-5cb46dccf6-swrff from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 03:17:59.868: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container weave ready: true, restart count 1
Aug 25 03:17:59.868: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 03:17:59.868: INFO: local-path-provisioner-56c55476f9-5hg9n from local-path-storage started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 03:17:59.868: INFO: controller-77cc7ff558-n8vft from metallb-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:17:59.868: INFO: speaker-ghnxf from metallb-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container speaker ready: true, restart count 0
Aug 25 03:17:59.868: INFO: distribution-7f9dff85c4-tjxf8 from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container distribution ready: true, restart count 0
Aug 25 03:17:59.868: INFO: environment-exposer-5bb6d44465-cvdbh from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 03:17:59.868: INFO: powerdns-5c6dbcf579-k5l4c from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 03:17:59.868: INFO: powerdns-db-7c897fddf4-899c6 from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 03:17:59.868: INFO: public-html-go-http-server-55b9f584d4-9dvd8 from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 03:17:59.868: INFO: reveal-multiplex-6bbbf59d6-lh5vq from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 03:17:59.868: INFO: test-rs-hfqb4 from replicaset-7181 started at 2022-08-25 03:17:56 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container httpd ready: true, restart count 0
Aug 25 03:17:59.868: INFO: test-rs-sv9nn from replicaset-7181 started at 2022-08-25 03:17:56 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container httpd ready: true, restart count 0
Aug 25 03:17:59.868: INFO: 	Container test-rs ready: true, restart count 0
Aug 25 03:17:59.868: INFO: test-rs-wqcgr from replicaset-7181 started at 2022-08-25 03:17:51 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container httpd ready: true, restart count 0
Aug 25 03:17:59.868: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 03:17:59.868: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container e2e ready: true, restart count 0
Aug 25 03:17:59.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 03:17:59.868: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 03:17:59.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 03:17:59.868: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/25/22 03:17:59.868
Aug 25 03:17:59.875: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9448" to be "running"
Aug 25 03:17:59.879: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.423006ms
Aug 25 03:18:01.883: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007696696s
Aug 25 03:18:01.883: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/25/22 03:18:01.887
STEP: Trying to apply a random label on the found node. 08/25/22 03:18:01.894
STEP: verifying the node has the label kubernetes.io/e2e-457c809e-0fcf-440d-9ff2-08e1a1a92699 42 08/25/22 03:18:01.904
STEP: Trying to relaunch the pod, now with labels. 08/25/22 03:18:01.909
Aug 25 03:18:01.914: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9448" to be "not pending"
Aug 25 03:18:01.916: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324978ms
Aug 25 03:18:03.922: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008061901s
Aug 25 03:18:03.922: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-457c809e-0fcf-440d-9ff2-08e1a1a92699 off the node bobymcbobs-c849-control-plane-p55pp 08/25/22 03:18:03.926
STEP: verifying the node doesn't have the label kubernetes.io/e2e-457c809e-0fcf-440d-9ff2-08e1a1a92699 08/25/22 03:18:03.942
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:18:03.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9448" for this suite. 08/25/22 03:18:03.949
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":107,"skipped":1825,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.150 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:17:59.804
    Aug 25 03:17:59.805: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-pred 08/25/22 03:17:59.806
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:17:59.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:17:59.822
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 25 03:17:59.826: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 25 03:17:59.832: INFO: Waiting for terminating namespaces to be deleted...
    Aug 25 03:17:59.837: INFO: 
    Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
    Aug 25 03:17:59.867: INFO: environment-0 from bobymcbobs started at 2022-08-25 02:58:14 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.867: INFO: 	Container environment ready: true, restart count 0
    Aug 25 03:17:59.867: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 03:17:59.867: INFO: cert-manager-66bd77df8f-ndf6l from cert-manager started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.867: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 03:17:59.867: INFO: cert-manager-cainjector-6495667ff4-j7vbr from cert-manager started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.867: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: cert-manager-webhook-59d6cdfb6f-b6qpv from cert-manager started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: contour-588fc6cc6d-9gf27 from contour-external started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: contour-588fc6cc6d-jspht from contour-external started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: envoy-w6h8q from contour-external started at 2022-08-25 02:58:47 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: contour-8597b78798-6xc8c from contour-internal started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: contour-8597b78798-rvx58 from contour-internal started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: envoy-9vl8n from contour-internal started at 2022-08-25 02:58:47 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: pod-sharedvolume-23c55dac-2bda-4b7d-b99b-1dc7adbe6614 from emptydir-4639 started at 2022-08-25 03:17:57 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container busybox-main-container ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: 	Container busybox-sub-container ready: false, restart count 0
    Aug 25 03:17:59.868: INFO: external-dns-67d79cbcd4-2t9mm from external-dns started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: helm-operator-7959478576-dx96s from helm-operator started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: knative-operator-594876444d-ns75x from knative-operator started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: activator-88b7df5c-gjfj9 from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container activator ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: autoscaler-7bf8ff94db-2pl8r from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: autoscaler-hpa-776c44cc57-2rrjv from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: controller-7fd644cbc6-558wp from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: domain-mapping-5bcd85fbc6-ksvjk from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: domainmapping-webhook-77f466bbc9-g29sv from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: net-certmanager-controller-6c8cb88879-4jbnf from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: net-certmanager-webhook-79cfb96f68-5nxmb from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: net-contour-controller-5c5fd89fdb-6mvbj from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: webhook-69d55f5549-chd9c from knative-serving started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: coredns-565d847f94-2jsfd from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: coredns-565d847f94-hv8kn from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: kubed-568bdbc6c4-5vhn9 from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: metrics-server-5cb46dccf6-swrff from kube-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container weave ready: true, restart count 1
    Aug 25 03:17:59.868: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: local-path-provisioner-56c55476f9-5hg9n from local-path-storage started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: controller-77cc7ff558-n8vft from metallb-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: speaker-ghnxf from metallb-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: distribution-7f9dff85c4-tjxf8 from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: environment-exposer-5bb6d44465-cvdbh from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: powerdns-5c6dbcf579-k5l4c from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: powerdns-db-7c897fddf4-899c6 from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: public-html-go-http-server-55b9f584d4-9dvd8 from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: reveal-multiplex-6bbbf59d6-lh5vq from pair-system started at 2022-08-25 02:57:59 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: test-rs-hfqb4 from replicaset-7181 started at 2022-08-25 03:17:56 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container httpd ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: test-rs-sv9nn from replicaset-7181 started at 2022-08-25 03:17:56 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container httpd ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: 	Container test-rs ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: test-rs-wqcgr from replicaset-7181 started at 2022-08-25 03:17:51 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container httpd ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 03:17:59.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 03:17:59.868: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/25/22 03:17:59.868
    Aug 25 03:17:59.875: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9448" to be "running"
    Aug 25 03:17:59.879: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.423006ms
    Aug 25 03:18:01.883: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007696696s
    Aug 25 03:18:01.883: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/25/22 03:18:01.887
    STEP: Trying to apply a random label on the found node. 08/25/22 03:18:01.894
    STEP: verifying the node has the label kubernetes.io/e2e-457c809e-0fcf-440d-9ff2-08e1a1a92699 42 08/25/22 03:18:01.904
    STEP: Trying to relaunch the pod, now with labels. 08/25/22 03:18:01.909
    Aug 25 03:18:01.914: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9448" to be "not pending"
    Aug 25 03:18:01.916: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324978ms
    Aug 25 03:18:03.922: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008061901s
    Aug 25 03:18:03.922: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-457c809e-0fcf-440d-9ff2-08e1a1a92699 off the node bobymcbobs-c849-control-plane-p55pp 08/25/22 03:18:03.926
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-457c809e-0fcf-440d-9ff2-08e1a1a92699 08/25/22 03:18:03.942
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:18:03.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9448" for this suite. 08/25/22 03:18:03.949
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:18:03.955
Aug 25 03:18:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:18:03.956
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:03.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:03.97
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-792e57ad-55b2-4524-961f-5358f041cf58 08/25/22 03:18:03.973
STEP: Creating a pod to test consume secrets 08/25/22 03:18:03.977
Aug 25 03:18:03.992: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf" in namespace "projected-8745" to be "Succeeded or Failed"
Aug 25 03:18:03.999: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.999095ms
Aug 25 03:18:06.003: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011384538s
Aug 25 03:18:08.004: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011919878s
STEP: Saw pod success 08/25/22 03:18:08.004
Aug 25 03:18:08.004: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf" satisfied condition "Succeeded or Failed"
Aug 25 03:18:08.008: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf container projected-secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:18:08.018
Aug 25 03:18:08.024: INFO: Waiting for pod pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf to disappear
Aug 25 03:18:08.026: INFO: Pod pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 03:18:08.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8745" for this suite. 08/25/22 03:18:08.028
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":108,"skipped":1829,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.076 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:18:03.955
    Aug 25 03:18:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:18:03.956
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:03.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:03.97
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-792e57ad-55b2-4524-961f-5358f041cf58 08/25/22 03:18:03.973
    STEP: Creating a pod to test consume secrets 08/25/22 03:18:03.977
    Aug 25 03:18:03.992: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf" in namespace "projected-8745" to be "Succeeded or Failed"
    Aug 25 03:18:03.999: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.999095ms
    Aug 25 03:18:06.003: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011384538s
    Aug 25 03:18:08.004: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011919878s
    STEP: Saw pod success 08/25/22 03:18:08.004
    Aug 25 03:18:08.004: INFO: Pod "pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf" satisfied condition "Succeeded or Failed"
    Aug 25 03:18:08.008: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:18:08.018
    Aug 25 03:18:08.024: INFO: Waiting for pod pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf to disappear
    Aug 25 03:18:08.026: INFO: Pod pod-projected-secrets-a080527b-96e6-4bf1-b5dc-f91d507e3faf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 03:18:08.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8745" for this suite. 08/25/22 03:18:08.028
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:18:08.034
Aug 25 03:18:08.034: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:18:08.035
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:08.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:08.046
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 08/25/22 03:18:08.049
Aug 25 03:18:08.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 25 03:18:08.138: INFO: stderr: ""
Aug 25 03:18:08.138: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 08/25/22 03:18:08.138
Aug 25 03:18:08.138: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 25 03:18:08.138: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1658" to be "running and ready, or succeeded"
Aug 25 03:18:08.142: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081059ms
Aug 25 03:18:08.142: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'bobymcbobs-c849-control-plane-p55pp' to be 'Running' but was 'Pending'
Aug 25 03:18:10.147: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009115765s
Aug 25 03:18:10.147: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 25 03:18:10.147: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/25/22 03:18:10.147
Aug 25 03:18:10.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator'
Aug 25 03:18:10.231: INFO: stderr: ""
Aug 25 03:18:10.231: INFO: stdout: "I0825 03:18:09.001580       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/c92q 243\nI0825 03:18:09.201971       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/zjs 242\nI0825 03:18:09.402268       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/ch8x 492\nI0825 03:18:09.602569       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/94sj 489\nI0825 03:18:09.801916       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/26c6 346\nI0825 03:18:10.002175       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx2b 554\nI0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\n"
STEP: limiting log lines 08/25/22 03:18:10.231
Aug 25 03:18:10.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --tail=1'
Aug 25 03:18:10.318: INFO: stderr: ""
Aug 25 03:18:10.318: INFO: stdout: "I0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\n"
Aug 25 03:18:10.318: INFO: got output "I0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\n"
STEP: limiting log bytes 08/25/22 03:18:10.318
Aug 25 03:18:10.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --limit-bytes=1'
Aug 25 03:18:10.406: INFO: stderr: ""
Aug 25 03:18:10.406: INFO: stdout: "I"
Aug 25 03:18:10.406: INFO: got output "I"
STEP: exposing timestamps 08/25/22 03:18:10.406
Aug 25 03:18:10.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 25 03:18:10.484: INFO: stderr: ""
Aug 25 03:18:10.484: INFO: stdout: "2022-08-25T03:18:10.401899203Z I0825 03:18:10.401805       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/g4n 381\n"
Aug 25 03:18:10.484: INFO: got output "2022-08-25T03:18:10.401899203Z I0825 03:18:10.401805       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/g4n 381\n"
STEP: restricting to a time range 08/25/22 03:18:10.484
Aug 25 03:18:12.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --since=1s'
Aug 25 03:18:13.062: INFO: stderr: ""
Aug 25 03:18:13.062: INFO: stdout: "I0825 03:18:12.202255       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bnm 590\nI0825 03:18:12.402642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/bxdl 393\nI0825 03:18:12.602003       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/lz9 599\nI0825 03:18:12.802429       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/whp 409\nI0825 03:18:13.001790       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gnk9 443\n"
Aug 25 03:18:13.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --since=24h'
Aug 25 03:18:13.144: INFO: stderr: ""
Aug 25 03:18:13.144: INFO: stdout: "I0825 03:18:09.001580       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/c92q 243\nI0825 03:18:09.201971       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/zjs 242\nI0825 03:18:09.402268       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/ch8x 492\nI0825 03:18:09.602569       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/94sj 489\nI0825 03:18:09.801916       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/26c6 346\nI0825 03:18:10.002175       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx2b 554\nI0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\nI0825 03:18:10.401805       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/g4n 381\nI0825 03:18:10.602153       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4fj 217\nI0825 03:18:10.802576       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wds 231\nI0825 03:18:11.001985       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/9mkr 223\nI0825 03:18:11.202412       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/4k6 257\nI0825 03:18:11.401752       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ltp 347\nI0825 03:18:11.602120       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/7b55 278\nI0825 03:18:11.802484       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/wv2 200\nI0825 03:18:12.001838       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/xthb 277\nI0825 03:18:12.202255       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bnm 590\nI0825 03:18:12.402642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/bxdl 393\nI0825 03:18:12.602003       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/lz9 599\nI0825 03:18:12.802429       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/whp 409\nI0825 03:18:13.001790       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gnk9 443\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Aug 25 03:18:13.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 delete pod logs-generator'
Aug 25 03:18:14.639: INFO: stderr: ""
Aug 25 03:18:14.639: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:18:14.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1658" for this suite. 08/25/22 03:18:14.642
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":109,"skipped":1891,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.616 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:18:08.034
    Aug 25 03:18:08.034: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:18:08.035
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:08.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:08.046
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 08/25/22 03:18:08.049
    Aug 25 03:18:08.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 25 03:18:08.138: INFO: stderr: ""
    Aug 25 03:18:08.138: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 08/25/22 03:18:08.138
    Aug 25 03:18:08.138: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 25 03:18:08.138: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1658" to be "running and ready, or succeeded"
    Aug 25 03:18:08.142: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081059ms
    Aug 25 03:18:08.142: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'bobymcbobs-c849-control-plane-p55pp' to be 'Running' but was 'Pending'
    Aug 25 03:18:10.147: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009115765s
    Aug 25 03:18:10.147: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 25 03:18:10.147: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/25/22 03:18:10.147
    Aug 25 03:18:10.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator'
    Aug 25 03:18:10.231: INFO: stderr: ""
    Aug 25 03:18:10.231: INFO: stdout: "I0825 03:18:09.001580       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/c92q 243\nI0825 03:18:09.201971       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/zjs 242\nI0825 03:18:09.402268       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/ch8x 492\nI0825 03:18:09.602569       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/94sj 489\nI0825 03:18:09.801916       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/26c6 346\nI0825 03:18:10.002175       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx2b 554\nI0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\n"
    STEP: limiting log lines 08/25/22 03:18:10.231
    Aug 25 03:18:10.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --tail=1'
    Aug 25 03:18:10.318: INFO: stderr: ""
    Aug 25 03:18:10.318: INFO: stdout: "I0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\n"
    Aug 25 03:18:10.318: INFO: got output "I0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\n"
    STEP: limiting log bytes 08/25/22 03:18:10.318
    Aug 25 03:18:10.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --limit-bytes=1'
    Aug 25 03:18:10.406: INFO: stderr: ""
    Aug 25 03:18:10.406: INFO: stdout: "I"
    Aug 25 03:18:10.406: INFO: got output "I"
    STEP: exposing timestamps 08/25/22 03:18:10.406
    Aug 25 03:18:10.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 25 03:18:10.484: INFO: stderr: ""
    Aug 25 03:18:10.484: INFO: stdout: "2022-08-25T03:18:10.401899203Z I0825 03:18:10.401805       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/g4n 381\n"
    Aug 25 03:18:10.484: INFO: got output "2022-08-25T03:18:10.401899203Z I0825 03:18:10.401805       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/g4n 381\n"
    STEP: restricting to a time range 08/25/22 03:18:10.484
    Aug 25 03:18:12.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --since=1s'
    Aug 25 03:18:13.062: INFO: stderr: ""
    Aug 25 03:18:13.062: INFO: stdout: "I0825 03:18:12.202255       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bnm 590\nI0825 03:18:12.402642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/bxdl 393\nI0825 03:18:12.602003       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/lz9 599\nI0825 03:18:12.802429       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/whp 409\nI0825 03:18:13.001790       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gnk9 443\n"
    Aug 25 03:18:13.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 logs logs-generator logs-generator --since=24h'
    Aug 25 03:18:13.144: INFO: stderr: ""
    Aug 25 03:18:13.144: INFO: stdout: "I0825 03:18:09.001580       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/c92q 243\nI0825 03:18:09.201971       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/zjs 242\nI0825 03:18:09.402268       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/ch8x 492\nI0825 03:18:09.602569       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/94sj 489\nI0825 03:18:09.801916       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/26c6 346\nI0825 03:18:10.002175       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/fx2b 554\nI0825 03:18:10.202514       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/bxq4 517\nI0825 03:18:10.401805       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/g4n 381\nI0825 03:18:10.602153       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4fj 217\nI0825 03:18:10.802576       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wds 231\nI0825 03:18:11.001985       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/9mkr 223\nI0825 03:18:11.202412       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/4k6 257\nI0825 03:18:11.401752       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ltp 347\nI0825 03:18:11.602120       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/7b55 278\nI0825 03:18:11.802484       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/wv2 200\nI0825 03:18:12.001838       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/xthb 277\nI0825 03:18:12.202255       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/bnm 590\nI0825 03:18:12.402642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/bxdl 393\nI0825 03:18:12.602003       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/lz9 599\nI0825 03:18:12.802429       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/whp 409\nI0825 03:18:13.001790       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gnk9 443\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Aug 25 03:18:13.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1658 delete pod logs-generator'
    Aug 25 03:18:14.639: INFO: stderr: ""
    Aug 25 03:18:14.639: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:18:14.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1658" for this suite. 08/25/22 03:18:14.642
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:18:14.651
Aug 25 03:18:14.651: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename runtimeclass 08/25/22 03:18:14.652
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:14.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:14.668
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 25 03:18:14.679: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3277 to be scheduled
Aug 25 03:18:14.681: INFO: 1 pods are not scheduled: [runtimeclass-3277/test-runtimeclass-runtimeclass-3277-preconfigured-handler-czvlv(437a843e-176a-40ed-9803-e17217425384)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 25 03:18:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3277" for this suite. 08/25/22 03:18:16.696
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":110,"skipped":1902,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:18:14.651
    Aug 25 03:18:14.651: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename runtimeclass 08/25/22 03:18:14.652
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:14.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:14.668
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 25 03:18:14.679: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3277 to be scheduled
    Aug 25 03:18:14.681: INFO: 1 pods are not scheduled: [runtimeclass-3277/test-runtimeclass-runtimeclass-3277-preconfigured-handler-czvlv(437a843e-176a-40ed-9803-e17217425384)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 25 03:18:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3277" for this suite. 08/25/22 03:18:16.696
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:18:16.706
Aug 25 03:18:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 03:18:16.707
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:16.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:16.723
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/25/22 03:18:16.736
Aug 25 03:18:16.741: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6677" to be "running and ready"
Aug 25 03:18:16.744: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180937ms
Aug 25 03:18:16.744: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:18:18.749: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007164365s
Aug 25 03:18:18.749: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 25 03:18:18.749: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 08/25/22 03:18:18.753
Aug 25 03:18:18.758: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6677" to be "running and ready"
Aug 25 03:18:18.762: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882498ms
Aug 25 03:18:18.762: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:18:20.767: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492328s
Aug 25 03:18:20.767: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 25 03:18:20.767: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/25/22 03:18:20.771
Aug 25 03:18:20.775: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 25 03:18:20.778: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 25 03:18:22.780: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 25 03:18:22.784: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/25/22 03:18:22.784
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 25 03:18:22.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6677" for this suite. 08/25/22 03:18:22.794
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":111,"skipped":1955,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:18:16.706
    Aug 25 03:18:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 03:18:16.707
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:16.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:16.723
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/25/22 03:18:16.736
    Aug 25 03:18:16.741: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6677" to be "running and ready"
    Aug 25 03:18:16.744: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180937ms
    Aug 25 03:18:16.744: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:18:18.749: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007164365s
    Aug 25 03:18:18.749: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 25 03:18:18.749: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 08/25/22 03:18:18.753
    Aug 25 03:18:18.758: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6677" to be "running and ready"
    Aug 25 03:18:18.762: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882498ms
    Aug 25 03:18:18.762: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:18:20.767: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492328s
    Aug 25 03:18:20.767: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 25 03:18:20.767: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/25/22 03:18:20.771
    Aug 25 03:18:20.775: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 25 03:18:20.778: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 25 03:18:22.780: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 25 03:18:22.784: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/25/22 03:18:22.784
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 25 03:18:22.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6677" for this suite. 08/25/22 03:18:22.794
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:18:22.802
Aug 25 03:18:22.803: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:18:22.804
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:22.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:22.82
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/25/22 03:18:22.825
Aug 25 03:18:22.825: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:18:26.933: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:18:41.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3130" for this suite. 08/25/22 03:18:41.16
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":112,"skipped":2007,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [18.362 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:18:22.802
    Aug 25 03:18:22.803: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:18:22.804
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:22.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:22.82
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/25/22 03:18:22.825
    Aug 25 03:18:22.825: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:18:26.933: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:18:41.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3130" for this suite. 08/25/22 03:18:41.16
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:18:41.167
Aug 25 03:18:41.167: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 03:18:41.168
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:41.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:41.185
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4430 08/25/22 03:18:41.191
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-4430 08/25/22 03:18:41.198
Aug 25 03:18:41.204: INFO: Found 0 stateful pods, waiting for 1
Aug 25 03:18:51.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/25/22 03:18:51.218
STEP: Getting /status 08/25/22 03:18:51.232
Aug 25 03:18:51.238: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/25/22 03:18:51.238
Aug 25 03:18:51.248: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/25/22 03:18:51.248
Aug 25 03:18:51.250: INFO: Observed &StatefulSet event: ADDED
Aug 25 03:18:51.250: INFO: Found Statefulset ss in namespace statefulset-4430 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 25 03:18:51.250: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/25/22 03:18:51.25
Aug 25 03:18:51.251: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 25 03:18:51.256: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/25/22 03:18:51.256
Aug 25 03:18:51.258: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 03:18:51.259: INFO: Deleting all statefulset in ns statefulset-4430
Aug 25 03:18:51.262: INFO: Scaling statefulset ss to 0
Aug 25 03:19:01.280: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:19:01.284: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 03:19:01.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4430" for this suite. 08/25/22 03:19:01.302
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":113,"skipped":2025,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [20.140 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:18:41.167
    Aug 25 03:18:41.167: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 03:18:41.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:18:41.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:18:41.185
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4430 08/25/22 03:18:41.191
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-4430 08/25/22 03:18:41.198
    Aug 25 03:18:41.204: INFO: Found 0 stateful pods, waiting for 1
    Aug 25 03:18:51.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/25/22 03:18:51.218
    STEP: Getting /status 08/25/22 03:18:51.232
    Aug 25 03:18:51.238: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/25/22 03:18:51.238
    Aug 25 03:18:51.248: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/25/22 03:18:51.248
    Aug 25 03:18:51.250: INFO: Observed &StatefulSet event: ADDED
    Aug 25 03:18:51.250: INFO: Found Statefulset ss in namespace statefulset-4430 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 25 03:18:51.250: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/25/22 03:18:51.25
    Aug 25 03:18:51.251: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 25 03:18:51.256: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/25/22 03:18:51.256
    Aug 25 03:18:51.258: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 03:18:51.259: INFO: Deleting all statefulset in ns statefulset-4430
    Aug 25 03:18:51.262: INFO: Scaling statefulset ss to 0
    Aug 25 03:19:01.280: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:19:01.284: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 03:19:01.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4430" for this suite. 08/25/22 03:19:01.302
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:01.307
Aug 25 03:19:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 03:19:01.309
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:01.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:01.324
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 08/25/22 03:19:01.328
STEP: Creating a ResourceQuota 08/25/22 03:19:06.332
STEP: Ensuring resource quota status is calculated 08/25/22 03:19:06.336
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 03:19:08.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3904" for this suite. 08/25/22 03:19:08.345
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":114,"skipped":2025,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.043 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:01.307
    Aug 25 03:19:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 03:19:01.309
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:01.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:01.324
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 08/25/22 03:19:01.328
    STEP: Creating a ResourceQuota 08/25/22 03:19:06.332
    STEP: Ensuring resource quota status is calculated 08/25/22 03:19:06.336
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 03:19:08.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3904" for this suite. 08/25/22 03:19:08.345
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:08.353
Aug 25 03:19:08.353: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 03:19:08.354
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:08.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:08.371
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-949ba823-fd04-47e5-a436-bce332f75668 08/25/22 03:19:08.374
STEP: Creating a pod to test consume configMaps 08/25/22 03:19:08.377
Aug 25 03:19:08.383: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671" in namespace "configmap-8098" to be "Succeeded or Failed"
Aug 25 03:19:08.386: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924933ms
Aug 25 03:19:10.390: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006906002s
Aug 25 03:19:12.392: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008647431s
STEP: Saw pod success 08/25/22 03:19:12.392
Aug 25 03:19:12.392: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671" satisfied condition "Succeeded or Failed"
Aug 25 03:19:12.396: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:19:12.401
Aug 25 03:19:12.408: INFO: Waiting for pod pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671 to disappear
Aug 25 03:19:12.410: INFO: Pod pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 03:19:12.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8098" for this suite. 08/25/22 03:19:12.414
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":115,"skipped":2060,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:08.353
    Aug 25 03:19:08.353: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 03:19:08.354
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:08.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:08.371
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-949ba823-fd04-47e5-a436-bce332f75668 08/25/22 03:19:08.374
    STEP: Creating a pod to test consume configMaps 08/25/22 03:19:08.377
    Aug 25 03:19:08.383: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671" in namespace "configmap-8098" to be "Succeeded or Failed"
    Aug 25 03:19:08.386: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924933ms
    Aug 25 03:19:10.390: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006906002s
    Aug 25 03:19:12.392: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008647431s
    STEP: Saw pod success 08/25/22 03:19:12.392
    Aug 25 03:19:12.392: INFO: Pod "pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671" satisfied condition "Succeeded or Failed"
    Aug 25 03:19:12.396: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:19:12.401
    Aug 25 03:19:12.408: INFO: Waiting for pod pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671 to disappear
    Aug 25 03:19:12.410: INFO: Pod pod-configmaps-7a42cdf5-e16b-40b4-bce0-f4c395a1c671 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 03:19:12.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8098" for this suite. 08/25/22 03:19:12.414
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:12.421
Aug 25 03:19:12.421: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename endpointslice 08/25/22 03:19:12.422
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:12.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:12.435
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 08/25/22 03:19:12.438
STEP: getting /apis/discovery.k8s.io 08/25/22 03:19:12.441
STEP: getting /apis/discovery.k8s.iov1 08/25/22 03:19:12.442
STEP: creating 08/25/22 03:19:12.444
STEP: getting 08/25/22 03:19:12.454
STEP: listing 08/25/22 03:19:12.457
STEP: watching 08/25/22 03:19:12.46
Aug 25 03:19:12.460: INFO: starting watch
STEP: cluster-wide listing 08/25/22 03:19:12.462
STEP: cluster-wide watching 08/25/22 03:19:12.465
Aug 25 03:19:12.465: INFO: starting watch
STEP: patching 08/25/22 03:19:12.467
STEP: updating 08/25/22 03:19:12.471
Aug 25 03:19:12.478: INFO: waiting for watch events with expected annotations
Aug 25 03:19:12.478: INFO: saw patched and updated annotations
STEP: deleting 08/25/22 03:19:12.478
STEP: deleting a collection 08/25/22 03:19:12.488
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 25 03:19:12.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5557" for this suite. 08/25/22 03:19:12.503
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":116,"skipped":2064,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.086 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:12.421
    Aug 25 03:19:12.421: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename endpointslice 08/25/22 03:19:12.422
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:12.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:12.435
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 08/25/22 03:19:12.438
    STEP: getting /apis/discovery.k8s.io 08/25/22 03:19:12.441
    STEP: getting /apis/discovery.k8s.iov1 08/25/22 03:19:12.442
    STEP: creating 08/25/22 03:19:12.444
    STEP: getting 08/25/22 03:19:12.454
    STEP: listing 08/25/22 03:19:12.457
    STEP: watching 08/25/22 03:19:12.46
    Aug 25 03:19:12.460: INFO: starting watch
    STEP: cluster-wide listing 08/25/22 03:19:12.462
    STEP: cluster-wide watching 08/25/22 03:19:12.465
    Aug 25 03:19:12.465: INFO: starting watch
    STEP: patching 08/25/22 03:19:12.467
    STEP: updating 08/25/22 03:19:12.471
    Aug 25 03:19:12.478: INFO: waiting for watch events with expected annotations
    Aug 25 03:19:12.478: INFO: saw patched and updated annotations
    STEP: deleting 08/25/22 03:19:12.478
    STEP: deleting a collection 08/25/22 03:19:12.488
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 25 03:19:12.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5557" for this suite. 08/25/22 03:19:12.503
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:12.508
Aug 25 03:19:12.509: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:19:12.51
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:12.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:12.525
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:19:12.528
Aug 25 03:19:12.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12" in namespace "downward-api-7432" to be "Succeeded or Failed"
Aug 25 03:19:12.538: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.651994ms
Aug 25 03:19:14.544: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008446408s
Aug 25 03:19:16.543: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008144084s
STEP: Saw pod success 08/25/22 03:19:16.543
Aug 25 03:19:16.544: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12" satisfied condition "Succeeded or Failed"
Aug 25 03:19:16.548: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12 container client-container: <nil>
STEP: delete the pod 08/25/22 03:19:16.553
Aug 25 03:19:16.561: INFO: Waiting for pod downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12 to disappear
Aug 25 03:19:16.564: INFO: Pod downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 03:19:16.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7432" for this suite. 08/25/22 03:19:16.569
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":117,"skipped":2073,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:12.508
    Aug 25 03:19:12.509: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:19:12.51
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:12.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:12.525
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:19:12.528
    Aug 25 03:19:12.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12" in namespace "downward-api-7432" to be "Succeeded or Failed"
    Aug 25 03:19:12.538: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.651994ms
    Aug 25 03:19:14.544: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008446408s
    Aug 25 03:19:16.543: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008144084s
    STEP: Saw pod success 08/25/22 03:19:16.543
    Aug 25 03:19:16.544: INFO: Pod "downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12" satisfied condition "Succeeded or Failed"
    Aug 25 03:19:16.548: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:19:16.553
    Aug 25 03:19:16.561: INFO: Waiting for pod downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12 to disappear
    Aug 25 03:19:16.564: INFO: Pod downwardapi-volume-9fc54cf0-7c99-4c98-ae2f-56168161bb12 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 03:19:16.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7432" for this suite. 08/25/22 03:19:16.569
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:16.575
Aug 25 03:19:16.576: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:19:16.576
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:16.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:16.591
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-aaaafe9f-2d52-44bd-8f71-3ee3db338d4d 08/25/22 03:19:16.595
STEP: Creating a pod to test consume configMaps 08/25/22 03:19:16.599
Aug 25 03:19:16.605: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076" in namespace "projected-2950" to be "Succeeded or Failed"
Aug 25 03:19:16.609: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940475ms
Aug 25 03:19:18.614: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0086701s
Aug 25 03:19:20.613: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008364646s
STEP: Saw pod success 08/25/22 03:19:20.614
Aug 25 03:19:20.614: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076" satisfied condition "Succeeded or Failed"
Aug 25 03:19:20.618: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:19:20.623
Aug 25 03:19:20.630: INFO: Waiting for pod pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076 to disappear
Aug 25 03:19:20.634: INFO: Pod pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 03:19:20.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2950" for this suite. 08/25/22 03:19:20.638
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":118,"skipped":2103,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:16.575
    Aug 25 03:19:16.576: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:19:16.576
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:16.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:16.591
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-aaaafe9f-2d52-44bd-8f71-3ee3db338d4d 08/25/22 03:19:16.595
    STEP: Creating a pod to test consume configMaps 08/25/22 03:19:16.599
    Aug 25 03:19:16.605: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076" in namespace "projected-2950" to be "Succeeded or Failed"
    Aug 25 03:19:16.609: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940475ms
    Aug 25 03:19:18.614: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0086701s
    Aug 25 03:19:20.613: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008364646s
    STEP: Saw pod success 08/25/22 03:19:20.614
    Aug 25 03:19:20.614: INFO: Pod "pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076" satisfied condition "Succeeded or Failed"
    Aug 25 03:19:20.618: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:19:20.623
    Aug 25 03:19:20.630: INFO: Waiting for pod pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076 to disappear
    Aug 25 03:19:20.634: INFO: Pod pod-projected-configmaps-900a4336-f4ac-4fcc-b65d-edde9f300076 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 03:19:20.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2950" for this suite. 08/25/22 03:19:20.638
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:20.647
Aug 25 03:19:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 03:19:20.649
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:20.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:20.663
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/25/22 03:19:20.673
STEP: waiting for Deployment to be created 08/25/22 03:19:20.678
STEP: waiting for all Replicas to be Ready 08/25/22 03:19:20.681
Aug 25 03:19:20.682: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.682: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.687: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.687: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.696: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.696: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.712: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:20.712: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 25 03:19:21.972: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 25 03:19:21.972: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 25 03:19:21.981: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/25/22 03:19:21.981
W0825 03:19:21.985323      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 25 03:19:21.987: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/25/22 03:19:21.987
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:21.994: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:21.994: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:22.002: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:22.002: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:22.007: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:22.007: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:22.014: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:22.014: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:22.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:22.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:23.005: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
STEP: listing Deployments 08/25/22 03:19:23.005
Aug 25 03:19:23.011: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/25/22 03:19:23.011
Aug 25 03:19:23.029: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/25/22 03:19:23.029
Aug 25 03:19:23.039: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:23.039: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:23.050: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:23.060: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:23.065: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:23.990: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:24.000: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:24.009: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:24.014: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 25 03:19:27.126: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/25/22 03:19:27.144
STEP: fetching the DeploymentStatus 08/25/22 03:19:27.152
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:27.159: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
Aug 25 03:19:27.159: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 3
STEP: deleting the Deployment 08/25/22 03:19:27.159
Aug 25 03:19:27.169: INFO: observed event type MODIFIED
Aug 25 03:19:27.169: INFO: observed event type MODIFIED
Aug 25 03:19:27.169: INFO: observed event type MODIFIED
Aug 25 03:19:27.169: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
Aug 25 03:19:27.170: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 03:19:27.172: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 03:19:27.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5682" for this suite. 08/25/22 03:19:27.176
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":119,"skipped":2172,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.536 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:20.647
    Aug 25 03:19:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 03:19:20.649
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:20.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:20.663
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/25/22 03:19:20.673
    STEP: waiting for Deployment to be created 08/25/22 03:19:20.678
    STEP: waiting for all Replicas to be Ready 08/25/22 03:19:20.681
    Aug 25 03:19:20.682: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.682: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.687: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.687: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.696: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.696: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.712: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:20.712: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 25 03:19:21.972: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 25 03:19:21.972: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 25 03:19:21.981: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/25/22 03:19:21.981
    W0825 03:19:21.985323      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 25 03:19:21.987: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/25/22 03:19:21.987
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.988: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 0
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:21.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:21.994: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:21.994: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:22.002: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:22.002: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:22.007: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:22.007: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:22.014: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:22.014: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:22.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:22.989: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:23.005: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    STEP: listing Deployments 08/25/22 03:19:23.005
    Aug 25 03:19:23.011: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/25/22 03:19:23.011
    Aug 25 03:19:23.029: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/25/22 03:19:23.029
    Aug 25 03:19:23.039: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:23.039: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:23.050: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:23.060: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:23.065: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:23.990: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:24.000: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:24.009: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:24.014: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 25 03:19:27.126: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/25/22 03:19:27.144
    STEP: fetching the DeploymentStatus 08/25/22 03:19:27.152
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 1
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:27.158: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:27.159: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 2
    Aug 25 03:19:27.159: INFO: observed Deployment test-deployment in namespace deployment-5682 with ReadyReplicas 3
    STEP: deleting the Deployment 08/25/22 03:19:27.159
    Aug 25 03:19:27.169: INFO: observed event type MODIFIED
    Aug 25 03:19:27.169: INFO: observed event type MODIFIED
    Aug 25 03:19:27.169: INFO: observed event type MODIFIED
    Aug 25 03:19:27.169: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    Aug 25 03:19:27.170: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 03:19:27.172: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 03:19:27.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5682" for this suite. 08/25/22 03:19:27.176
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:27.185
Aug 25 03:19:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:19:27.185
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:27.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:27.198
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:19:27.211
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:19:27.547
STEP: Deploying the webhook pod 08/25/22 03:19:27.555
STEP: Wait for the deployment to be ready 08/25/22 03:19:27.565
Aug 25 03:19:27.572: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 03:19:29.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:19:31.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:19:33.593
STEP: Verifying the service has paired with the endpoint 08/25/22 03:19:33.602
Aug 25 03:19:34.602: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Aug 25 03:19:34.606: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3377-crds.webhook.example.com via the AdmissionRegistration API 08/25/22 03:19:35.119
STEP: Creating a custom resource that should be mutated by the webhook 08/25/22 03:19:35.141
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:19:37.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2599" for this suite. 08/25/22 03:19:37.715
STEP: Destroying namespace "webhook-2599-markers" for this suite. 08/25/22 03:19:37.72
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":120,"skipped":2187,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.564 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:27.185
    Aug 25 03:19:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:19:27.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:27.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:27.198
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:19:27.211
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:19:27.547
    STEP: Deploying the webhook pod 08/25/22 03:19:27.555
    STEP: Wait for the deployment to be ready 08/25/22 03:19:27.565
    Aug 25 03:19:27.572: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 03:19:29.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:19:31.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 19, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:19:33.593
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:19:33.602
    Aug 25 03:19:34.602: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Aug 25 03:19:34.606: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3377-crds.webhook.example.com via the AdmissionRegistration API 08/25/22 03:19:35.119
    STEP: Creating a custom resource that should be mutated by the webhook 08/25/22 03:19:35.141
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:19:37.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2599" for this suite. 08/25/22 03:19:37.715
    STEP: Destroying namespace "webhook-2599-markers" for this suite. 08/25/22 03:19:37.72
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:37.749
Aug 25 03:19:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:19:37.75
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:37.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:37.764
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:19:37.768
Aug 25 03:19:37.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e" in namespace "projected-9890" to be "Succeeded or Failed"
Aug 25 03:19:37.778: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262698ms
Aug 25 03:19:39.782: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007369258s
Aug 25 03:19:41.784: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009507401s
STEP: Saw pod success 08/25/22 03:19:41.784
Aug 25 03:19:41.784: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e" satisfied condition "Succeeded or Failed"
Aug 25 03:19:41.789: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e container client-container: <nil>
STEP: delete the pod 08/25/22 03:19:41.794
Aug 25 03:19:41.802: INFO: Waiting for pod downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e to disappear
Aug 25 03:19:41.805: INFO: Pod downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:19:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9890" for this suite. 08/25/22 03:19:41.809
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":121,"skipped":2189,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:37.749
    Aug 25 03:19:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:19:37.75
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:37.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:37.764
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:19:37.768
    Aug 25 03:19:37.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e" in namespace "projected-9890" to be "Succeeded or Failed"
    Aug 25 03:19:37.778: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262698ms
    Aug 25 03:19:39.782: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007369258s
    Aug 25 03:19:41.784: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009507401s
    STEP: Saw pod success 08/25/22 03:19:41.784
    Aug 25 03:19:41.784: INFO: Pod "downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e" satisfied condition "Succeeded or Failed"
    Aug 25 03:19:41.789: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e container client-container: <nil>
    STEP: delete the pod 08/25/22 03:19:41.794
    Aug 25 03:19:41.802: INFO: Waiting for pod downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e to disappear
    Aug 25 03:19:41.805: INFO: Pod downwardapi-volume-0268316d-d22a-4472-8141-d384e5810c9e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:19:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9890" for this suite. 08/25/22 03:19:41.809
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:19:41.819
Aug 25 03:19:41.819: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename subpath 08/25/22 03:19:41.82
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:41.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:41.844
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/25/22 03:19:41.848
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-bbx8 08/25/22 03:19:41.855
STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:19:41.855
Aug 25 03:19:41.862: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bbx8" in namespace "subpath-2385" to be "Succeeded or Failed"
Aug 25 03:19:41.865: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213052ms
Aug 25 03:19:43.871: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009324491s
Aug 25 03:19:45.869: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 4.007777242s
Aug 25 03:19:47.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 6.008381361s
Aug 25 03:19:49.872: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 8.009933406s
Aug 25 03:19:51.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 10.008067964s
Aug 25 03:19:53.872: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 12.010083426s
Aug 25 03:19:55.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 14.008591174s
Aug 25 03:19:57.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 16.007899675s
Aug 25 03:19:59.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 18.007890715s
Aug 25 03:20:01.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 20.008618663s
Aug 25 03:20:03.871: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=false. Elapsed: 22.009051036s
Aug 25 03:20:05.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00864955s
STEP: Saw pod success 08/25/22 03:20:05.87
Aug 25 03:20:05.871: INFO: Pod "pod-subpath-test-secret-bbx8" satisfied condition "Succeeded or Failed"
Aug 25 03:20:05.875: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-secret-bbx8 container test-container-subpath-secret-bbx8: <nil>
STEP: delete the pod 08/25/22 03:20:05.881
Aug 25 03:20:05.889: INFO: Waiting for pod pod-subpath-test-secret-bbx8 to disappear
Aug 25 03:20:05.892: INFO: Pod pod-subpath-test-secret-bbx8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-bbx8 08/25/22 03:20:05.892
Aug 25 03:20:05.893: INFO: Deleting pod "pod-subpath-test-secret-bbx8" in namespace "subpath-2385"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 25 03:20:05.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2385" for this suite. 08/25/22 03:20:05.9
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":122,"skipped":2272,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [24.085 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:19:41.819
    Aug 25 03:19:41.819: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename subpath 08/25/22 03:19:41.82
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:19:41.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:19:41.844
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/25/22 03:19:41.848
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-bbx8 08/25/22 03:19:41.855
    STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:19:41.855
    Aug 25 03:19:41.862: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bbx8" in namespace "subpath-2385" to be "Succeeded or Failed"
    Aug 25 03:19:41.865: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213052ms
    Aug 25 03:19:43.871: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009324491s
    Aug 25 03:19:45.869: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 4.007777242s
    Aug 25 03:19:47.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 6.008381361s
    Aug 25 03:19:49.872: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 8.009933406s
    Aug 25 03:19:51.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 10.008067964s
    Aug 25 03:19:53.872: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 12.010083426s
    Aug 25 03:19:55.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 14.008591174s
    Aug 25 03:19:57.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 16.007899675s
    Aug 25 03:19:59.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 18.007890715s
    Aug 25 03:20:01.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=true. Elapsed: 20.008618663s
    Aug 25 03:20:03.871: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Running", Reason="", readiness=false. Elapsed: 22.009051036s
    Aug 25 03:20:05.870: INFO: Pod "pod-subpath-test-secret-bbx8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00864955s
    STEP: Saw pod success 08/25/22 03:20:05.87
    Aug 25 03:20:05.871: INFO: Pod "pod-subpath-test-secret-bbx8" satisfied condition "Succeeded or Failed"
    Aug 25 03:20:05.875: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-secret-bbx8 container test-container-subpath-secret-bbx8: <nil>
    STEP: delete the pod 08/25/22 03:20:05.881
    Aug 25 03:20:05.889: INFO: Waiting for pod pod-subpath-test-secret-bbx8 to disappear
    Aug 25 03:20:05.892: INFO: Pod pod-subpath-test-secret-bbx8 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-bbx8 08/25/22 03:20:05.892
    Aug 25 03:20:05.893: INFO: Deleting pod "pod-subpath-test-secret-bbx8" in namespace "subpath-2385"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 25 03:20:05.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2385" for this suite. 08/25/22 03:20:05.9
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:20:05.905
Aug 25 03:20:05.906: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-watch 08/25/22 03:20:05.907
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:20:05.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:20:05.923
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 25 03:20:05.926: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Creating first CR  08/25/22 03:20:08.49
Aug 25 03:20:08.496: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:08Z]] name:name1 resourceVersion:22907 uid:04ce1ca7-1c2f-4f02-986a-5260793b0190] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/25/22 03:20:18.498
Aug 25 03:20:18.505: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:18Z]] name:name2 resourceVersion:22975 uid:b92da2f0-7c01-42c1-bb18-c3aa3163eb4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/25/22 03:20:28.508
Aug 25 03:20:28.515: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:28Z]] name:name1 resourceVersion:23035 uid:04ce1ca7-1c2f-4f02-986a-5260793b0190] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/25/22 03:20:38.516
Aug 25 03:20:38.523: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:38Z]] name:name2 resourceVersion:23089 uid:b92da2f0-7c01-42c1-bb18-c3aa3163eb4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/25/22 03:20:48.524
Aug 25 03:20:48.534: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:28Z]] name:name1 resourceVersion:23147 uid:04ce1ca7-1c2f-4f02-986a-5260793b0190] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/25/22 03:20:58.537
Aug 25 03:20:58.544: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:38Z]] name:name2 resourceVersion:23205 uid:b92da2f0-7c01-42c1-bb18-c3aa3163eb4e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:21:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9051" for this suite. 08/25/22 03:21:09.062
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":123,"skipped":2312,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [63.162 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:20:05.905
    Aug 25 03:20:05.906: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-watch 08/25/22 03:20:05.907
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:20:05.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:20:05.923
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 25 03:20:05.926: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Creating first CR  08/25/22 03:20:08.49
    Aug 25 03:20:08.496: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:08Z]] name:name1 resourceVersion:22907 uid:04ce1ca7-1c2f-4f02-986a-5260793b0190] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/25/22 03:20:18.498
    Aug 25 03:20:18.505: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:18Z]] name:name2 resourceVersion:22975 uid:b92da2f0-7c01-42c1-bb18-c3aa3163eb4e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/25/22 03:20:28.508
    Aug 25 03:20:28.515: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:28Z]] name:name1 resourceVersion:23035 uid:04ce1ca7-1c2f-4f02-986a-5260793b0190] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/25/22 03:20:38.516
    Aug 25 03:20:38.523: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:38Z]] name:name2 resourceVersion:23089 uid:b92da2f0-7c01-42c1-bb18-c3aa3163eb4e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/25/22 03:20:48.524
    Aug 25 03:20:48.534: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:28Z]] name:name1 resourceVersion:23147 uid:04ce1ca7-1c2f-4f02-986a-5260793b0190] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/25/22 03:20:58.537
    Aug 25 03:20:58.544: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-25T03:20:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-25T03:20:38Z]] name:name2 resourceVersion:23205 uid:b92da2f0-7c01-42c1-bb18-c3aa3163eb4e] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:21:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-9051" for this suite. 08/25/22 03:21:09.062
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:21:09.069
Aug 25 03:21:09.069: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:21:09.07
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:09.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:09.086
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-4a78928c-3687-40a0-85cd-7f1d7ddf980d 08/25/22 03:21:09.09
STEP: Creating a pod to test consume secrets 08/25/22 03:21:09.093
Aug 25 03:21:09.100: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8" in namespace "projected-3493" to be "Succeeded or Failed"
Aug 25 03:21:09.104: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188162ms
Aug 25 03:21:11.109: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008164395s
Aug 25 03:21:13.109: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008918249s
STEP: Saw pod success 08/25/22 03:21:13.109
Aug 25 03:21:13.110: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8" satisfied condition "Succeeded or Failed"
Aug 25 03:21:13.114: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:21:13.119
Aug 25 03:21:13.128: INFO: Waiting for pod pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8 to disappear
Aug 25 03:21:13.131: INFO: Pod pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 03:21:13.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3493" for this suite. 08/25/22 03:21:13.135
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":124,"skipped":2329,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.070 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:21:09.069
    Aug 25 03:21:09.069: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:21:09.07
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:09.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:09.086
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-4a78928c-3687-40a0-85cd-7f1d7ddf980d 08/25/22 03:21:09.09
    STEP: Creating a pod to test consume secrets 08/25/22 03:21:09.093
    Aug 25 03:21:09.100: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8" in namespace "projected-3493" to be "Succeeded or Failed"
    Aug 25 03:21:09.104: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188162ms
    Aug 25 03:21:11.109: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008164395s
    Aug 25 03:21:13.109: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008918249s
    STEP: Saw pod success 08/25/22 03:21:13.109
    Aug 25 03:21:13.110: INFO: Pod "pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8" satisfied condition "Succeeded or Failed"
    Aug 25 03:21:13.114: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:21:13.119
    Aug 25 03:21:13.128: INFO: Waiting for pod pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8 to disappear
    Aug 25 03:21:13.131: INFO: Pod pod-projected-secrets-9d457a03-d1fe-43b9-b6b4-04d4266218d8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 03:21:13.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3493" for this suite. 08/25/22 03:21:13.135
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:21:13.143
Aug 25 03:21:13.144: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 03:21:13.145
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:13.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:13.16
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 25 03:21:13.163: INFO: Creating deployment "webserver-deployment"
Aug 25 03:21:13.168: INFO: Waiting for observed generation 1
Aug 25 03:21:15.176: INFO: Waiting for all required pods to come up
Aug 25 03:21:15.183: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/25/22 03:21:15.183
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rs59r" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4glrr" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kdkz6" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pr466" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kzkc4" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-cmcjb" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gsqmt" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-85nc7" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rkk4p" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qhzvx" in namespace "deployment-9702" to be "running"
Aug 25 03:21:15.192: INFO: Pod "webserver-deployment-845c8977d9-rs59r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469257ms
Aug 25 03:21:15.192: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.68006ms
Aug 25 03:21:15.193: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.818769ms
Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.304746ms
Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-cmcjb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.374229ms
Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-qhzvx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.733844ms
Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.94608ms
Aug 25 03:21:15.195: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.113773ms
Aug 25 03:21:15.195: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 11.206521ms
Aug 25 03:21:15.195: INFO: Pod "webserver-deployment-845c8977d9-rkk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 11.451089ms
Aug 25 03:21:17.197: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014066495s
Aug 25 03:21:17.201: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017253704s
Aug 25 03:21:17.201: INFO: Pod "webserver-deployment-845c8977d9-rs59r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017411286s
Aug 25 03:21:17.201: INFO: Pod "webserver-deployment-845c8977d9-rkk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017328268s
Aug 25 03:21:17.203: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019331777s
Aug 25 03:21:17.203: INFO: Pod "webserver-deployment-845c8977d9-qhzvx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019617022s
Aug 25 03:21:17.203: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019788939s
Aug 25 03:21:17.204: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020187846s
Aug 25 03:21:17.204: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0200975s
Aug 25 03:21:17.204: INFO: Pod "webserver-deployment-845c8977d9-cmcjb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02014536s
Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-rs59r": Phase="Running", Reason="", readiness=true. Elapsed: 4.017719483s
Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-qhzvx": Phase="Running", Reason="", readiness=true. Elapsed: 4.017507246s
Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-qhzvx" satisfied condition "running"
Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017738004s
Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-rs59r" satisfied condition "running"
Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017830687s
Aug 25 03:21:19.203: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01980254s
Aug 25 03:21:19.203: INFO: Pod "webserver-deployment-845c8977d9-cmcjb": Phase="Running", Reason="", readiness=true. Elapsed: 4.019867684s
Aug 25 03:21:19.203: INFO: Pod "webserver-deployment-845c8977d9-cmcjb" satisfied condition "running"
Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020070162s
Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020363872s
Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-rkk4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.020602128s
Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-rkk4p" satisfied condition "running"
Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020792623s
Aug 25 03:21:21.197: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Running", Reason="", readiness=true. Elapsed: 6.014115409s
Aug 25 03:21:21.197: INFO: Pod "webserver-deployment-845c8977d9-4glrr" satisfied condition "running"
Aug 25 03:21:21.198: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Running", Reason="", readiness=true. Elapsed: 6.015039851s
Aug 25 03:21:21.199: INFO: Pod "webserver-deployment-845c8977d9-85nc7" satisfied condition "running"
Aug 25 03:21:21.199: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Running", Reason="", readiness=true. Elapsed: 6.015863495s
Aug 25 03:21:21.199: INFO: Pod "webserver-deployment-845c8977d9-gsqmt" satisfied condition "running"
Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Running", Reason="", readiness=true. Elapsed: 6.016776832s
Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-kdkz6" satisfied condition "running"
Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016947635s
Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Running", Reason="", readiness=true. Elapsed: 6.017093202s
Aug 25 03:21:21.201: INFO: Pod "webserver-deployment-845c8977d9-kzkc4" satisfied condition "running"
Aug 25 03:21:23.199: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Running", Reason="", readiness=true. Elapsed: 8.015832099s
Aug 25 03:21:23.199: INFO: Pod "webserver-deployment-845c8977d9-pr466" satisfied condition "running"
Aug 25 03:21:23.199: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 25 03:21:23.207: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 25 03:21:23.217: INFO: Updating deployment webserver-deployment
Aug 25 03:21:23.217: INFO: Waiting for observed generation 2
Aug 25 03:21:25.226: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 25 03:21:25.230: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 25 03:21:25.233: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 25 03:21:25.243: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 25 03:21:25.243: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 25 03:21:25.247: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 25 03:21:25.253: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 25 03:21:25.253: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 25 03:21:25.264: INFO: Updating deployment webserver-deployment
Aug 25 03:21:25.264: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 25 03:21:25.270: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 25 03:21:25.274: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 03:21:25.281: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9702  6b496403-6d29-4353-be9e-ca403247f305 23578 3 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e12768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-08-25 03:21:23 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-25 03:21:25 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 25 03:21:25.285: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-9702  197ff3da-03b6-4599-8814-6300e6d1164c 23572 3 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6b496403-6d29-4353-be9e-ca403247f305 0xc005dea537 0xc005dea538}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b496403-6d29-4353-be9e-ca403247f305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dea5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:21:25.285: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 25 03:21:25.286: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-9702  f4a26db2-6015-46d6-b3a9-db3b500116ad 23570 3 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6b496403-6d29-4353-be9e-ca403247f305 0xc005dea637 0xc005dea638}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b496403-6d29-4353-be9e-ca403247f305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dea6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:21:25.295: INFO: Pod "webserver-deployment-69b7448995-4zk9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4zk9d webserver-deployment-69b7448995- deployment-9702  eae9ed4c-2e88-461d-a91c-550099ded9d0 23562 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e12b77 0xc005e12b78}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8p27l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p27l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.295: INFO: Pod "webserver-deployment-69b7448995-gf6jm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gf6jm webserver-deployment-69b7448995- deployment-9702  4a988f2f-28da-4041-b8e8-991570ceab61 23534 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e12d57 0xc005e12d58}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6qd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6qd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.295: INFO: Pod "webserver-deployment-69b7448995-hmf5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hmf5z webserver-deployment-69b7448995- deployment-9702  4e7c8972-06bc-460b-81c0-97dc175ab905 23586 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e12f37 0xc005e12f38}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2qxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2qxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-k2fww" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-k2fww webserver-deployment-69b7448995- deployment-9702  a214d51d-580e-4c17-accb-0bbd87ff3dc5 23588 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e13087 0xc005e13088}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkwz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkwz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-l6z78" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-l6z78 webserver-deployment-69b7448995- deployment-9702  06280e9d-11c4-4a0a-becb-1dd07893b9b5 23541 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e131d7 0xc005e131d8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-269rq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-269rq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-tlgfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tlgfh webserver-deployment-69b7448995- deployment-9702  5a6971c1-70db-49eb-a2a7-6600d56deffb 23582 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e133b7 0xc005e133b8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njdp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njdp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-tv6dh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tv6dh webserver-deployment-69b7448995- deployment-9702  5f27b10a-34e3-4e7b-b683-18c99757c063 23525 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e13520 0xc005e13521}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwpbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwpbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-v7s5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-v7s5z webserver-deployment-69b7448995- deployment-9702  886cd47b-1b9c-45f3-bdd7-6d92275096b3 23550 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e136f7 0xc005e136f8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nd2ll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nd2ll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-45gnl" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-45gnl webserver-deployment-845c8977d9- deployment-9702  ac775887-9594-4f4b-962b-24b1eb12e14e 23587 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e138d7 0xc005e138d8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p428z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p428z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-4glrr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4glrr webserver-deployment-845c8977d9- deployment-9702  b0dc3979-580c-43e7-ae41-8e825c95d003 23466 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13a30 0xc005e13a31}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nkp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nkp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.34,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f84dad6c65c3755e93595f7c10589ead1b86bd2c0af43a4ea099fbbd4061212e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-7j5dh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7j5dh webserver-deployment-845c8977d9- deployment-9702  540eff01-cc52-4571-ad6f-3c9f7405798c 23581 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13c07 0xc005e13c08}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v48r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v48r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-7s44t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7s44t webserver-deployment-845c8977d9- deployment-9702  851fb0dd-91d3-48fb-a2e5-74660c87abd7 23593 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13d60 0xc005e13d61}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dwgh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dwgh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-85nc7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-85nc7 webserver-deployment-845c8977d9- deployment-9702  b26a657a-6398-48cd-9916-cfd2c225bfc3 23461 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13e97 0xc005e13e98}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5crz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5crz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.47,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f05c1ac4b7ac69d9eab131392a7283fdfd57842e22605bee391fcd3b034e0776,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-bhdfd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bhdfd webserver-deployment-845c8977d9- deployment-9702  3cf5a4c8-1bc6-4197-9691-be15d52102ac 23579 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de077 0xc0098de078}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xscb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xscb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-cmcjb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cmcjb webserver-deployment-845c8977d9- deployment-9702  661ad886-13b4-4a26-9876-c57a62bf050c 23424 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de1d0 0xc0098de1d1}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7v5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7v5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.13,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://37de4abb42810534d129af58c79d0aaf9e4b03e246015da4ddd4bad7de5fa012,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-f5m4g" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f5m4g webserver-deployment-845c8977d9- deployment-9702  f154fc6a-826e-4a9b-b50d-d31b229a8628 23590 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de3a7 0xc0098de3a8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfk5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfk5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-gsqmt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gsqmt webserver-deployment-845c8977d9- deployment-9702  5ab3cd0d-bfbb-44f1-b506-0e9da80dcd89 23456 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de4e7 0xc0098de4e8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbhbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbhbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.26,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a2fbed874a78c9b1916acb7fdab0b8b3070b0fc3b64dd52c0582fa28fd80a8ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-h827r" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h827r webserver-deployment-845c8977d9- deployment-9702  47f80686-2de0-4e77-82e7-590bebc06384 23585 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de6c7 0xc0098de6c8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxjxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxjxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-jt2fh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jt2fh webserver-deployment-845c8977d9- deployment-9702  45e40597-7f32-4d92-b00f-8309dbb4d0df 23589 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de820 0xc0098de821}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ff778,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ff778,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-kdkz6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kdkz6 webserver-deployment-845c8977d9- deployment-9702  987d480a-c4bf-4ff4-b9de-7a1311fddb82 23476 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de970 0xc0098de971}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l97zd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l97zd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.48,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1c9403ea5200cdaa50e3a7f06545dc39d3dd9f78908aefbcf505595f4491add9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.300: INFO: Pod "webserver-deployment-845c8977d9-pr466" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pr466 webserver-deployment-845c8977d9- deployment-9702  0c552fa8-7f55-480e-b61c-f210938ade99 23481 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098deb47 0xc0098deb48}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z9vc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z9vc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.37,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://76d58f5e38eb1838f42f738654bc2201a757c5296d0422be02f117b04c64be23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.300: INFO: Pod "webserver-deployment-845c8977d9-qhzvx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhzvx webserver-deployment-845c8977d9- deployment-9702  6e8e4b2a-8e21-4e1b-a79a-715424a9f5d2 23432 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098ded37 0xc0098ded38}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kv9x9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kv9x9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.27,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f9ee0c401273d64fd066ee515120ce7aa4777fb802a2fb8e1dc88a932cd466d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 03:21:25.300: INFO: Pod "webserver-deployment-845c8977d9-rkk4p" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rkk4p webserver-deployment-845c8977d9- deployment-9702  d45c0a20-b822-4af3-8e79-b19538ac2792 23452 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098def17 0xc0098def18}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpz9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpz9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.20,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://caa174fd252fe563686a83d534c594278b8d004ade323872a34876702f937371,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 03:21:25.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9702" for this suite. 08/25/22 03:21:25.304
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":125,"skipped":2365,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [12.163 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:21:13.143
    Aug 25 03:21:13.144: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 03:21:13.145
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:13.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:13.16
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 25 03:21:13.163: INFO: Creating deployment "webserver-deployment"
    Aug 25 03:21:13.168: INFO: Waiting for observed generation 1
    Aug 25 03:21:15.176: INFO: Waiting for all required pods to come up
    Aug 25 03:21:15.183: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/25/22 03:21:15.183
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rs59r" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4glrr" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kdkz6" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pr466" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kzkc4" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-cmcjb" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gsqmt" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-85nc7" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rkk4p" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.183: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qhzvx" in namespace "deployment-9702" to be "running"
    Aug 25 03:21:15.192: INFO: Pod "webserver-deployment-845c8977d9-rs59r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469257ms
    Aug 25 03:21:15.192: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.68006ms
    Aug 25 03:21:15.193: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.818769ms
    Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.304746ms
    Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-cmcjb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.374229ms
    Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-qhzvx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.733844ms
    Aug 25 03:21:15.194: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.94608ms
    Aug 25 03:21:15.195: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.113773ms
    Aug 25 03:21:15.195: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 11.206521ms
    Aug 25 03:21:15.195: INFO: Pod "webserver-deployment-845c8977d9-rkk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 11.451089ms
    Aug 25 03:21:17.197: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014066495s
    Aug 25 03:21:17.201: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017253704s
    Aug 25 03:21:17.201: INFO: Pod "webserver-deployment-845c8977d9-rs59r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017411286s
    Aug 25 03:21:17.201: INFO: Pod "webserver-deployment-845c8977d9-rkk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017328268s
    Aug 25 03:21:17.203: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019331777s
    Aug 25 03:21:17.203: INFO: Pod "webserver-deployment-845c8977d9-qhzvx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019617022s
    Aug 25 03:21:17.203: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019788939s
    Aug 25 03:21:17.204: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020187846s
    Aug 25 03:21:17.204: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0200975s
    Aug 25 03:21:17.204: INFO: Pod "webserver-deployment-845c8977d9-cmcjb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02014536s
    Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-rs59r": Phase="Running", Reason="", readiness=true. Elapsed: 4.017719483s
    Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-qhzvx": Phase="Running", Reason="", readiness=true. Elapsed: 4.017507246s
    Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-qhzvx" satisfied condition "running"
    Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017738004s
    Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-rs59r" satisfied condition "running"
    Aug 25 03:21:19.201: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017830687s
    Aug 25 03:21:19.203: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01980254s
    Aug 25 03:21:19.203: INFO: Pod "webserver-deployment-845c8977d9-cmcjb": Phase="Running", Reason="", readiness=true. Elapsed: 4.019867684s
    Aug 25 03:21:19.203: INFO: Pod "webserver-deployment-845c8977d9-cmcjb" satisfied condition "running"
    Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020070162s
    Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020363872s
    Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-rkk4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.020602128s
    Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-rkk4p" satisfied condition "running"
    Aug 25 03:21:19.204: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020792623s
    Aug 25 03:21:21.197: INFO: Pod "webserver-deployment-845c8977d9-4glrr": Phase="Running", Reason="", readiness=true. Elapsed: 6.014115409s
    Aug 25 03:21:21.197: INFO: Pod "webserver-deployment-845c8977d9-4glrr" satisfied condition "running"
    Aug 25 03:21:21.198: INFO: Pod "webserver-deployment-845c8977d9-85nc7": Phase="Running", Reason="", readiness=true. Elapsed: 6.015039851s
    Aug 25 03:21:21.199: INFO: Pod "webserver-deployment-845c8977d9-85nc7" satisfied condition "running"
    Aug 25 03:21:21.199: INFO: Pod "webserver-deployment-845c8977d9-gsqmt": Phase="Running", Reason="", readiness=true. Elapsed: 6.015863495s
    Aug 25 03:21:21.199: INFO: Pod "webserver-deployment-845c8977d9-gsqmt" satisfied condition "running"
    Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-kdkz6": Phase="Running", Reason="", readiness=true. Elapsed: 6.016776832s
    Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-kdkz6" satisfied condition "running"
    Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016947635s
    Aug 25 03:21:21.200: INFO: Pod "webserver-deployment-845c8977d9-kzkc4": Phase="Running", Reason="", readiness=true. Elapsed: 6.017093202s
    Aug 25 03:21:21.201: INFO: Pod "webserver-deployment-845c8977d9-kzkc4" satisfied condition "running"
    Aug 25 03:21:23.199: INFO: Pod "webserver-deployment-845c8977d9-pr466": Phase="Running", Reason="", readiness=true. Elapsed: 8.015832099s
    Aug 25 03:21:23.199: INFO: Pod "webserver-deployment-845c8977d9-pr466" satisfied condition "running"
    Aug 25 03:21:23.199: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 25 03:21:23.207: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 25 03:21:23.217: INFO: Updating deployment webserver-deployment
    Aug 25 03:21:23.217: INFO: Waiting for observed generation 2
    Aug 25 03:21:25.226: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 25 03:21:25.230: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 25 03:21:25.233: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 25 03:21:25.243: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 25 03:21:25.243: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 25 03:21:25.247: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 25 03:21:25.253: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 25 03:21:25.253: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 25 03:21:25.264: INFO: Updating deployment webserver-deployment
    Aug 25 03:21:25.264: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 25 03:21:25.270: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 25 03:21:25.274: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 03:21:25.281: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9702  6b496403-6d29-4353-be9e-ca403247f305 23578 3 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e12768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-08-25 03:21:23 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-25 03:21:25 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 25 03:21:25.285: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-9702  197ff3da-03b6-4599-8814-6300e6d1164c 23572 3 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6b496403-6d29-4353-be9e-ca403247f305 0xc005dea537 0xc005dea538}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b496403-6d29-4353-be9e-ca403247f305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dea5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:21:25.285: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 25 03:21:25.286: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-9702  f4a26db2-6015-46d6-b3a9-db3b500116ad 23570 3 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6b496403-6d29-4353-be9e-ca403247f305 0xc005dea637 0xc005dea638}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b496403-6d29-4353-be9e-ca403247f305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dea6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:21:25.295: INFO: Pod "webserver-deployment-69b7448995-4zk9d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4zk9d webserver-deployment-69b7448995- deployment-9702  eae9ed4c-2e88-461d-a91c-550099ded9d0 23562 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e12b77 0xc005e12b78}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8p27l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p27l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.295: INFO: Pod "webserver-deployment-69b7448995-gf6jm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gf6jm webserver-deployment-69b7448995- deployment-9702  4a988f2f-28da-4041-b8e8-991570ceab61 23534 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e12d57 0xc005e12d58}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6qd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6qd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.295: INFO: Pod "webserver-deployment-69b7448995-hmf5z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hmf5z webserver-deployment-69b7448995- deployment-9702  4e7c8972-06bc-460b-81c0-97dc175ab905 23586 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e12f37 0xc005e12f38}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2qxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2qxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-k2fww" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-k2fww webserver-deployment-69b7448995- deployment-9702  a214d51d-580e-4c17-accb-0bbd87ff3dc5 23588 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e13087 0xc005e13088}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkwz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkwz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-l6z78" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-l6z78 webserver-deployment-69b7448995- deployment-9702  06280e9d-11c4-4a0a-becb-1dd07893b9b5 23541 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e131d7 0xc005e131d8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-269rq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-269rq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-tlgfh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tlgfh webserver-deployment-69b7448995- deployment-9702  5a6971c1-70db-49eb-a2a7-6600d56deffb 23582 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e133b7 0xc005e133b8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njdp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njdp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-tv6dh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tv6dh webserver-deployment-69b7448995- deployment-9702  5f27b10a-34e3-4e7b-b683-18c99757c063 23525 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e13520 0xc005e13521}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwpbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwpbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.296: INFO: Pod "webserver-deployment-69b7448995-v7s5z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-v7s5z webserver-deployment-69b7448995- deployment-9702  886cd47b-1b9c-45f3-bdd7-6d92275096b3 23550 0 2022-08-25 03:21:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 197ff3da-03b6-4599-8814-6300e6d1164c 0xc005e136f7 0xc005e136f8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197ff3da-03b6-4599-8814-6300e6d1164c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nd2ll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nd2ll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 03:21:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-45gnl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-45gnl webserver-deployment-845c8977d9- deployment-9702  ac775887-9594-4f4b-962b-24b1eb12e14e 23587 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e138d7 0xc005e138d8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p428z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p428z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-4glrr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4glrr webserver-deployment-845c8977d9- deployment-9702  b0dc3979-580c-43e7-ae41-8e825c95d003 23466 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13a30 0xc005e13a31}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nkp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nkp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.34,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f84dad6c65c3755e93595f7c10589ead1b86bd2c0af43a4ea099fbbd4061212e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-7j5dh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7j5dh webserver-deployment-845c8977d9- deployment-9702  540eff01-cc52-4571-ad6f-3c9f7405798c 23581 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13c07 0xc005e13c08}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v48r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v48r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.297: INFO: Pod "webserver-deployment-845c8977d9-7s44t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7s44t webserver-deployment-845c8977d9- deployment-9702  851fb0dd-91d3-48fb-a2e5-74660c87abd7 23593 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13d60 0xc005e13d61}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dwgh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dwgh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-85nc7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-85nc7 webserver-deployment-845c8977d9- deployment-9702  b26a657a-6398-48cd-9916-cfd2c225bfc3 23461 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc005e13e97 0xc005e13e98}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5crz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5crz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.47,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f05c1ac4b7ac69d9eab131392a7283fdfd57842e22605bee391fcd3b034e0776,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-bhdfd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bhdfd webserver-deployment-845c8977d9- deployment-9702  3cf5a4c8-1bc6-4197-9691-be15d52102ac 23579 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de077 0xc0098de078}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xscb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xscb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-cmcjb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cmcjb webserver-deployment-845c8977d9- deployment-9702  661ad886-13b4-4a26-9876-c57a62bf050c 23424 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de1d0 0xc0098de1d1}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7v5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7v5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.13,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://37de4abb42810534d129af58c79d0aaf9e4b03e246015da4ddd4bad7de5fa012,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.298: INFO: Pod "webserver-deployment-845c8977d9-f5m4g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f5m4g webserver-deployment-845c8977d9- deployment-9702  f154fc6a-826e-4a9b-b50d-d31b229a8628 23590 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de3a7 0xc0098de3a8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfk5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfk5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-gsqmt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gsqmt webserver-deployment-845c8977d9- deployment-9702  5ab3cd0d-bfbb-44f1-b506-0e9da80dcd89 23456 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de4e7 0xc0098de4e8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbhbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbhbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.26,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a2fbed874a78c9b1916acb7fdab0b8b3070b0fc3b64dd52c0582fa28fd80a8ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-h827r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h827r webserver-deployment-845c8977d9- deployment-9702  47f80686-2de0-4e77-82e7-590bebc06384 23585 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de6c7 0xc0098de6c8}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxjxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxjxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-jt2fh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jt2fh webserver-deployment-845c8977d9- deployment-9702  45e40597-7f32-4d92-b00f-8309dbb4d0df 23589 0 2022-08-25 03:21:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de820 0xc0098de821}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ff778,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ff778,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.299: INFO: Pod "webserver-deployment-845c8977d9-kdkz6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kdkz6 webserver-deployment-845c8977d9- deployment-9702  987d480a-c4bf-4ff4-b9de-7a1311fddb82 23476 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098de970 0xc0098de971}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l97zd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l97zd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.48,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1c9403ea5200cdaa50e3a7f06545dc39d3dd9f78908aefbcf505595f4491add9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.300: INFO: Pod "webserver-deployment-845c8977d9-pr466" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pr466 webserver-deployment-845c8977d9- deployment-9702  0c552fa8-7f55-480e-b61c-f210938ade99 23481 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098deb47 0xc0098deb48}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z9vc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z9vc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.37,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://76d58f5e38eb1838f42f738654bc2201a757c5296d0422be02f117b04c64be23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.300: INFO: Pod "webserver-deployment-845c8977d9-qhzvx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhzvx webserver-deployment-845c8977d9- deployment-9702  6e8e4b2a-8e21-4e1b-a79a-715424a9f5d2 23432 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098ded37 0xc0098ded38}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kv9x9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kv9x9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.27,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f9ee0c401273d64fd066ee515120ce7aa4777fb802a2fb8e1dc88a932cd466d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 03:21:25.300: INFO: Pod "webserver-deployment-845c8977d9-rkk4p" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rkk4p webserver-deployment-845c8977d9- deployment-9702  d45c0a20-b822-4af3-8e79-b19538ac2792 23452 0 2022-08-25 03:21:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4a26db2-6015-46d6-b3a9-db3b500116ad 0xc0098def17 0xc0098def18}] [] [{kube-controller-manager Update v1 2022-08-25 03:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4a26db2-6015-46d6-b3a9-db3b500116ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:21:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpz9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpz9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.20,StartTime:2022-08-25 03:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://caa174fd252fe563686a83d534c594278b8d004ade323872a34876702f937371,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 03:21:25.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9702" for this suite. 08/25/22 03:21:25.304
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:21:25.307
Aug 25 03:21:25.308: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:21:25.308
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:25.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:25.319
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Aug 25 03:21:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: creating the pod 08/25/22 03:21:25.323
STEP: submitting the pod to kubernetes 08/25/22 03:21:25.323
Aug 25 03:21:25.328: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9" in namespace "pods-9593" to be "running and ready"
Aug 25 03:21:25.330: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041535ms
Aug 25 03:21:25.330: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:27.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008150419s
Aug 25 03:21:27.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:29.335: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006987084s
Aug 25 03:21:29.335: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:31.335: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006584959s
Aug 25 03:21:31.335: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:33.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007870873s
Aug 25 03:21:33.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:35.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007348113s
Aug 25 03:21:35.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:37.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007575293s
Aug 25 03:21:37.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:21:39.337: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Running", Reason="", readiness=true. Elapsed: 14.00833879s
Aug 25 03:21:39.337: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Running (Ready = true)
Aug 25 03:21:39.337: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:21:39.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9593" for this suite. 08/25/22 03:21:39.464
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":126,"skipped":2366,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [14.162 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:21:25.307
    Aug 25 03:21:25.308: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:21:25.308
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:25.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:25.319
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Aug 25 03:21:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: creating the pod 08/25/22 03:21:25.323
    STEP: submitting the pod to kubernetes 08/25/22 03:21:25.323
    Aug 25 03:21:25.328: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9" in namespace "pods-9593" to be "running and ready"
    Aug 25 03:21:25.330: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041535ms
    Aug 25 03:21:25.330: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:27.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008150419s
    Aug 25 03:21:27.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:29.335: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006987084s
    Aug 25 03:21:29.335: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:31.335: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006584959s
    Aug 25 03:21:31.335: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:33.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007870873s
    Aug 25 03:21:33.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:35.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007348113s
    Aug 25 03:21:35.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:37.336: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007575293s
    Aug 25 03:21:37.336: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:21:39.337: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9": Phase="Running", Reason="", readiness=true. Elapsed: 14.00833879s
    Aug 25 03:21:39.337: INFO: The phase of Pod pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9 is Running (Ready = true)
    Aug 25 03:21:39.337: INFO: Pod "pod-exec-websocket-03c9b578-2001-4095-8862-3ff0fb90e8a9" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:21:39.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9593" for this suite. 08/25/22 03:21:39.464
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:21:39.472
Aug 25 03:21:39.472: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 03:21:39.473
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:39.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:39.49
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2816 08/25/22 03:21:39.494
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 08/25/22 03:21:39.497
STEP: Creating stateful set ss in namespace statefulset-2816 08/25/22 03:21:39.501
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2816 08/25/22 03:21:39.506
Aug 25 03:21:39.508: INFO: Found 0 stateful pods, waiting for 1
Aug 25 03:21:49.514: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/25/22 03:21:49.514
Aug 25 03:21:49.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 03:21:49.716: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 03:21:49.716: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 03:21:49.716: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 03:21:49.721: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 25 03:21:59.727: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 03:21:59.727: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:21:59.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999522s
Aug 25 03:22:00.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995293967s
Aug 25 03:22:01.753: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990717767s
Aug 25 03:22:02.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98608005s
Aug 25 03:22:03.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98141857s
Aug 25 03:22:04.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976827792s
Aug 25 03:22:05.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972059989s
Aug 25 03:22:06.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967641933s
Aug 25 03:22:07.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963092205s
Aug 25 03:22:08.786: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.997604ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2816 08/25/22 03:22:09.786
Aug 25 03:22:09.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 03:22:09.992: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 03:22:09.992: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 03:22:09.992: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 03:22:09.997: INFO: Found 1 stateful pods, waiting for 3
Aug 25 03:22:20.007: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:22:20.007: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:22:20.007: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/25/22 03:22:20.007
STEP: Scale down will halt with unhealthy stateful pod 08/25/22 03:22:20.007
Aug 25 03:22:20.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 03:22:20.220: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 03:22:20.221: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 03:22:20.221: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 03:22:20.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 03:22:20.408: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 03:22:20.408: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 03:22:20.408: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 03:22:20.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 03:22:20.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 03:22:20.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 03:22:20.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 03:22:20.593: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:22:20.596: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 25 03:22:30.606: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 03:22:30.606: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 03:22:30.606: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 03:22:30.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999995s
Aug 25 03:22:31.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994763119s
Aug 25 03:22:32.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989380039s
Aug 25 03:22:33.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983564813s
Aug 25 03:22:34.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977550692s
Aug 25 03:22:35.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970965683s
Aug 25 03:22:36.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964973139s
Aug 25 03:22:37.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959144627s
Aug 25 03:22:38.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953749461s
Aug 25 03:22:39.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.212972ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2816 08/25/22 03:22:40.674
Aug 25 03:22:40.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 03:22:40.872: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 03:22:40.872: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 03:22:40.872: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 03:22:40.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 03:22:41.052: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 03:22:41.052: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 03:22:41.052: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 03:22:41.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 03:22:41.232: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 03:22:41.232: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 03:22:41.232: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 03:22:41.232: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/25/22 03:22:51.254
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 03:22:51.254: INFO: Deleting all statefulset in ns statefulset-2816
Aug 25 03:22:51.257: INFO: Scaling statefulset ss to 0
Aug 25 03:22:51.271: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:22:51.278: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 03:22:51.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2816" for this suite. 08/25/22 03:22:51.294
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":127,"skipped":2401,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [71.827 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:21:39.472
    Aug 25 03:21:39.472: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 03:21:39.473
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:21:39.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:21:39.49
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2816 08/25/22 03:21:39.494
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/25/22 03:21:39.497
    STEP: Creating stateful set ss in namespace statefulset-2816 08/25/22 03:21:39.501
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2816 08/25/22 03:21:39.506
    Aug 25 03:21:39.508: INFO: Found 0 stateful pods, waiting for 1
    Aug 25 03:21:49.514: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/25/22 03:21:49.514
    Aug 25 03:21:49.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 03:21:49.716: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 03:21:49.716: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 03:21:49.716: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 03:21:49.721: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 25 03:21:59.727: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 03:21:59.727: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:21:59.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999522s
    Aug 25 03:22:00.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995293967s
    Aug 25 03:22:01.753: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990717767s
    Aug 25 03:22:02.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98608005s
    Aug 25 03:22:03.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98141857s
    Aug 25 03:22:04.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976827792s
    Aug 25 03:22:05.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972059989s
    Aug 25 03:22:06.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967641933s
    Aug 25 03:22:07.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963092205s
    Aug 25 03:22:08.786: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.997604ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2816 08/25/22 03:22:09.786
    Aug 25 03:22:09.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 03:22:09.992: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 03:22:09.992: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 03:22:09.992: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 03:22:09.997: INFO: Found 1 stateful pods, waiting for 3
    Aug 25 03:22:20.007: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:22:20.007: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:22:20.007: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/25/22 03:22:20.007
    STEP: Scale down will halt with unhealthy stateful pod 08/25/22 03:22:20.007
    Aug 25 03:22:20.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 03:22:20.220: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 03:22:20.221: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 03:22:20.221: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 03:22:20.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 03:22:20.408: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 03:22:20.408: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 03:22:20.408: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 03:22:20.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 03:22:20.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 03:22:20.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 03:22:20.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 03:22:20.593: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:22:20.596: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 25 03:22:30.606: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 03:22:30.606: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 03:22:30.606: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 03:22:30.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999995s
    Aug 25 03:22:31.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994763119s
    Aug 25 03:22:32.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989380039s
    Aug 25 03:22:33.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983564813s
    Aug 25 03:22:34.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977550692s
    Aug 25 03:22:35.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970965683s
    Aug 25 03:22:36.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964973139s
    Aug 25 03:22:37.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959144627s
    Aug 25 03:22:38.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953749461s
    Aug 25 03:22:39.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.212972ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2816 08/25/22 03:22:40.674
    Aug 25 03:22:40.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 03:22:40.872: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 03:22:40.872: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 03:22:40.872: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 03:22:40.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 03:22:41.052: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 03:22:41.052: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 03:22:41.052: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 03:22:41.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-2816 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 03:22:41.232: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 03:22:41.232: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 03:22:41.232: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 03:22:41.232: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/25/22 03:22:51.254
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 03:22:51.254: INFO: Deleting all statefulset in ns statefulset-2816
    Aug 25 03:22:51.257: INFO: Scaling statefulset ss to 0
    Aug 25 03:22:51.271: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:22:51.278: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 03:22:51.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2816" for this suite. 08/25/22 03:22:51.294
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:22:51.299
Aug 25 03:22:51.299: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-runtime 08/25/22 03:22:51.3
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:22:51.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:22:51.317
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/25/22 03:22:51.325
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/25/22 03:23:06.397
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/25/22 03:23:06.402
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/25/22 03:23:06.41
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/25/22 03:23:06.41
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/25/22 03:23:06.425
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/25/22 03:23:09.443
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/25/22 03:23:11.458
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/25/22 03:23:11.466
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/25/22 03:23:11.466
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/25/22 03:23:11.479
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/25/22 03:23:12.486
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/25/22 03:23:15.504
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/25/22 03:23:15.512
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/25/22 03:23:15.512
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 25 03:23:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-32" for this suite. 08/25/22 03:23:15.536
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":128,"skipped":2401,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [24.241 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:22:51.299
    Aug 25 03:22:51.299: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-runtime 08/25/22 03:22:51.3
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:22:51.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:22:51.317
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/25/22 03:22:51.325
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/25/22 03:23:06.397
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/25/22 03:23:06.402
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/25/22 03:23:06.41
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/25/22 03:23:06.41
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/25/22 03:23:06.425
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/25/22 03:23:09.443
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/25/22 03:23:11.458
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/25/22 03:23:11.466
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/25/22 03:23:11.466
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/25/22 03:23:11.479
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/25/22 03:23:12.486
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/25/22 03:23:15.504
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/25/22 03:23:15.512
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/25/22 03:23:15.512
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 25 03:23:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-32" for this suite. 08/25/22 03:23:15.536
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:23:15.541
Aug 25 03:23:15.541: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 03:23:15.542
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:15.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:15.561
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 08/25/22 03:23:15.576
STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 03:23:15.58
Aug 25 03:23:15.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 03:23:15.586: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 03:23:16.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 03:23:16.596: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 03:23:17.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 03:23:17.596: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: listing all DeamonSets 08/25/22 03:23:17.6
STEP: DeleteCollection of the DaemonSets 08/25/22 03:23:17.604
STEP: Verify that ReplicaSets have been deleted 08/25/22 03:23:17.61
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Aug 25 03:23:17.620: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24745"},"items":null}

Aug 25 03:23:17.624: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24745"},"items":[{"metadata":{"name":"daemon-set-n78pb","generateName":"daemon-set-","namespace":"daemonsets-4949","uid":"77a75476-1723-4d66-8b35-7fcda42b390b","resourceVersion":"24744","creationTimestamp":"2022-08-25T03:23:15Z","deletionTimestamp":"2022-08-25T03:23:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c71077c4-7561-452e-a679-3ce0c08ebd17","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-25T03:23:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c71077c4-7561-452e-a679-3ce0c08ebd17\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-25T03:23:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s9qc9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s9qc9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"bobymcbobs-c849-control-plane-p55pp","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["bobymcbobs-c849-control-plane-p55pp"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:15Z"}],"hostIP":"86.109.11.217","podIP":"192.168.0.20","podIPs":[{"ip":"192.168.0.20"}],"startTime":"2022-08-25T03:23:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-25T03:23:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://dd817a4addba6384b9350e0c3ee0361e8efbfdbd2fbe3590cad132f7f9f06557","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:23:17.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4949" for this suite. 08/25/22 03:23:17.632
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":129,"skipped":2413,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.096 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:23:15.541
    Aug 25 03:23:15.541: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 03:23:15.542
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:15.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:15.561
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 08/25/22 03:23:15.576
    STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 03:23:15.58
    Aug 25 03:23:15.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 03:23:15.586: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 03:23:16.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 03:23:16.596: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 03:23:17.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 03:23:17.596: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: listing all DeamonSets 08/25/22 03:23:17.6
    STEP: DeleteCollection of the DaemonSets 08/25/22 03:23:17.604
    STEP: Verify that ReplicaSets have been deleted 08/25/22 03:23:17.61
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Aug 25 03:23:17.620: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24745"},"items":null}

    Aug 25 03:23:17.624: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24745"},"items":[{"metadata":{"name":"daemon-set-n78pb","generateName":"daemon-set-","namespace":"daemonsets-4949","uid":"77a75476-1723-4d66-8b35-7fcda42b390b","resourceVersion":"24744","creationTimestamp":"2022-08-25T03:23:15Z","deletionTimestamp":"2022-08-25T03:23:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c71077c4-7561-452e-a679-3ce0c08ebd17","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-25T03:23:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c71077c4-7561-452e-a679-3ce0c08ebd17\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-25T03:23:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s9qc9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s9qc9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"bobymcbobs-c849-control-plane-p55pp","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["bobymcbobs-c849-control-plane-p55pp"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-25T03:23:15Z"}],"hostIP":"86.109.11.217","podIP":"192.168.0.20","podIPs":[{"ip":"192.168.0.20"}],"startTime":"2022-08-25T03:23:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-25T03:23:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://dd817a4addba6384b9350e0c3ee0361e8efbfdbd2fbe3590cad132f7f9f06557","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:23:17.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4949" for this suite. 08/25/22 03:23:17.632
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:23:17.639
Aug 25 03:23:17.639: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replicaset 08/25/22 03:23:17.639
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:17.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:17.654
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 25 03:23:17.659: INFO: Creating ReplicaSet my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb
Aug 25 03:23:17.666: INFO: Pod name my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb: Found 0 pods out of 1
Aug 25 03:23:22.671: INFO: Pod name my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb: Found 1 pods out of 1
Aug 25 03:23:22.671: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb" is running
Aug 25 03:23:22.671: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd" in namespace "replicaset-1234" to be "running"
Aug 25 03:23:22.673: INFO: Pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd": Phase="Running", Reason="", readiness=true. Elapsed: 2.089623ms
Aug 25 03:23:22.673: INFO: Pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd" satisfied condition "running"
Aug 25 03:23:22.673: INFO: Pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:17 +0000 UTC Reason: Message:}])
Aug 25 03:23:22.673: INFO: Trying to dial the pod
Aug 25 03:23:27.685: INFO: Controller my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb: Got expected result from replica 1 [my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd]: "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 25 03:23:27.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1234" for this suite. 08/25/22 03:23:27.689
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":130,"skipped":2464,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.055 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:23:17.639
    Aug 25 03:23:17.639: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replicaset 08/25/22 03:23:17.639
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:17.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:17.654
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 25 03:23:17.659: INFO: Creating ReplicaSet my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb
    Aug 25 03:23:17.666: INFO: Pod name my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb: Found 0 pods out of 1
    Aug 25 03:23:22.671: INFO: Pod name my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb: Found 1 pods out of 1
    Aug 25 03:23:22.671: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb" is running
    Aug 25 03:23:22.671: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd" in namespace "replicaset-1234" to be "running"
    Aug 25 03:23:22.673: INFO: Pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd": Phase="Running", Reason="", readiness=true. Elapsed: 2.089623ms
    Aug 25 03:23:22.673: INFO: Pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd" satisfied condition "running"
    Aug 25 03:23:22.673: INFO: Pod "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-25 03:23:17 +0000 UTC Reason: Message:}])
    Aug 25 03:23:22.673: INFO: Trying to dial the pod
    Aug 25 03:23:27.685: INFO: Controller my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb: Got expected result from replica 1 [my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd]: "my-hostname-basic-d21e4129-8a2d-4042-a9bf-bb2704e06edb-vcvhd", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 25 03:23:27.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1234" for this suite. 08/25/22 03:23:27.689
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:23:27.695
Aug 25 03:23:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename security-context-test 08/25/22 03:23:27.696
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:27.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:27.712
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Aug 25 03:23:27.722: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84" in namespace "security-context-test-1760" to be "Succeeded or Failed"
Aug 25 03:23:27.725: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390183ms
Aug 25 03:23:29.729: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006950187s
Aug 25 03:23:31.730: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00770673s
Aug 25 03:23:33.731: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008507003s
Aug 25 03:23:33.731: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 25 03:23:33.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1760" for this suite. 08/25/22 03:23:33.735
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":131,"skipped":2481,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:23:27.695
    Aug 25 03:23:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename security-context-test 08/25/22 03:23:27.696
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:27.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:27.712
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Aug 25 03:23:27.722: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84" in namespace "security-context-test-1760" to be "Succeeded or Failed"
    Aug 25 03:23:27.725: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390183ms
    Aug 25 03:23:29.729: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006950187s
    Aug 25 03:23:31.730: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00770673s
    Aug 25 03:23:33.731: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008507003s
    Aug 25 03:23:33.731: INFO: Pod "busybox-readonly-false-693aa411-5b3e-478b-a081-aa6c463dce84" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 25 03:23:33.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1760" for this suite. 08/25/22 03:23:33.735
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:23:33.742
Aug 25 03:23:33.742: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svcaccounts 08/25/22 03:23:33.743
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:33.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:33.761
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Aug 25 03:23:33.768: INFO: Got root ca configmap in namespace "svcaccounts-6390"
Aug 25 03:23:33.771: INFO: Deleted root ca configmap in namespace "svcaccounts-6390"
STEP: waiting for a new root ca configmap created 08/25/22 03:23:34.272
Aug 25 03:23:34.275: INFO: Recreated root ca configmap in namespace "svcaccounts-6390"
Aug 25 03:23:34.278: INFO: Updated root ca configmap in namespace "svcaccounts-6390"
STEP: waiting for the root ca configmap reconciled 08/25/22 03:23:34.779
Aug 25 03:23:34.783: INFO: Reconciled root ca configmap in namespace "svcaccounts-6390"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 25 03:23:34.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6390" for this suite. 08/25/22 03:23:34.788
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":132,"skipped":2510,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [1.052 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:23:33.742
    Aug 25 03:23:33.742: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svcaccounts 08/25/22 03:23:33.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:33.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:33.761
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Aug 25 03:23:33.768: INFO: Got root ca configmap in namespace "svcaccounts-6390"
    Aug 25 03:23:33.771: INFO: Deleted root ca configmap in namespace "svcaccounts-6390"
    STEP: waiting for a new root ca configmap created 08/25/22 03:23:34.272
    Aug 25 03:23:34.275: INFO: Recreated root ca configmap in namespace "svcaccounts-6390"
    Aug 25 03:23:34.278: INFO: Updated root ca configmap in namespace "svcaccounts-6390"
    STEP: waiting for the root ca configmap reconciled 08/25/22 03:23:34.779
    Aug 25 03:23:34.783: INFO: Reconciled root ca configmap in namespace "svcaccounts-6390"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 25 03:23:34.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6390" for this suite. 08/25/22 03:23:34.788
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:23:34.796
Aug 25 03:23:34.796: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubelet-test 08/25/22 03:23:34.798
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:34.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:34.814
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 25 03:23:34.825: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc" in namespace "kubelet-test-4781" to be "running and ready"
Aug 25 03:23:34.828: INFO: Pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992493ms
Aug 25 03:23:34.828: INFO: The phase of Pod busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:23:36.834: INFO: Pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008530102s
Aug 25 03:23:36.834: INFO: The phase of Pod busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc is Running (Ready = true)
Aug 25 03:23:36.834: INFO: Pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 25 03:23:36.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4781" for this suite. 08/25/22 03:23:36.861
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":133,"skipped":2530,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.069 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:23:34.796
    Aug 25 03:23:34.796: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubelet-test 08/25/22 03:23:34.798
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:34.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:34.814
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 25 03:23:34.825: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc" in namespace "kubelet-test-4781" to be "running and ready"
    Aug 25 03:23:34.828: INFO: Pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992493ms
    Aug 25 03:23:34.828: INFO: The phase of Pod busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:23:36.834: INFO: Pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008530102s
    Aug 25 03:23:36.834: INFO: The phase of Pod busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc is Running (Ready = true)
    Aug 25 03:23:36.834: INFO: Pod "busybox-readonly-fs7f03416d-156c-445d-9c32-17907903cabc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 25 03:23:36.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4781" for this suite. 08/25/22 03:23:36.861
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:23:36.869
Aug 25 03:23:36.869: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename taint-single-pod 08/25/22 03:23:36.87
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:36.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:36.888
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Aug 25 03:23:36.892: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 25 03:24:36.968: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Aug 25 03:24:36.973: INFO: Starting informer...
STEP: Starting pod... 08/25/22 03:24:36.973
Aug 25 03:24:37.188: INFO: Pod is running on bobymcbobs-c849-control-plane-p55pp. Tainting Node
STEP: Trying to apply a taint on the Node 08/25/22 03:24:37.188
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 03:24:37.207
STEP: Waiting short time to make sure Pod is queued for deletion 08/25/22 03:24:37.211
Aug 25 03:24:37.212: INFO: Pod wasn't evicted. Proceeding
Aug 25 03:24:37.212: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 03:24:37.235
STEP: Waiting some time to make sure that toleration time passed. 08/25/22 03:24:37.241
Aug 25 03:25:52.243: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:25:52.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1423" for this suite. 08/25/22 03:25:52.248
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":134,"skipped":2577,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [135.385 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:23:36.869
    Aug 25 03:23:36.869: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename taint-single-pod 08/25/22 03:23:36.87
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:23:36.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:23:36.888
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Aug 25 03:23:36.892: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 25 03:24:36.968: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Aug 25 03:24:36.973: INFO: Starting informer...
    STEP: Starting pod... 08/25/22 03:24:36.973
    Aug 25 03:24:37.188: INFO: Pod is running on bobymcbobs-c849-control-plane-p55pp. Tainting Node
    STEP: Trying to apply a taint on the Node 08/25/22 03:24:37.188
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 03:24:37.207
    STEP: Waiting short time to make sure Pod is queued for deletion 08/25/22 03:24:37.211
    Aug 25 03:24:37.212: INFO: Pod wasn't evicted. Proceeding
    Aug 25 03:24:37.212: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/25/22 03:24:37.235
    STEP: Waiting some time to make sure that toleration time passed. 08/25/22 03:24:37.241
    Aug 25 03:25:52.243: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:25:52.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-1423" for this suite. 08/25/22 03:25:52.248
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:25:52.255
Aug 25 03:25:52.255: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:25:52.257
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:52.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:52.276
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:25:52.28
Aug 25 03:25:52.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-5541 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 25 03:25:52.366: INFO: stderr: ""
Aug 25 03:25:52.366: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/25/22 03:25:52.366
Aug 25 03:25:52.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-5541 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Aug 25 03:25:52.612: INFO: stderr: ""
Aug 25 03:25:52.612: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:25:52.612
Aug 25 03:25:52.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-5541 delete pods e2e-test-httpd-pod'
Aug 25 03:25:54.162: INFO: stderr: ""
Aug 25 03:25:54.162: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:25:54.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5541" for this suite. 08/25/22 03:25:54.167
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":135,"skipped":2590,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [1.916 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:25:52.255
    Aug 25 03:25:52.255: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:25:52.257
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:52.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:52.276
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:25:52.28
    Aug 25 03:25:52.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-5541 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 25 03:25:52.366: INFO: stderr: ""
    Aug 25 03:25:52.366: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/25/22 03:25:52.366
    Aug 25 03:25:52.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-5541 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Aug 25 03:25:52.612: INFO: stderr: ""
    Aug 25 03:25:52.612: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:25:52.612
    Aug 25 03:25:52.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-5541 delete pods e2e-test-httpd-pod'
    Aug 25 03:25:54.162: INFO: stderr: ""
    Aug 25 03:25:54.162: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:25:54.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5541" for this suite. 08/25/22 03:25:54.167
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:25:54.172
Aug 25 03:25:54.172: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename podtemplate 08/25/22 03:25:54.173
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:54.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:54.195
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/25/22 03:25:54.199
Aug 25 03:25:54.202: INFO: created test-podtemplate-1
Aug 25 03:25:54.205: INFO: created test-podtemplate-2
Aug 25 03:25:54.208: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/25/22 03:25:54.208
STEP: delete collection of pod templates 08/25/22 03:25:54.211
Aug 25 03:25:54.211: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/25/22 03:25:54.219
Aug 25 03:25:54.219: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Aug 25 03:25:54.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9765" for this suite. 08/25/22 03:25:54.225
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":136,"skipped":2596,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.057 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:25:54.172
    Aug 25 03:25:54.172: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename podtemplate 08/25/22 03:25:54.173
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:54.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:54.195
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/25/22 03:25:54.199
    Aug 25 03:25:54.202: INFO: created test-podtemplate-1
    Aug 25 03:25:54.205: INFO: created test-podtemplate-2
    Aug 25 03:25:54.208: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/25/22 03:25:54.208
    STEP: delete collection of pod templates 08/25/22 03:25:54.211
    Aug 25 03:25:54.211: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/25/22 03:25:54.219
    Aug 25 03:25:54.219: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Aug 25 03:25:54.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9765" for this suite. 08/25/22 03:25:54.225
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:25:54.23
Aug 25 03:25:54.230: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:25:54.232
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:54.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:54.253
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-b38eaf68-fa83-488a-b901-f99efa80eaed 08/25/22 03:25:54.258
STEP: Creating a pod to test consume secrets 08/25/22 03:25:54.261
Aug 25 03:25:54.268: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77" in namespace "projected-5758" to be "Succeeded or Failed"
Aug 25 03:25:54.271: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170765ms
Aug 25 03:25:56.277: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77": Phase="Running", Reason="", readiness=false. Elapsed: 2.008557122s
Aug 25 03:25:58.276: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007857344s
STEP: Saw pod success 08/25/22 03:25:58.276
Aug 25 03:25:58.276: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77" satisfied condition "Succeeded or Failed"
Aug 25 03:25:58.279: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:25:58.295
Aug 25 03:25:58.303: INFO: Waiting for pod pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77 to disappear
Aug 25 03:25:58.306: INFO: Pod pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 03:25:58.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5758" for this suite. 08/25/22 03:25:58.31
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":137,"skipped":2604,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.085 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:25:54.23
    Aug 25 03:25:54.230: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:25:54.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:54.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:54.253
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-b38eaf68-fa83-488a-b901-f99efa80eaed 08/25/22 03:25:54.258
    STEP: Creating a pod to test consume secrets 08/25/22 03:25:54.261
    Aug 25 03:25:54.268: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77" in namespace "projected-5758" to be "Succeeded or Failed"
    Aug 25 03:25:54.271: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170765ms
    Aug 25 03:25:56.277: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77": Phase="Running", Reason="", readiness=false. Elapsed: 2.008557122s
    Aug 25 03:25:58.276: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007857344s
    STEP: Saw pod success 08/25/22 03:25:58.276
    Aug 25 03:25:58.276: INFO: Pod "pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77" satisfied condition "Succeeded or Failed"
    Aug 25 03:25:58.279: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:25:58.295
    Aug 25 03:25:58.303: INFO: Waiting for pod pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77 to disappear
    Aug 25 03:25:58.306: INFO: Pod pod-projected-secrets-88c23981-587e-4f3a-b515-e4cc862c3a77 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 03:25:58.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5758" for this suite. 08/25/22 03:25:58.31
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:25:58.315
Aug 25 03:25:58.315: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 03:25:58.316
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:58.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:58.333
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-212de138-8401-49b4-9640-e45aede476d1 in namespace container-probe-729 08/25/22 03:25:58.336
Aug 25 03:25:58.342: INFO: Waiting up to 5m0s for pod "busybox-212de138-8401-49b4-9640-e45aede476d1" in namespace "container-probe-729" to be "not pending"
Aug 25 03:25:58.344: INFO: Pod "busybox-212de138-8401-49b4-9640-e45aede476d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537848ms
Aug 25 03:26:00.349: INFO: Pod "busybox-212de138-8401-49b4-9640-e45aede476d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007276788s
Aug 25 03:26:00.349: INFO: Pod "busybox-212de138-8401-49b4-9640-e45aede476d1" satisfied condition "not pending"
Aug 25 03:26:00.349: INFO: Started pod busybox-212de138-8401-49b4-9640-e45aede476d1 in namespace container-probe-729
STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:26:00.349
Aug 25 03:26:00.353: INFO: Initial restart count of pod busybox-212de138-8401-49b4-9640-e45aede476d1 is 0
STEP: deleting the pod 08/25/22 03:30:01.004
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 03:30:01.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-729" for this suite. 08/25/22 03:30:01.018
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":138,"skipped":2605,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [242.708 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:25:58.315
    Aug 25 03:25:58.315: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 03:25:58.316
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:25:58.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:25:58.333
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-212de138-8401-49b4-9640-e45aede476d1 in namespace container-probe-729 08/25/22 03:25:58.336
    Aug 25 03:25:58.342: INFO: Waiting up to 5m0s for pod "busybox-212de138-8401-49b4-9640-e45aede476d1" in namespace "container-probe-729" to be "not pending"
    Aug 25 03:25:58.344: INFO: Pod "busybox-212de138-8401-49b4-9640-e45aede476d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537848ms
    Aug 25 03:26:00.349: INFO: Pod "busybox-212de138-8401-49b4-9640-e45aede476d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007276788s
    Aug 25 03:26:00.349: INFO: Pod "busybox-212de138-8401-49b4-9640-e45aede476d1" satisfied condition "not pending"
    Aug 25 03:26:00.349: INFO: Started pod busybox-212de138-8401-49b4-9640-e45aede476d1 in namespace container-probe-729
    STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:26:00.349
    Aug 25 03:26:00.353: INFO: Initial restart count of pod busybox-212de138-8401-49b4-9640-e45aede476d1 is 0
    STEP: deleting the pod 08/25/22 03:30:01.004
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 03:30:01.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-729" for this suite. 08/25/22 03:30:01.018
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:30:01.024
Aug 25 03:30:01.024: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 03:30:01.025
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:30:01.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:30:01.054
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb in namespace container-probe-2082 08/25/22 03:30:01.06
Aug 25 03:30:01.068: INFO: Waiting up to 5m0s for pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb" in namespace "container-probe-2082" to be "not pending"
Aug 25 03:30:01.071: INFO: Pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338986ms
Aug 25 03:30:03.076: INFO: Pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00796163s
Aug 25 03:30:03.076: INFO: Pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb" satisfied condition "not pending"
Aug 25 03:30:03.076: INFO: Started pod test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb in namespace container-probe-2082
STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:30:03.076
Aug 25 03:30:03.080: INFO: Initial restart count of pod test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb is 0
STEP: deleting the pod 08/25/22 03:34:03.73
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 03:34:03.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2082" for this suite. 08/25/22 03:34:03.743
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":139,"skipped":2619,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [242.724 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:30:01.024
    Aug 25 03:30:01.024: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 03:30:01.025
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:30:01.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:30:01.054
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb in namespace container-probe-2082 08/25/22 03:30:01.06
    Aug 25 03:30:01.068: INFO: Waiting up to 5m0s for pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb" in namespace "container-probe-2082" to be "not pending"
    Aug 25 03:30:01.071: INFO: Pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338986ms
    Aug 25 03:30:03.076: INFO: Pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00796163s
    Aug 25 03:30:03.076: INFO: Pod "test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb" satisfied condition "not pending"
    Aug 25 03:30:03.076: INFO: Started pod test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb in namespace container-probe-2082
    STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:30:03.076
    Aug 25 03:30:03.080: INFO: Initial restart count of pod test-webserver-1d37d80d-1554-4de8-9f42-f4bfdcce3ceb is 0
    STEP: deleting the pod 08/25/22 03:34:03.73
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 03:34:03.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2082" for this suite. 08/25/22 03:34:03.743
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:34:03.754
Aug 25 03:34:03.754: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/25/22 03:34:03.755
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:34:03.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:34:03.772
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/25/22 03:34:03.777
STEP: Creating hostNetwork=false pod 08/25/22 03:34:03.777
Aug 25 03:34:03.784: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3678" to be "running and ready"
Aug 25 03:34:03.788: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.93013ms
Aug 25 03:34:03.788: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:34:05.793: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008775861s
Aug 25 03:34:05.793: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 25 03:34:05.793: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/25/22 03:34:05.797
Aug 25 03:34:05.803: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3678" to be "running and ready"
Aug 25 03:34:05.807: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018415ms
Aug 25 03:34:05.807: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:34:07.812: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008791123s
Aug 25 03:34:07.812: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 25 03:34:07.812: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/25/22 03:34:07.816
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/25/22 03:34:07.816
Aug 25 03:34:07.816: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:07.816: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:07.817: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:07.817: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 25 03:34:07.950: INFO: Exec stderr: ""
Aug 25 03:34:07.950: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:07.951: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:07.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 25 03:34:08.002: INFO: Exec stderr: ""
Aug 25 03:34:08.002: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.003: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.003: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 25 03:34:08.100: INFO: Exec stderr: ""
Aug 25 03:34:08.100: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.100: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.101: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.101: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 25 03:34:08.174: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/25/22 03:34:08.174
Aug 25 03:34:08.174: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.174: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.175: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.175: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 25 03:34:08.276: INFO: Exec stderr: ""
Aug 25 03:34:08.276: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.276: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.277: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.277: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 25 03:34:08.349: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/25/22 03:34:08.349
Aug 25 03:34:08.349: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.349: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.350: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.350: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 25 03:34:08.457: INFO: Exec stderr: ""
Aug 25 03:34:08.457: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.457: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.457: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.457: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 25 03:34:08.555: INFO: Exec stderr: ""
Aug 25 03:34:08.555: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.555: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.556: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.556: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 25 03:34:08.662: INFO: Exec stderr: ""
Aug 25 03:34:08.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:34:08.662: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:34:08.662: INFO: ExecWithOptions: Clientset creation
Aug 25 03:34:08.663: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 25 03:34:08.760: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Aug 25 03:34:08.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3678" for this suite. 08/25/22 03:34:08.764
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":140,"skipped":2697,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.015 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:34:03.754
    Aug 25 03:34:03.754: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/25/22 03:34:03.755
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:34:03.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:34:03.772
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/25/22 03:34:03.777
    STEP: Creating hostNetwork=false pod 08/25/22 03:34:03.777
    Aug 25 03:34:03.784: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3678" to be "running and ready"
    Aug 25 03:34:03.788: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.93013ms
    Aug 25 03:34:03.788: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:34:05.793: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008775861s
    Aug 25 03:34:05.793: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 25 03:34:05.793: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/25/22 03:34:05.797
    Aug 25 03:34:05.803: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3678" to be "running and ready"
    Aug 25 03:34:05.807: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018415ms
    Aug 25 03:34:05.807: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:34:07.812: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008791123s
    Aug 25 03:34:07.812: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 25 03:34:07.812: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/25/22 03:34:07.816
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/25/22 03:34:07.816
    Aug 25 03:34:07.816: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:07.816: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:07.817: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:07.817: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 25 03:34:07.950: INFO: Exec stderr: ""
    Aug 25 03:34:07.950: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:07.951: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:07.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 25 03:34:08.002: INFO: Exec stderr: ""
    Aug 25 03:34:08.002: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.003: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.003: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 25 03:34:08.100: INFO: Exec stderr: ""
    Aug 25 03:34:08.100: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.100: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.101: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.101: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 25 03:34:08.174: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/25/22 03:34:08.174
    Aug 25 03:34:08.174: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.174: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.175: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.175: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 25 03:34:08.276: INFO: Exec stderr: ""
    Aug 25 03:34:08.276: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.276: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.277: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.277: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 25 03:34:08.349: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/25/22 03:34:08.349
    Aug 25 03:34:08.349: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.349: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.350: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.350: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 25 03:34:08.457: INFO: Exec stderr: ""
    Aug 25 03:34:08.457: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.457: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.457: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.457: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 25 03:34:08.555: INFO: Exec stderr: ""
    Aug 25 03:34:08.555: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.555: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.556: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.556: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 25 03:34:08.662: INFO: Exec stderr: ""
    Aug 25 03:34:08.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3678 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:34:08.662: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:34:08.662: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:34:08.663: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3678/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 25 03:34:08.760: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Aug 25 03:34:08.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3678" for this suite. 08/25/22 03:34:08.764
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:34:08.771
Aug 25 03:34:08.771: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename cronjob 08/25/22 03:34:08.772
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:34:08.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:34:08.79
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/25/22 03:34:08.793
STEP: Ensuring a job is scheduled 08/25/22 03:34:08.796
STEP: Ensuring exactly one is scheduled 08/25/22 03:35:00.801
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/25/22 03:35:00.805
STEP: Ensuring the job is replaced with a new one 08/25/22 03:35:00.81
STEP: Removing cronjob 08/25/22 03:36:00.815
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 25 03:36:00.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4862" for this suite. 08/25/22 03:36:00.825
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":141,"skipped":2712,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [112.059 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:34:08.771
    Aug 25 03:34:08.771: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename cronjob 08/25/22 03:34:08.772
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:34:08.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:34:08.79
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/25/22 03:34:08.793
    STEP: Ensuring a job is scheduled 08/25/22 03:34:08.796
    STEP: Ensuring exactly one is scheduled 08/25/22 03:35:00.801
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/25/22 03:35:00.805
    STEP: Ensuring the job is replaced with a new one 08/25/22 03:35:00.81
    STEP: Removing cronjob 08/25/22 03:36:00.815
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 25 03:36:00.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4862" for this suite. 08/25/22 03:36:00.825
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:00.83
Aug 25 03:36:00.830: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:36:00.831
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:00.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:00.849
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-73e4157b-a89f-4304-95f5-178261b832d8 08/25/22 03:36:00.852
STEP: Creating a pod to test consume configMaps 08/25/22 03:36:00.856
Aug 25 03:36:00.863: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f" in namespace "projected-5800" to be "Succeeded or Failed"
Aug 25 03:36:00.867: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5449ms
Aug 25 03:36:02.872: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008388788s
Aug 25 03:36:04.873: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010278371s
STEP: Saw pod success 08/25/22 03:36:04.874
Aug 25 03:36:04.874: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f" satisfied condition "Succeeded or Failed"
Aug 25 03:36:04.878: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:36:04.9
Aug 25 03:36:04.907: INFO: Waiting for pod pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f to disappear
Aug 25 03:36:04.910: INFO: Pod pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 03:36:04.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5800" for this suite. 08/25/22 03:36:04.915
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":142,"skipped":2715,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.089 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:00.83
    Aug 25 03:36:00.830: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:36:00.831
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:00.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:00.849
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-73e4157b-a89f-4304-95f5-178261b832d8 08/25/22 03:36:00.852
    STEP: Creating a pod to test consume configMaps 08/25/22 03:36:00.856
    Aug 25 03:36:00.863: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f" in namespace "projected-5800" to be "Succeeded or Failed"
    Aug 25 03:36:00.867: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5449ms
    Aug 25 03:36:02.872: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008388788s
    Aug 25 03:36:04.873: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010278371s
    STEP: Saw pod success 08/25/22 03:36:04.874
    Aug 25 03:36:04.874: INFO: Pod "pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f" satisfied condition "Succeeded or Failed"
    Aug 25 03:36:04.878: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:36:04.9
    Aug 25 03:36:04.907: INFO: Waiting for pod pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f to disappear
    Aug 25 03:36:04.910: INFO: Pod pod-projected-configmaps-7add233a-2790-4006-9f78-536aecec521f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 03:36:04.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5800" for this suite. 08/25/22 03:36:04.915
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:04.921
Aug 25 03:36:04.921: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:36:04.923
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:04.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:04.939
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/25/22 03:36:04.943
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/25/22 03:36:04.944
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/25/22 03:36:04.944
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/25/22 03:36:04.944
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/25/22 03:36:04.946
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/25/22 03:36:04.946
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/25/22 03:36:04.948
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:36:04.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5059" for this suite. 08/25/22 03:36:04.951
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":143,"skipped":2741,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.034 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:04.921
    Aug 25 03:36:04.921: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:36:04.923
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:04.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:04.939
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/25/22 03:36:04.943
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/25/22 03:36:04.944
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/25/22 03:36:04.944
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/25/22 03:36:04.944
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/25/22 03:36:04.946
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/25/22 03:36:04.946
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/25/22 03:36:04.948
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:36:04.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5059" for this suite. 08/25/22 03:36:04.951
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:04.957
Aug 25 03:36:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:36:04.958
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:04.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:04.972
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:36:04.985
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:36:05.218
STEP: Deploying the webhook pod 08/25/22 03:36:05.225
STEP: Wait for the deployment to be ready 08/25/22 03:36:05.235
Aug 25 03:36:05.241: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 25 03:36:07.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:36:09.261
STEP: Verifying the service has paired with the endpoint 08/25/22 03:36:09.27
Aug 25 03:36:10.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 08/25/22 03:36:10.314
STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 03:36:10.35
STEP: Deleting the collection of validation webhooks 08/25/22 03:36:10.375
STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 03:36:10.401
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:36:10.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9700" for this suite. 08/25/22 03:36:10.413
STEP: Destroying namespace "webhook-9700-markers" for this suite. 08/25/22 03:36:10.416
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":144,"skipped":2753,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:04.957
    Aug 25 03:36:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:36:04.958
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:04.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:04.972
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:36:04.985
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:36:05.218
    STEP: Deploying the webhook pod 08/25/22 03:36:05.225
    STEP: Wait for the deployment to be ready 08/25/22 03:36:05.235
    Aug 25 03:36:05.241: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Aug 25 03:36:07.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 36, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:36:09.261
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:36:09.27
    Aug 25 03:36:10.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 08/25/22 03:36:10.314
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 03:36:10.35
    STEP: Deleting the collection of validation webhooks 08/25/22 03:36:10.375
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 03:36:10.401
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:36:10.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9700" for this suite. 08/25/22 03:36:10.413
    STEP: Destroying namespace "webhook-9700-markers" for this suite. 08/25/22 03:36:10.416
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:10.446
Aug 25 03:36:10.446: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:36:10.447
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:10.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:10.46
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 08/25/22 03:36:10.464
STEP: submitting the pod to kubernetes 08/25/22 03:36:10.464
Aug 25 03:36:10.468: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" in namespace "pods-6846" to be "running and ready"
Aug 25 03:36:10.471: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583116ms
Aug 25 03:36:10.471: INFO: The phase of Pod pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:36:12.477: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008466816s
Aug 25 03:36:12.477: INFO: The phase of Pod pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb is Running (Ready = true)
Aug 25 03:36:12.477: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/25/22 03:36:12.481
STEP: updating the pod 08/25/22 03:36:12.485
Aug 25 03:36:12.998: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb"
Aug 25 03:36:12.999: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" in namespace "pods-6846" to be "terminated with reason DeadlineExceeded"
Aug 25 03:36:13.002: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Running", Reason="", readiness=true. Elapsed: 3.777891ms
Aug 25 03:36:15.008: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009156718s
Aug 25 03:36:17.008: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009002953s
Aug 25 03:36:17.008: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:36:17.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6846" for this suite. 08/25/22 03:36:17.012
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":145,"skipped":2763,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.572 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:10.446
    Aug 25 03:36:10.446: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:36:10.447
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:10.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:10.46
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 08/25/22 03:36:10.464
    STEP: submitting the pod to kubernetes 08/25/22 03:36:10.464
    Aug 25 03:36:10.468: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" in namespace "pods-6846" to be "running and ready"
    Aug 25 03:36:10.471: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583116ms
    Aug 25 03:36:10.471: INFO: The phase of Pod pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:36:12.477: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008466816s
    Aug 25 03:36:12.477: INFO: The phase of Pod pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb is Running (Ready = true)
    Aug 25 03:36:12.477: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/25/22 03:36:12.481
    STEP: updating the pod 08/25/22 03:36:12.485
    Aug 25 03:36:12.998: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb"
    Aug 25 03:36:12.999: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" in namespace "pods-6846" to be "terminated with reason DeadlineExceeded"
    Aug 25 03:36:13.002: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Running", Reason="", readiness=true. Elapsed: 3.777891ms
    Aug 25 03:36:15.008: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009156718s
    Aug 25 03:36:17.008: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009002953s
    Aug 25 03:36:17.008: INFO: Pod "pod-update-activedeadlineseconds-b577ba66-b5e2-42a9-b6a1-2328c1ce1deb" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:36:17.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6846" for this suite. 08/25/22 03:36:17.012
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:17.018
Aug 25 03:36:17.019: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:36:17.02
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:17.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:17.037
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:36:17.052
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:36:17.65
STEP: Deploying the webhook pod 08/25/22 03:36:17.655
STEP: Wait for the deployment to be ready 08/25/22 03:36:17.665
Aug 25 03:36:17.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:36:19.688
STEP: Verifying the service has paired with the endpoint 08/25/22 03:36:19.697
Aug 25 03:36:20.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/25/22 03:36:20.702
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/25/22 03:36:20.723
STEP: Creating a dummy validating-webhook-configuration object 08/25/22 03:36:20.739
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/25/22 03:36:20.749
STEP: Creating a dummy mutating-webhook-configuration object 08/25/22 03:36:20.755
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/25/22 03:36:20.763
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:36:20.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3838" for this suite. 08/25/22 03:36:20.778
STEP: Destroying namespace "webhook-3838-markers" for this suite. 08/25/22 03:36:20.781
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":146,"skipped":2771,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.796 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:17.018
    Aug 25 03:36:17.019: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:36:17.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:17.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:17.037
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:36:17.052
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:36:17.65
    STEP: Deploying the webhook pod 08/25/22 03:36:17.655
    STEP: Wait for the deployment to be ready 08/25/22 03:36:17.665
    Aug 25 03:36:17.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:36:19.688
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:36:19.697
    Aug 25 03:36:20.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/25/22 03:36:20.702
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/25/22 03:36:20.723
    STEP: Creating a dummy validating-webhook-configuration object 08/25/22 03:36:20.739
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/25/22 03:36:20.749
    STEP: Creating a dummy mutating-webhook-configuration object 08/25/22 03:36:20.755
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/25/22 03:36:20.763
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:36:20.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3838" for this suite. 08/25/22 03:36:20.778
    STEP: Destroying namespace "webhook-3838-markers" for this suite. 08/25/22 03:36:20.781
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:20.815
Aug 25 03:36:20.815: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:36:20.817
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:20.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:20.831
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 08/25/22 03:36:20.835
Aug 25 03:36:20.840: INFO: Waiting up to 5m0s for pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec" in namespace "emptydir-4742" to be "Succeeded or Failed"
Aug 25 03:36:20.843: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645121ms
Aug 25 03:36:22.848: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008221315s
Aug 25 03:36:24.849: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00864062s
STEP: Saw pod success 08/25/22 03:36:24.849
Aug 25 03:36:24.849: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec" satisfied condition "Succeeded or Failed"
Aug 25 03:36:24.853: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-2720e40e-f489-40a9-99f5-286da302b5ec container test-container: <nil>
STEP: delete the pod 08/25/22 03:36:24.859
Aug 25 03:36:24.866: INFO: Waiting for pod pod-2720e40e-f489-40a9-99f5-286da302b5ec to disappear
Aug 25 03:36:24.869: INFO: Pod pod-2720e40e-f489-40a9-99f5-286da302b5ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:36:24.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4742" for this suite. 08/25/22 03:36:24.874
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":2776,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:20.815
    Aug 25 03:36:20.815: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:36:20.817
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:20.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:20.831
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/25/22 03:36:20.835
    Aug 25 03:36:20.840: INFO: Waiting up to 5m0s for pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec" in namespace "emptydir-4742" to be "Succeeded or Failed"
    Aug 25 03:36:20.843: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645121ms
    Aug 25 03:36:22.848: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008221315s
    Aug 25 03:36:24.849: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00864062s
    STEP: Saw pod success 08/25/22 03:36:24.849
    Aug 25 03:36:24.849: INFO: Pod "pod-2720e40e-f489-40a9-99f5-286da302b5ec" satisfied condition "Succeeded or Failed"
    Aug 25 03:36:24.853: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-2720e40e-f489-40a9-99f5-286da302b5ec container test-container: <nil>
    STEP: delete the pod 08/25/22 03:36:24.859
    Aug 25 03:36:24.866: INFO: Waiting for pod pod-2720e40e-f489-40a9-99f5-286da302b5ec to disappear
    Aug 25 03:36:24.869: INFO: Pod pod-2720e40e-f489-40a9-99f5-286da302b5ec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:36:24.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4742" for this suite. 08/25/22 03:36:24.874
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:24.881
Aug 25 03:36:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:36:24.883
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:24.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:24.895
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/25/22 03:36:24.899
Aug 25 03:36:24.904: INFO: Waiting up to 5m0s for pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74" in namespace "emptydir-5563" to be "Succeeded or Failed"
Aug 25 03:36:24.906: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.442333ms
Aug 25 03:36:26.911: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007449546s
Aug 25 03:36:28.912: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007923004s
STEP: Saw pod success 08/25/22 03:36:28.912
Aug 25 03:36:28.912: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74" satisfied condition "Succeeded or Failed"
Aug 25 03:36:28.916: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-8db5461e-95e7-49b7-8b14-e18ebc13be74 container test-container: <nil>
STEP: delete the pod 08/25/22 03:36:28.922
Aug 25 03:36:28.928: INFO: Waiting for pod pod-8db5461e-95e7-49b7-8b14-e18ebc13be74 to disappear
Aug 25 03:36:28.931: INFO: Pod pod-8db5461e-95e7-49b7-8b14-e18ebc13be74 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:36:28.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5563" for this suite. 08/25/22 03:36:28.935
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2788,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:24.881
    Aug 25 03:36:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:36:24.883
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:24.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:24.895
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/25/22 03:36:24.899
    Aug 25 03:36:24.904: INFO: Waiting up to 5m0s for pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74" in namespace "emptydir-5563" to be "Succeeded or Failed"
    Aug 25 03:36:24.906: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.442333ms
    Aug 25 03:36:26.911: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007449546s
    Aug 25 03:36:28.912: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007923004s
    STEP: Saw pod success 08/25/22 03:36:28.912
    Aug 25 03:36:28.912: INFO: Pod "pod-8db5461e-95e7-49b7-8b14-e18ebc13be74" satisfied condition "Succeeded or Failed"
    Aug 25 03:36:28.916: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-8db5461e-95e7-49b7-8b14-e18ebc13be74 container test-container: <nil>
    STEP: delete the pod 08/25/22 03:36:28.922
    Aug 25 03:36:28.928: INFO: Waiting for pod pod-8db5461e-95e7-49b7-8b14-e18ebc13be74 to disappear
    Aug 25 03:36:28.931: INFO: Pod pod-8db5461e-95e7-49b7-8b14-e18ebc13be74 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:36:28.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5563" for this suite. 08/25/22 03:36:28.935
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:28.944
Aug 25 03:36:28.944: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:36:28.945
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:28.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:28.961
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Aug 25 03:36:28.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 create -f -'
Aug 25 03:36:29.209: INFO: stderr: ""
Aug 25 03:36:29.209: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 25 03:36:29.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 create -f -'
Aug 25 03:36:29.468: INFO: stderr: ""
Aug 25 03:36:29.469: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/25/22 03:36:29.469
Aug 25 03:36:30.474: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:36:30.474: INFO: Found 0 / 1
Aug 25 03:36:31.474: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:36:31.474: INFO: Found 1 / 1
Aug 25 03:36:31.474: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 25 03:36:31.478: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 25 03:36:31.478: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 25 03:36:31.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe pod agnhost-primary-jf6ng'
Aug 25 03:36:31.571: INFO: stderr: ""
Aug 25 03:36:31.571: INFO: stdout: "Name:             agnhost-primary-jf6ng\nNamespace:        kubectl-9831\nPriority:         0\nService Account:  default\nNode:             bobymcbobs-c849-control-plane-p55pp/86.109.11.217\nStart Time:       Thu, 25 Aug 2022 03:36:29 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.0.14\nIPs:\n  IP:           192.168.0.14\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://936edf609190538f492a4efd7e52262f424f69f224e14318321d63b10e94a5c3\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 25 Aug 2022 03:36:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fhbk7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fhbk7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9831/agnhost-primary-jf6ng to bobymcbobs-c849-control-plane-p55pp\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Aug 25 03:36:31.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe rc agnhost-primary'
Aug 25 03:36:31.665: INFO: stderr: ""
Aug 25 03:36:31.665: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9831\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-jf6ng\n"
Aug 25 03:36:31.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe service agnhost-primary'
Aug 25 03:36:31.747: INFO: stderr: ""
Aug 25 03:36:31.747: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9831\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.152.234\nIPs:               10.105.152.234\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.0.14:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 25 03:36:31.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe node bobymcbobs-c849-control-plane-p55pp'
Aug 25 03:36:31.944: INFO: stderr: ""
Aug 25 03:36:31.944: INFO: stdout: "Name:               bobymcbobs-c849-control-plane-p55pp\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=bobymcbobs-c849-control-plane-p55pp\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        cluster.x-k8s.io/cluster-name: bobymcbobs-c849\n                    cluster.x-k8s.io/cluster-namespace: sharingio-pair\n                    cluster.x-k8s.io/machine: bobymcbobs-c849-control-plane-fmvtt\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: bobymcbobs-c849-control-plane\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 25 Aug 2022 02:40:28 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  bobymcbobs-c849-control-plane-p55pp\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 25 Aug 2022 03:36:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 25 Aug 2022 02:40:57 +0000   Thu, 25 Aug 2022 02:40:57 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:52 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  86.109.11.217\n  Hostname:    bobymcbobs-c849-control-plane-p55pp\nCapacity:\n  cpu:                    64\n  ephemeral-storage:      228651856Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 394837272Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    64\n  ephemeral-storage:      210725550141\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 394734872Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 4b0c7bda23ca4aa6a943cda55f9fa1e2\n  System UUID:                4c4c4544-0053-4710-8038-b8c04f483033\n  Boot ID:                    217f066f-7295-498b-9b6e-db50783a6a97\n  Kernel Version:             5.4.0-109-generic\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.7\n  Kubelet Version:            v1.25.0\n  Kube-Proxy Version:         v1.25.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3\nNon-terminated Pods:          (47 in total)\n  Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---\n  bobymcbobs                  environment-0                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  cert-manager                cert-manager-66bd77df8f-qlhqg                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  cert-manager                cert-manager-cainjector-6495667ff4-x6lll                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  cert-manager                cert-manager-webhook-59d6cdfb6f-5fml5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-external            contour-588fc6cc6d-dq6wf                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-external            contour-588fc6cc6d-f66zz                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-external            envoy-cxlbl                                                    240m (0%)     900m (1%)   240Mi (0%)       900Mi (0%)     10m\n  contour-internal            contour-8597b78798-5755z                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-internal            contour-8597b78798-lb4wb                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-internal            envoy-7sxnc                                                    240m (0%)     900m (1%)   240Mi (0%)       900Mi (0%)     10m\n  cronjob-4862                replace-27689976-ldgq8                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         31s\n  external-dns                external-dns-67d79cbcd4-bf29h                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  helm-operator               helm-operator-7959478576-6mcjz                                 50m (0%)      0 (0%)      64Mi (0%)        0 (0%)         11m\n  knative-operator            knative-operator-594876444d-qzjs4                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  knative-serving             activator-88b7df5c-cbxk8                                       300m (0%)     1 (1%)      60Mi (0%)        600Mi (0%)     11m\n  knative-serving             autoscaler-7bf8ff94db-98bhj                                    100m (0%)     1 (1%)      100Mi (0%)       1000Mi (0%)    11m\n  knative-serving             autoscaler-hpa-776c44cc57-rcqj4                                30m (0%)      300m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             controller-7fd644cbc6-dfb6n                                    100m (0%)     1 (1%)      100Mi (0%)       1000Mi (0%)    11m\n  knative-serving             domain-mapping-5bcd85fbc6-2gshd                                30m (0%)      300m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             domainmapping-webhook-77f466bbc9-97ft7                         100m (0%)     500m (0%)   100Mi (0%)       500Mi (0%)     11m\n  knative-serving             net-certmanager-controller-6c8cb88879-86vmg                    30m (0%)      300m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             net-certmanager-webhook-79cfb96f68-kfnlr                       20m (0%)      200m (0%)   20Mi (0%)        200Mi (0%)     11m\n  knative-serving             net-contour-controller-5c5fd89fdb-lnf94                        40m (0%)      400m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             webhook-69d55f5549-ghp24                                       100m (0%)     500m (0%)   100Mi (0%)       500Mi (0%)     11m\n  kube-system                 coredns-565d847f94-684lz                                       100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     11m\n  kube-system                 coredns-565d847f94-lzgv5                                       100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     11m\n  kube-system                 etcd-bobymcbobs-c849-control-plane-p55pp                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         55m\n  kube-system                 kube-apiserver-bobymcbobs-c849-control-plane-p55pp             250m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-controller-manager-bobymcbobs-c849-control-plane-p55pp    200m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-proxy-b4lgh                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-scheduler-bobymcbobs-c849-control-plane-p55pp             100m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kubed-568bdbc6c4-vx7m6                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  kube-system                 metrics-server-5cb46dccf6-w9d6b                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  kube-system                 weave-net-75fql                                                100m (0%)     0 (0%)      200Mi (0%)       0 (0%)         55m\n  kubectl-9831                agnhost-primary-jf6ng                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  local-path-storage          local-path-provisioner-56c55476f9-xbkr9                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  metallb-system              controller-77cc7ff558-mrwjl                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  metallb-system              speaker-f82r7                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 distribution-7f9dff85c4-c276b                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 environment-exposer-5bb6d44465-8btrx                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 powerdns-5c6dbcf579-rgzxs                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 powerdns-db-7c897fddf4-l5cjb                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 public-html-go-http-server-55b9f584d4-4jrp5                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 reveal-multiplex-6bbbf59d6-k9lt8                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  sonobuoy                    sonobuoy                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  sonobuoy                    sonobuoy-e2e-job-39386ef5d0694a49                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj        0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests     Limits\n  --------               --------     ------\n  cpu                    2330m (3%)   7300m (11%)\n  memory                 1624Mi (0%)  7540Mi (1%)\n  ephemeral-storage      0 (0%)       0 (0%)\n  hugepages-1Gi          0 (0%)       0 (0%)\n  hugepages-2Mi          0 (0%)       0 (0%)\n  scheduling.k8s.io/foo  0            0\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 55m   kube-proxy       \n  Normal   Starting                 55m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      55m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  55m   kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           55m   node-controller  Node bobymcbobs-c849-control-plane-p55pp event: Registered Node bobymcbobs-c849-control-plane-p55pp in Controller\n  Normal   NodeReady                55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeReady\n"
Aug 25 03:36:31.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe namespace kubectl-9831'
Aug 25 03:36:32.031: INFO: stderr: ""
Aug 25 03:36:32.031: INFO: stdout: "Name:         kubectl-9831\nLabels:       e2e-framework=kubectl\n              e2e-run=065f2d67-9c1e-46c1-8ee8-6bb72077ffef\n              kubernetes.io/metadata.name=kubectl-9831\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:36:32.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9831" for this suite. 08/25/22 03:36:32.035
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":149,"skipped":2799,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.095 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:28.944
    Aug 25 03:36:28.944: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:36:28.945
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:28.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:28.961
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Aug 25 03:36:28.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 create -f -'
    Aug 25 03:36:29.209: INFO: stderr: ""
    Aug 25 03:36:29.209: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 25 03:36:29.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 create -f -'
    Aug 25 03:36:29.468: INFO: stderr: ""
    Aug 25 03:36:29.469: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/25/22 03:36:29.469
    Aug 25 03:36:30.474: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:36:30.474: INFO: Found 0 / 1
    Aug 25 03:36:31.474: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:36:31.474: INFO: Found 1 / 1
    Aug 25 03:36:31.474: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 25 03:36:31.478: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 25 03:36:31.478: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 25 03:36:31.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe pod agnhost-primary-jf6ng'
    Aug 25 03:36:31.571: INFO: stderr: ""
    Aug 25 03:36:31.571: INFO: stdout: "Name:             agnhost-primary-jf6ng\nNamespace:        kubectl-9831\nPriority:         0\nService Account:  default\nNode:             bobymcbobs-c849-control-plane-p55pp/86.109.11.217\nStart Time:       Thu, 25 Aug 2022 03:36:29 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.0.14\nIPs:\n  IP:           192.168.0.14\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://936edf609190538f492a4efd7e52262f424f69f224e14318321d63b10e94a5c3\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 25 Aug 2022 03:36:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fhbk7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fhbk7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9831/agnhost-primary-jf6ng to bobymcbobs-c849-control-plane-p55pp\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Aug 25 03:36:31.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe rc agnhost-primary'
    Aug 25 03:36:31.665: INFO: stderr: ""
    Aug 25 03:36:31.665: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9831\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-jf6ng\n"
    Aug 25 03:36:31.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe service agnhost-primary'
    Aug 25 03:36:31.747: INFO: stderr: ""
    Aug 25 03:36:31.747: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9831\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.152.234\nIPs:               10.105.152.234\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.0.14:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 25 03:36:31.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe node bobymcbobs-c849-control-plane-p55pp'
    Aug 25 03:36:31.944: INFO: stderr: ""
    Aug 25 03:36:31.944: INFO: stdout: "Name:               bobymcbobs-c849-control-plane-p55pp\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=bobymcbobs-c849-control-plane-p55pp\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        cluster.x-k8s.io/cluster-name: bobymcbobs-c849\n                    cluster.x-k8s.io/cluster-namespace: sharingio-pair\n                    cluster.x-k8s.io/machine: bobymcbobs-c849-control-plane-fmvtt\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: bobymcbobs-c849-control-plane\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 25 Aug 2022 02:40:28 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  bobymcbobs-c849-control-plane-p55pp\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 25 Aug 2022 03:36:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 25 Aug 2022 02:40:57 +0000   Thu, 25 Aug 2022 02:40:57 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 25 Aug 2022 03:32:02 +0000   Thu, 25 Aug 2022 02:40:52 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  86.109.11.217\n  Hostname:    bobymcbobs-c849-control-plane-p55pp\nCapacity:\n  cpu:                    64\n  ephemeral-storage:      228651856Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 394837272Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    64\n  ephemeral-storage:      210725550141\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 394734872Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 4b0c7bda23ca4aa6a943cda55f9fa1e2\n  System UUID:                4c4c4544-0053-4710-8038-b8c04f483033\n  Boot ID:                    217f066f-7295-498b-9b6e-db50783a6a97\n  Kernel Version:             5.4.0-109-generic\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.7\n  Kubelet Version:            v1.25.0\n  Kube-Proxy Version:         v1.25.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3\nNon-terminated Pods:          (47 in total)\n  Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---\n  bobymcbobs                  environment-0                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  cert-manager                cert-manager-66bd77df8f-qlhqg                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  cert-manager                cert-manager-cainjector-6495667ff4-x6lll                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  cert-manager                cert-manager-webhook-59d6cdfb6f-5fml5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-external            contour-588fc6cc6d-dq6wf                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-external            contour-588fc6cc6d-f66zz                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-external            envoy-cxlbl                                                    240m (0%)     900m (1%)   240Mi (0%)       900Mi (0%)     10m\n  contour-internal            contour-8597b78798-5755z                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-internal            contour-8597b78798-lb4wb                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  contour-internal            envoy-7sxnc                                                    240m (0%)     900m (1%)   240Mi (0%)       900Mi (0%)     10m\n  cronjob-4862                replace-27689976-ldgq8                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         31s\n  external-dns                external-dns-67d79cbcd4-bf29h                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  helm-operator               helm-operator-7959478576-6mcjz                                 50m (0%)      0 (0%)      64Mi (0%)        0 (0%)         11m\n  knative-operator            knative-operator-594876444d-qzjs4                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  knative-serving             activator-88b7df5c-cbxk8                                       300m (0%)     1 (1%)      60Mi (0%)        600Mi (0%)     11m\n  knative-serving             autoscaler-7bf8ff94db-98bhj                                    100m (0%)     1 (1%)      100Mi (0%)       1000Mi (0%)    11m\n  knative-serving             autoscaler-hpa-776c44cc57-rcqj4                                30m (0%)      300m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             controller-7fd644cbc6-dfb6n                                    100m (0%)     1 (1%)      100Mi (0%)       1000Mi (0%)    11m\n  knative-serving             domain-mapping-5bcd85fbc6-2gshd                                30m (0%)      300m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             domainmapping-webhook-77f466bbc9-97ft7                         100m (0%)     500m (0%)   100Mi (0%)       500Mi (0%)     11m\n  knative-serving             net-certmanager-controller-6c8cb88879-86vmg                    30m (0%)      300m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             net-certmanager-webhook-79cfb96f68-kfnlr                       20m (0%)      200m (0%)   20Mi (0%)        200Mi (0%)     11m\n  knative-serving             net-contour-controller-5c5fd89fdb-lnf94                        40m (0%)      400m (0%)   40Mi (0%)        400Mi (0%)     11m\n  knative-serving             webhook-69d55f5549-ghp24                                       100m (0%)     500m (0%)   100Mi (0%)       500Mi (0%)     11m\n  kube-system                 coredns-565d847f94-684lz                                       100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     11m\n  kube-system                 coredns-565d847f94-lzgv5                                       100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     11m\n  kube-system                 etcd-bobymcbobs-c849-control-plane-p55pp                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         55m\n  kube-system                 kube-apiserver-bobymcbobs-c849-control-plane-p55pp             250m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-controller-manager-bobymcbobs-c849-control-plane-p55pp    200m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-proxy-b4lgh                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-scheduler-bobymcbobs-c849-control-plane-p55pp             100m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kubed-568bdbc6c4-vx7m6                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  kube-system                 metrics-server-5cb46dccf6-w9d6b                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  kube-system                 weave-net-75fql                                                100m (0%)     0 (0%)      200Mi (0%)       0 (0%)         55m\n  kubectl-9831                agnhost-primary-jf6ng                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  local-path-storage          local-path-provisioner-56c55476f9-xbkr9                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  metallb-system              controller-77cc7ff558-mrwjl                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  metallb-system              speaker-f82r7                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 distribution-7f9dff85c4-c276b                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 environment-exposer-5bb6d44465-8btrx                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 powerdns-5c6dbcf579-rgzxs                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 powerdns-db-7c897fddf4-l5cjb                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 public-html-go-http-server-55b9f584d4-4jrp5                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pair-system                 reveal-multiplex-6bbbf59d6-k9lt8                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  sonobuoy                    sonobuoy                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  sonobuoy                    sonobuoy-e2e-job-39386ef5d0694a49                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj        0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests     Limits\n  --------               --------     ------\n  cpu                    2330m (3%)   7300m (11%)\n  memory                 1624Mi (0%)  7540Mi (1%)\n  ephemeral-storage      0 (0%)       0 (0%)\n  hugepages-1Gi          0 (0%)       0 (0%)\n  hugepages-2Mi          0 (0%)       0 (0%)\n  scheduling.k8s.io/foo  0            0\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 55m   kube-proxy       \n  Normal   Starting                 55m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      55m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  55m   kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           55m   node-controller  Node bobymcbobs-c849-control-plane-p55pp event: Registered Node bobymcbobs-c849-control-plane-p55pp in Controller\n  Normal   NodeReady                55m   kubelet          Node bobymcbobs-c849-control-plane-p55pp status is now: NodeReady\n"
    Aug 25 03:36:31.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9831 describe namespace kubectl-9831'
    Aug 25 03:36:32.031: INFO: stderr: ""
    Aug 25 03:36:32.031: INFO: stdout: "Name:         kubectl-9831\nLabels:       e2e-framework=kubectl\n              e2e-run=065f2d67-9c1e-46c1-8ee8-6bb72077ffef\n              kubernetes.io/metadata.name=kubectl-9831\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:36:32.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9831" for this suite. 08/25/22 03:36:32.035
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:32.04
Aug 25 03:36:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename watch 08/25/22 03:36:32.042
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:32.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:32.057
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/25/22 03:36:32.06
STEP: creating a new configmap 08/25/22 03:36:32.062
STEP: modifying the configmap once 08/25/22 03:36:32.065
STEP: changing the label value of the configmap 08/25/22 03:36:32.073
STEP: Expecting to observe a delete notification for the watched object 08/25/22 03:36:32.079
Aug 25 03:36:32.079: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31100 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:36:32.079: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31101 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:36:32.079: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31102 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/25/22 03:36:32.079
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/25/22 03:36:32.087
STEP: changing the label value of the configmap back 08/25/22 03:36:42.088
STEP: modifying the configmap a third time 08/25/22 03:36:42.096
STEP: deleting the configmap 08/25/22 03:36:42.102
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/25/22 03:36:42.106
Aug 25 03:36:42.106: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31195 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:36:42.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31196 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:36:42.107: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31197 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 25 03:36:42.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8040" for this suite. 08/25/22 03:36:42.112
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":150,"skipped":2801,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.076 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:32.04
    Aug 25 03:36:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename watch 08/25/22 03:36:32.042
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:32.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:32.057
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/25/22 03:36:32.06
    STEP: creating a new configmap 08/25/22 03:36:32.062
    STEP: modifying the configmap once 08/25/22 03:36:32.065
    STEP: changing the label value of the configmap 08/25/22 03:36:32.073
    STEP: Expecting to observe a delete notification for the watched object 08/25/22 03:36:32.079
    Aug 25 03:36:32.079: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31100 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:36:32.079: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31101 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:36:32.079: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31102 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/25/22 03:36:32.079
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/25/22 03:36:32.087
    STEP: changing the label value of the configmap back 08/25/22 03:36:42.088
    STEP: modifying the configmap a third time 08/25/22 03:36:42.096
    STEP: deleting the configmap 08/25/22 03:36:42.102
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/25/22 03:36:42.106
    Aug 25 03:36:42.106: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31195 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:36:42.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31196 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:36:42.107: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8040  dff4763c-34fb-45dd-bf04-6e942cebbd3a 31197 0 2022-08-25 03:36:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-08-25 03:36:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 25 03:36:42.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8040" for this suite. 08/25/22 03:36:42.112
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:42.117
Aug 25 03:36:42.117: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:36:42.118
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:42.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:42.135
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:36:42.138
Aug 25 03:36:42.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7" in namespace "projected-5329" to be "Succeeded or Failed"
Aug 25 03:36:42.149: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989659ms
Aug 25 03:36:44.153: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7": Phase="Running", Reason="", readiness=false. Elapsed: 2.007049581s
Aug 25 03:36:46.154: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008124863s
STEP: Saw pod success 08/25/22 03:36:46.154
Aug 25 03:36:46.154: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7" satisfied condition "Succeeded or Failed"
Aug 25 03:36:46.158: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7 container client-container: <nil>
STEP: delete the pod 08/25/22 03:36:46.164
Aug 25 03:36:46.170: INFO: Waiting for pod downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7 to disappear
Aug 25 03:36:46.174: INFO: Pod downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:36:46.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5329" for this suite. 08/25/22 03:36:46.178
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":151,"skipped":2804,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:42.117
    Aug 25 03:36:42.117: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:36:42.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:42.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:42.135
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:36:42.138
    Aug 25 03:36:42.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7" in namespace "projected-5329" to be "Succeeded or Failed"
    Aug 25 03:36:42.149: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989659ms
    Aug 25 03:36:44.153: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7": Phase="Running", Reason="", readiness=false. Elapsed: 2.007049581s
    Aug 25 03:36:46.154: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008124863s
    STEP: Saw pod success 08/25/22 03:36:46.154
    Aug 25 03:36:46.154: INFO: Pod "downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7" satisfied condition "Succeeded or Failed"
    Aug 25 03:36:46.158: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:36:46.164
    Aug 25 03:36:46.170: INFO: Waiting for pod downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7 to disappear
    Aug 25 03:36:46.174: INFO: Pod downwardapi-volume-a92593ce-a2de-456d-aa61-3bc65f2cb3a7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:36:46.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5329" for this suite. 08/25/22 03:36:46.178
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:46.183
Aug 25 03:36:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:36:46.185
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:46.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:46.201
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:36:46.213
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:36:46.559
STEP: Deploying the webhook pod 08/25/22 03:36:46.567
STEP: Wait for the deployment to be ready 08/25/22 03:36:46.577
Aug 25 03:36:46.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:36:48.599
STEP: Verifying the service has paired with the endpoint 08/25/22 03:36:48.608
Aug 25 03:36:49.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/25/22 03:36:49.612
STEP: create a configmap that should be updated by the webhook 08/25/22 03:36:49.633
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:36:49.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4486" for this suite. 08/25/22 03:36:49.657
STEP: Destroying namespace "webhook-4486-markers" for this suite. 08/25/22 03:36:49.662
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":152,"skipped":2804,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.515 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:46.183
    Aug 25 03:36:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:36:46.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:46.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:46.201
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:36:46.213
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:36:46.559
    STEP: Deploying the webhook pod 08/25/22 03:36:46.567
    STEP: Wait for the deployment to be ready 08/25/22 03:36:46.577
    Aug 25 03:36:46.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:36:48.599
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:36:48.608
    Aug 25 03:36:49.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/25/22 03:36:49.612
    STEP: create a configmap that should be updated by the webhook 08/25/22 03:36:49.633
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:36:49.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4486" for this suite. 08/25/22 03:36:49.657
    STEP: Destroying namespace "webhook-4486-markers" for this suite. 08/25/22 03:36:49.662
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:49.699
Aug 25 03:36:49.699: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 03:36:49.7
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:49.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:49.713
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 08/25/22 03:36:49.717
Aug 25 03:36:49.722: INFO: Waiting up to 5m0s for pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897" in namespace "var-expansion-2785" to be "Succeeded or Failed"
Aug 25 03:36:49.724: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625498ms
Aug 25 03:36:51.731: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009164006s
Aug 25 03:36:53.731: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009062268s
STEP: Saw pod success 08/25/22 03:36:53.731
Aug 25 03:36:53.731: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897" satisfied condition "Succeeded or Failed"
Aug 25 03:36:53.735: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897 container dapi-container: <nil>
STEP: delete the pod 08/25/22 03:36:53.741
Aug 25 03:36:53.747: INFO: Waiting for pod var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897 to disappear
Aug 25 03:36:53.751: INFO: Pod var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 03:36:53.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2785" for this suite. 08/25/22 03:36:53.755
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":153,"skipped":2812,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.061 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:49.699
    Aug 25 03:36:49.699: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 03:36:49.7
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:49.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:49.713
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 08/25/22 03:36:49.717
    Aug 25 03:36:49.722: INFO: Waiting up to 5m0s for pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897" in namespace "var-expansion-2785" to be "Succeeded or Failed"
    Aug 25 03:36:49.724: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625498ms
    Aug 25 03:36:51.731: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009164006s
    Aug 25 03:36:53.731: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009062268s
    STEP: Saw pod success 08/25/22 03:36:53.731
    Aug 25 03:36:53.731: INFO: Pod "var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897" satisfied condition "Succeeded or Failed"
    Aug 25 03:36:53.735: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 03:36:53.741
    Aug 25 03:36:53.747: INFO: Waiting for pod var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897 to disappear
    Aug 25 03:36:53.751: INFO: Pod var-expansion-ea39ae30-eeab-4094-9e0d-a69504942897 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 03:36:53.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2785" for this suite. 08/25/22 03:36:53.755
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:36:53.763
Aug 25 03:36:53.763: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pod-network-test 08/25/22 03:36:53.765
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:53.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:53.779
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-785 08/25/22 03:36:53.782
STEP: creating a selector 08/25/22 03:36:53.783
STEP: Creating the service pods in kubernetes 08/25/22 03:36:53.783
Aug 25 03:36:53.783: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 25 03:36:53.796: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-785" to be "running and ready"
Aug 25 03:36:53.800: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.754601ms
Aug 25 03:36:53.800: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:36:55.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00934177s
Aug 25 03:36:55.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:36:57.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009324433s
Aug 25 03:36:57.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:36:59.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009458602s
Aug 25 03:36:59.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:37:01.806: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010693819s
Aug 25 03:37:01.806: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:37:03.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008604047s
Aug 25 03:37:03.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:37:05.806: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010505231s
Aug 25 03:37:05.806: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 25 03:37:05.806: INFO: Pod "netserver-0" satisfied condition "running and ready"
STEP: Creating test pods 08/25/22 03:37:05.81
Aug 25 03:37:05.821: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-785" to be "running"
Aug 25 03:37:05.825: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587606ms
Aug 25 03:37:07.830: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008783944s
Aug 25 03:37:07.830: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 25 03:37:07.833: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-785" to be "running"
Aug 25 03:37:07.837: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.100044ms
Aug 25 03:37:07.838: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 25 03:37:07.841: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
Aug 25 03:37:07.841: INFO: Going to poll 192.168.0.10 on port 8083 at least 0 times, with a maximum of 31 tries before failing
Aug 25 03:37:07.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.0.10:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:37:07.845: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:37:07.846: INFO: ExecWithOptions: Clientset creation
Aug 25 03:37:07.846: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.0.10%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 25 03:37:07.974: INFO: Found all 1 expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 25 03:37:07.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-785" for this suite. 08/25/22 03:37:07.98
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":154,"skipped":2858,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [14.221 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:36:53.763
    Aug 25 03:36:53.763: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pod-network-test 08/25/22 03:36:53.765
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:36:53.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:36:53.779
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-785 08/25/22 03:36:53.782
    STEP: creating a selector 08/25/22 03:36:53.783
    STEP: Creating the service pods in kubernetes 08/25/22 03:36:53.783
    Aug 25 03:36:53.783: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 25 03:36:53.796: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-785" to be "running and ready"
    Aug 25 03:36:53.800: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.754601ms
    Aug 25 03:36:53.800: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:36:55.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00934177s
    Aug 25 03:36:55.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:36:57.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009324433s
    Aug 25 03:36:57.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:36:59.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009458602s
    Aug 25 03:36:59.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:37:01.806: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010693819s
    Aug 25 03:37:01.806: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:37:03.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008604047s
    Aug 25 03:37:03.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:37:05.806: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010505231s
    Aug 25 03:37:05.806: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 25 03:37:05.806: INFO: Pod "netserver-0" satisfied condition "running and ready"
    STEP: Creating test pods 08/25/22 03:37:05.81
    Aug 25 03:37:05.821: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-785" to be "running"
    Aug 25 03:37:05.825: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587606ms
    Aug 25 03:37:07.830: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008783944s
    Aug 25 03:37:07.830: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 25 03:37:07.833: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-785" to be "running"
    Aug 25 03:37:07.837: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.100044ms
    Aug 25 03:37:07.838: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 25 03:37:07.841: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
    Aug 25 03:37:07.841: INFO: Going to poll 192.168.0.10 on port 8083 at least 0 times, with a maximum of 31 tries before failing
    Aug 25 03:37:07.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.0.10:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:37:07.845: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:37:07.846: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:37:07.846: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.0.10%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 25 03:37:07.974: INFO: Found all 1 expected endpoints: [netserver-0]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 25 03:37:07.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-785" for this suite. 08/25/22 03:37:07.98
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:37:07.985
Aug 25 03:37:07.985: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-pred 08/25/22 03:37:07.987
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:37:08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:37:08.004
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 25 03:37:08.007: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 25 03:37:08.012: INFO: Waiting for terminating namespaces to be deleted...
Aug 25 03:37:08.015: INFO: 
Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
Aug 25 03:37:08.034: INFO: environment-0 from bobymcbobs started at 2022-08-25 03:25:19 +0000 UTC (2 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container environment ready: true, restart count 0
Aug 25 03:37:08.034: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 03:37:08.034: INFO: cert-manager-66bd77df8f-qlhqg from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 03:37:08.034: INFO: cert-manager-cainjector-6495667ff4-x6lll from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 03:37:08.034: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 from cert-manager started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 03:37:08.034: INFO: contour-588fc6cc6d-dq6wf from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:37:08.034: INFO: contour-588fc6cc6d-f66zz from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:37:08.034: INFO: envoy-cxlbl from contour-external started at 2022-08-25 03:25:39 +0000 UTC (2 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container envoy ready: true, restart count 0
Aug 25 03:37:08.034: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 03:37:08.034: INFO: contour-8597b78798-5755z from contour-internal started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:37:08.034: INFO: contour-8597b78798-lb4wb from contour-internal started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
Aug 25 03:37:08.034: INFO: envoy-7sxnc from contour-internal started at 2022-08-25 03:25:38 +0000 UTC (2 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container envoy ready: true, restart count 0
Aug 25 03:37:08.034: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 03:37:08.034: INFO: external-dns-67d79cbcd4-bf29h from external-dns started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 03:37:08.034: INFO: helm-operator-7959478576-6mcjz from helm-operator started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 03:37:08.034: INFO: knative-operator-594876444d-qzjs4 from knative-operator started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 03:37:08.034: INFO: activator-88b7df5c-cbxk8 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container activator ready: true, restart count 0
Aug 25 03:37:08.034: INFO: autoscaler-7bf8ff94db-98bhj from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 03:37:08.034: INFO: autoscaler-hpa-776c44cc57-rcqj4 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 03:37:08.034: INFO: controller-7fd644cbc6-dfb6n from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:37:08.034: INFO: domain-mapping-5bcd85fbc6-2gshd from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 03:37:08.034: INFO: domainmapping-webhook-77f466bbc9-97ft7 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 03:37:08.034: INFO: net-certmanager-controller-6c8cb88879-86vmg from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:37:08.034: INFO: net-certmanager-webhook-79cfb96f68-kfnlr from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container webhook ready: true, restart count 0
Aug 25 03:37:08.034: INFO: net-contour-controller-5c5fd89fdb-lnf94 from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:37:08.034: INFO: webhook-69d55f5549-ghp24 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container webhook ready: true, restart count 0
Aug 25 03:37:08.034: INFO: coredns-565d847f94-684lz from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container coredns ready: true, restart count 0
Aug 25 03:37:08.034: INFO: coredns-565d847f94-lzgv5 from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container coredns ready: true, restart count 0
Aug 25 03:37:08.034: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container etcd ready: true, restart count 0
Aug 25 03:37:08.034: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 03:37:08.034: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 03:37:08.034: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 03:37:08.034: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 03:37:08.034: INFO: kubed-568bdbc6c4-vx7m6 from kube-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container kubed ready: true, restart count 0
Aug 25 03:37:08.034: INFO: metrics-server-5cb46dccf6-w9d6b from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 03:37:08.034: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container weave ready: true, restart count 1
Aug 25 03:37:08.034: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 03:37:08.034: INFO: local-path-provisioner-56c55476f9-xbkr9 from local-path-storage started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 03:37:08.034: INFO: controller-77cc7ff558-mrwjl from metallb-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
Aug 25 03:37:08.035: INFO: speaker-f82r7 from metallb-system started at 2022-08-25 03:25:09 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container speaker ready: true, restart count 0
Aug 25 03:37:08.035: INFO: distribution-7f9dff85c4-c276b from pair-system started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container distribution ready: true, restart count 0
Aug 25 03:37:08.035: INFO: environment-exposer-5bb6d44465-8btrx from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 03:37:08.035: INFO: powerdns-5c6dbcf579-rgzxs from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 03:37:08.035: INFO: powerdns-db-7c897fddf4-l5cjb from pair-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 03:37:08.035: INFO: public-html-go-http-server-55b9f584d4-4jrp5 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 03:37:08.035: INFO: reveal-multiplex-6bbbf59d6-k9lt8 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 03:37:08.035: INFO: host-test-container-pod from pod-network-test-785 started at 2022-08-25 03:37:05 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 25 03:37:08.035: INFO: netserver-0 from pod-network-test-785 started at 2022-08-25 03:36:53 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container webserver ready: true, restart count 0
Aug 25 03:37:08.035: INFO: test-container-pod from pod-network-test-785 started at 2022-08-25 03:37:05 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container webserver ready: true, restart count 0
Aug 25 03:37:08.035: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 03:37:08.035: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container e2e ready: true, restart count 0
Aug 25 03:37:08.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 03:37:08.035: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 03:37:08.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 03:37:08.035: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/25/22 03:37:08.035
Aug 25 03:37:08.042: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2710" to be "running"
Aug 25 03:37:08.045: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00408ms
Aug 25 03:37:10.051: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008582159s
Aug 25 03:37:10.051: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/25/22 03:37:10.055
STEP: Trying to apply a random label on the found node. 08/25/22 03:37:10.066
STEP: verifying the node has the label kubernetes.io/e2e-9259f237-a71f-4934-b272-dcbc27a911e8 95 08/25/22 03:37:10.076
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/25/22 03:37:10.08
Aug 25 03:37:10.086: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2710" to be "not pending"
Aug 25 03:37:10.089: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357046ms
Aug 25 03:37:12.096: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010075948s
Aug 25 03:37:12.096: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 86.109.11.217 on the node which pod4 resides and expect not scheduled 08/25/22 03:37:12.096
Aug 25 03:37:12.102: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2710" to be "not pending"
Aug 25 03:37:12.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905043ms
Aug 25 03:37:14.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009449676s
Aug 25 03:37:16.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008922547s
Aug 25 03:37:18.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008445241s
Aug 25 03:37:20.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009762644s
Aug 25 03:37:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010457106s
Aug 25 03:37:24.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00995538s
Aug 25 03:37:26.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008417287s
Aug 25 03:37:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009657383s
Aug 25 03:37:30.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00813748s
Aug 25 03:37:32.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009643378s
Aug 25 03:37:34.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010387883s
Aug 25 03:37:36.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009018907s
Aug 25 03:37:38.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008666253s
Aug 25 03:37:40.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009257991s
Aug 25 03:37:42.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010266759s
Aug 25 03:37:44.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009470805s
Aug 25 03:37:46.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008761801s
Aug 25 03:37:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01009402s
Aug 25 03:37:50.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009970729s
Aug 25 03:37:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010036228s
Aug 25 03:37:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010263096s
Aug 25 03:37:56.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008408865s
Aug 25 03:37:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009797651s
Aug 25 03:38:00.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00796578s
Aug 25 03:38:02.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009612691s
Aug 25 03:38:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01021733s
Aug 25 03:38:06.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008887924s
Aug 25 03:38:08.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010870966s
Aug 25 03:38:10.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010308241s
Aug 25 03:38:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009441949s
Aug 25 03:38:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009713445s
Aug 25 03:38:16.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008429076s
Aug 25 03:38:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009692172s
Aug 25 03:38:20.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01073131s
Aug 25 03:38:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009743241s
Aug 25 03:38:24.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009935913s
Aug 25 03:38:26.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008744586s
Aug 25 03:38:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010020516s
Aug 25 03:38:30.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008793532s
Aug 25 03:38:32.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009761725s
Aug 25 03:38:34.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008447189s
Aug 25 03:38:36.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009004701s
Aug 25 03:38:38.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008465867s
Aug 25 03:38:40.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009841404s
Aug 25 03:38:42.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008901786s
Aug 25 03:38:44.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010299844s
Aug 25 03:38:46.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008298738s
Aug 25 03:38:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009803614s
Aug 25 03:38:50.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010131847s
Aug 25 03:38:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010476708s
Aug 25 03:38:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009727029s
Aug 25 03:38:56.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009291448s
Aug 25 03:38:58.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009440119s
Aug 25 03:39:00.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00945239s
Aug 25 03:39:02.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009336241s
Aug 25 03:39:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009988711s
Aug 25 03:39:06.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008558956s
Aug 25 03:39:08.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009816544s
Aug 25 03:39:10.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010015143s
Aug 25 03:39:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009338336s
Aug 25 03:39:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009614039s
Aug 25 03:39:16.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008646764s
Aug 25 03:39:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009951679s
Aug 25 03:39:20.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.01018359s
Aug 25 03:39:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009942633s
Aug 25 03:39:24.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009148949s
Aug 25 03:39:26.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008706744s
Aug 25 03:39:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010223869s
Aug 25 03:39:30.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008957117s
Aug 25 03:39:32.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008249247s
Aug 25 03:39:34.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009126941s
Aug 25 03:39:36.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.021827794s
Aug 25 03:39:38.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010054321s
Aug 25 03:39:40.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010107679s
Aug 25 03:39:42.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.009273481s
Aug 25 03:39:44.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009729983s
Aug 25 03:39:46.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008720014s
Aug 25 03:39:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010004837s
Aug 25 03:39:50.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.010615207s
Aug 25 03:39:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009776333s
Aug 25 03:39:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.010022113s
Aug 25 03:39:56.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.0082827s
Aug 25 03:39:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009604787s
Aug 25 03:40:00.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009074502s
Aug 25 03:40:02.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009833806s
Aug 25 03:40:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010063461s
Aug 25 03:40:06.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008413393s
Aug 25 03:40:08.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009510169s
Aug 25 03:40:10.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010557812s
Aug 25 03:40:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009369651s
Aug 25 03:40:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009943497s
Aug 25 03:40:16.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008463983s
Aug 25 03:40:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009967205s
Aug 25 03:40:20.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.023670807s
Aug 25 03:40:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009690913s
Aug 25 03:40:24.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010008224s
Aug 25 03:40:26.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009709207s
Aug 25 03:40:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009685793s
Aug 25 03:40:30.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008498359s
Aug 25 03:40:32.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008847933s
Aug 25 03:40:34.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009928126s
Aug 25 03:40:36.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008303732s
Aug 25 03:40:38.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009514982s
Aug 25 03:40:40.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009892324s
Aug 25 03:40:42.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009240765s
Aug 25 03:40:44.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010017909s
Aug 25 03:40:46.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008816678s
Aug 25 03:40:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010276668s
Aug 25 03:40:50.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009383227s
Aug 25 03:40:52.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00936384s
Aug 25 03:40:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010100556s
Aug 25 03:40:56.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007876579s
Aug 25 03:40:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010282914s
Aug 25 03:41:00.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008890771s
Aug 25 03:41:02.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009018496s
Aug 25 03:41:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.010069678s
Aug 25 03:41:06.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00924669s
Aug 25 03:41:08.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009487741s
Aug 25 03:41:10.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009851738s
Aug 25 03:41:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009368928s
Aug 25 03:41:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009701386s
Aug 25 03:41:16.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009217545s
Aug 25 03:41:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009667704s
Aug 25 03:41:20.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009994484s
Aug 25 03:41:22.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009240732s
Aug 25 03:41:24.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009125562s
Aug 25 03:41:26.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008623898s
Aug 25 03:41:28.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009371811s
Aug 25 03:41:30.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008952277s
Aug 25 03:41:32.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.010323517s
Aug 25 03:41:34.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00966263s
Aug 25 03:41:36.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009026951s
Aug 25 03:41:38.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010133172s
Aug 25 03:41:40.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008634715s
Aug 25 03:41:42.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008090028s
Aug 25 03:41:44.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009209749s
Aug 25 03:41:46.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008446058s
Aug 25 03:41:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009555399s
Aug 25 03:41:50.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010290795s
Aug 25 03:41:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010056578s
Aug 25 03:41:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.010134498s
Aug 25 03:41:56.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008622606s
Aug 25 03:41:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009741203s
Aug 25 03:42:00.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008419122s
Aug 25 03:42:02.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008734614s
Aug 25 03:42:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009912526s
Aug 25 03:42:06.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008268483s
Aug 25 03:42:08.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009298004s
Aug 25 03:42:10.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009485031s
Aug 25 03:42:12.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010088159s
Aug 25 03:42:12.115: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013348353s
STEP: removing the label kubernetes.io/e2e-9259f237-a71f-4934-b272-dcbc27a911e8 off the node bobymcbobs-c849-control-plane-p55pp 08/25/22 03:42:12.115
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9259f237-a71f-4934-b272-dcbc27a911e8 08/25/22 03:42:12.131
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:42:12.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2710" for this suite. 08/25/22 03:42:12.14
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":155,"skipped":2866,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [304.159 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:37:07.985
    Aug 25 03:37:07.985: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-pred 08/25/22 03:37:07.987
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:37:08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:37:08.004
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 25 03:37:08.007: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 25 03:37:08.012: INFO: Waiting for terminating namespaces to be deleted...
    Aug 25 03:37:08.015: INFO: 
    Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
    Aug 25 03:37:08.034: INFO: environment-0 from bobymcbobs started at 2022-08-25 03:25:19 +0000 UTC (2 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container environment ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: cert-manager-66bd77df8f-qlhqg from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: cert-manager-cainjector-6495667ff4-x6lll from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 from cert-manager started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: contour-588fc6cc6d-dq6wf from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: contour-588fc6cc6d-f66zz from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: envoy-cxlbl from contour-external started at 2022-08-25 03:25:39 +0000 UTC (2 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: contour-8597b78798-5755z from contour-internal started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: contour-8597b78798-lb4wb from contour-internal started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container contour ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: envoy-7sxnc from contour-internal started at 2022-08-25 03:25:38 +0000 UTC (2 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: external-dns-67d79cbcd4-bf29h from external-dns started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: helm-operator-7959478576-6mcjz from helm-operator started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: knative-operator-594876444d-qzjs4 from knative-operator started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: activator-88b7df5c-cbxk8 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container activator ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: autoscaler-7bf8ff94db-98bhj from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: autoscaler-hpa-776c44cc57-rcqj4 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: controller-7fd644cbc6-dfb6n from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: domain-mapping-5bcd85fbc6-2gshd from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: domainmapping-webhook-77f466bbc9-97ft7 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: net-certmanager-controller-6c8cb88879-86vmg from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: net-certmanager-webhook-79cfb96f68-kfnlr from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: net-contour-controller-5c5fd89fdb-lnf94 from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: webhook-69d55f5549-ghp24 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: coredns-565d847f94-684lz from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: coredns-565d847f94-lzgv5 from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: kubed-568bdbc6c4-vx7m6 from kube-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: metrics-server-5cb46dccf6-w9d6b from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container weave ready: true, restart count 1
    Aug 25 03:37:08.034: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: local-path-provisioner-56c55476f9-xbkr9 from local-path-storage started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 03:37:08.034: INFO: controller-77cc7ff558-mrwjl from metallb-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.034: INFO: 	Container controller ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: speaker-f82r7 from metallb-system started at 2022-08-25 03:25:09 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: distribution-7f9dff85c4-c276b from pair-system started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: environment-exposer-5bb6d44465-8btrx from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: powerdns-5c6dbcf579-rgzxs from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: powerdns-db-7c897fddf4-l5cjb from pair-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: public-html-go-http-server-55b9f584d4-4jrp5 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: reveal-multiplex-6bbbf59d6-k9lt8 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: host-test-container-pod from pod-network-test-785 started at 2022-08-25 03:37:05 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container agnhost-container ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: netserver-0 from pod-network-test-785 started at 2022-08-25 03:36:53 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container webserver ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: test-container-pod from pod-network-test-785 started at 2022-08-25 03:37:05 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container webserver ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 03:37:08.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 03:37:08.035: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/25/22 03:37:08.035
    Aug 25 03:37:08.042: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2710" to be "running"
    Aug 25 03:37:08.045: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00408ms
    Aug 25 03:37:10.051: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008582159s
    Aug 25 03:37:10.051: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/25/22 03:37:10.055
    STEP: Trying to apply a random label on the found node. 08/25/22 03:37:10.066
    STEP: verifying the node has the label kubernetes.io/e2e-9259f237-a71f-4934-b272-dcbc27a911e8 95 08/25/22 03:37:10.076
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/25/22 03:37:10.08
    Aug 25 03:37:10.086: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2710" to be "not pending"
    Aug 25 03:37:10.089: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357046ms
    Aug 25 03:37:12.096: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010075948s
    Aug 25 03:37:12.096: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 86.109.11.217 on the node which pod4 resides and expect not scheduled 08/25/22 03:37:12.096
    Aug 25 03:37:12.102: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2710" to be "not pending"
    Aug 25 03:37:12.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905043ms
    Aug 25 03:37:14.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009449676s
    Aug 25 03:37:16.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008922547s
    Aug 25 03:37:18.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008445241s
    Aug 25 03:37:20.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009762644s
    Aug 25 03:37:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010457106s
    Aug 25 03:37:24.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00995538s
    Aug 25 03:37:26.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008417287s
    Aug 25 03:37:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009657383s
    Aug 25 03:37:30.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00813748s
    Aug 25 03:37:32.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009643378s
    Aug 25 03:37:34.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010387883s
    Aug 25 03:37:36.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009018907s
    Aug 25 03:37:38.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008666253s
    Aug 25 03:37:40.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009257991s
    Aug 25 03:37:42.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010266759s
    Aug 25 03:37:44.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009470805s
    Aug 25 03:37:46.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008761801s
    Aug 25 03:37:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01009402s
    Aug 25 03:37:50.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009970729s
    Aug 25 03:37:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010036228s
    Aug 25 03:37:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010263096s
    Aug 25 03:37:56.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008408865s
    Aug 25 03:37:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009797651s
    Aug 25 03:38:00.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00796578s
    Aug 25 03:38:02.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009612691s
    Aug 25 03:38:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01021733s
    Aug 25 03:38:06.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008887924s
    Aug 25 03:38:08.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010870966s
    Aug 25 03:38:10.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010308241s
    Aug 25 03:38:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009441949s
    Aug 25 03:38:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009713445s
    Aug 25 03:38:16.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008429076s
    Aug 25 03:38:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009692172s
    Aug 25 03:38:20.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01073131s
    Aug 25 03:38:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009743241s
    Aug 25 03:38:24.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009935913s
    Aug 25 03:38:26.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008744586s
    Aug 25 03:38:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010020516s
    Aug 25 03:38:30.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008793532s
    Aug 25 03:38:32.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009761725s
    Aug 25 03:38:34.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008447189s
    Aug 25 03:38:36.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009004701s
    Aug 25 03:38:38.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008465867s
    Aug 25 03:38:40.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009841404s
    Aug 25 03:38:42.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008901786s
    Aug 25 03:38:44.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010299844s
    Aug 25 03:38:46.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008298738s
    Aug 25 03:38:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009803614s
    Aug 25 03:38:50.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010131847s
    Aug 25 03:38:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010476708s
    Aug 25 03:38:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009727029s
    Aug 25 03:38:56.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009291448s
    Aug 25 03:38:58.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009440119s
    Aug 25 03:39:00.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00945239s
    Aug 25 03:39:02.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009336241s
    Aug 25 03:39:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009988711s
    Aug 25 03:39:06.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008558956s
    Aug 25 03:39:08.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009816544s
    Aug 25 03:39:10.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010015143s
    Aug 25 03:39:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009338336s
    Aug 25 03:39:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009614039s
    Aug 25 03:39:16.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008646764s
    Aug 25 03:39:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009951679s
    Aug 25 03:39:20.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.01018359s
    Aug 25 03:39:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009942633s
    Aug 25 03:39:24.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009148949s
    Aug 25 03:39:26.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008706744s
    Aug 25 03:39:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010223869s
    Aug 25 03:39:30.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008957117s
    Aug 25 03:39:32.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008249247s
    Aug 25 03:39:34.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009126941s
    Aug 25 03:39:36.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.021827794s
    Aug 25 03:39:38.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010054321s
    Aug 25 03:39:40.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010107679s
    Aug 25 03:39:42.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.009273481s
    Aug 25 03:39:44.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009729983s
    Aug 25 03:39:46.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008720014s
    Aug 25 03:39:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010004837s
    Aug 25 03:39:50.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.010615207s
    Aug 25 03:39:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009776333s
    Aug 25 03:39:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.010022113s
    Aug 25 03:39:56.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.0082827s
    Aug 25 03:39:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009604787s
    Aug 25 03:40:00.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009074502s
    Aug 25 03:40:02.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009833806s
    Aug 25 03:40:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010063461s
    Aug 25 03:40:06.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008413393s
    Aug 25 03:40:08.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009510169s
    Aug 25 03:40:10.113: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010557812s
    Aug 25 03:40:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009369651s
    Aug 25 03:40:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009943497s
    Aug 25 03:40:16.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008463983s
    Aug 25 03:40:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009967205s
    Aug 25 03:40:20.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.023670807s
    Aug 25 03:40:22.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009690913s
    Aug 25 03:40:24.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010008224s
    Aug 25 03:40:26.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009709207s
    Aug 25 03:40:28.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009685793s
    Aug 25 03:40:30.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008498359s
    Aug 25 03:40:32.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008847933s
    Aug 25 03:40:34.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009928126s
    Aug 25 03:40:36.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008303732s
    Aug 25 03:40:38.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009514982s
    Aug 25 03:40:40.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009892324s
    Aug 25 03:40:42.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009240765s
    Aug 25 03:40:44.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010017909s
    Aug 25 03:40:46.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008816678s
    Aug 25 03:40:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010276668s
    Aug 25 03:40:50.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009383227s
    Aug 25 03:40:52.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00936384s
    Aug 25 03:40:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010100556s
    Aug 25 03:40:56.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007876579s
    Aug 25 03:40:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010282914s
    Aug 25 03:41:00.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008890771s
    Aug 25 03:41:02.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009018496s
    Aug 25 03:41:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.010069678s
    Aug 25 03:41:06.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00924669s
    Aug 25 03:41:08.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009487741s
    Aug 25 03:41:10.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009851738s
    Aug 25 03:41:12.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009368928s
    Aug 25 03:41:14.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009701386s
    Aug 25 03:41:16.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009217545s
    Aug 25 03:41:18.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009667704s
    Aug 25 03:41:20.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009994484s
    Aug 25 03:41:22.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009240732s
    Aug 25 03:41:24.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009125562s
    Aug 25 03:41:26.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008623898s
    Aug 25 03:41:28.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009371811s
    Aug 25 03:41:30.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008952277s
    Aug 25 03:41:32.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.010323517s
    Aug 25 03:41:34.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00966263s
    Aug 25 03:41:36.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009026951s
    Aug 25 03:41:38.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010133172s
    Aug 25 03:41:40.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008634715s
    Aug 25 03:41:42.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008090028s
    Aug 25 03:41:44.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009209749s
    Aug 25 03:41:46.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008446058s
    Aug 25 03:41:48.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009555399s
    Aug 25 03:41:50.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010290795s
    Aug 25 03:41:52.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010056578s
    Aug 25 03:41:54.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.010134498s
    Aug 25 03:41:56.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008622606s
    Aug 25 03:41:58.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009741203s
    Aug 25 03:42:00.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008419122s
    Aug 25 03:42:02.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008734614s
    Aug 25 03:42:04.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009912526s
    Aug 25 03:42:06.110: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008268483s
    Aug 25 03:42:08.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009298004s
    Aug 25 03:42:10.111: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009485031s
    Aug 25 03:42:12.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010088159s
    Aug 25 03:42:12.115: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013348353s
    STEP: removing the label kubernetes.io/e2e-9259f237-a71f-4934-b272-dcbc27a911e8 off the node bobymcbobs-c849-control-plane-p55pp 08/25/22 03:42:12.115
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9259f237-a71f-4934-b272-dcbc27a911e8 08/25/22 03:42:12.131
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:42:12.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2710" for this suite. 08/25/22 03:42:12.14
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:42:12.146
Aug 25 03:42:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 03:42:12.147
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:42:12.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:42:12.164
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-68824938-7104-4b1c-969a-20f068db808f in namespace container-probe-5902 08/25/22 03:42:12.169
Aug 25 03:42:12.175: INFO: Waiting up to 5m0s for pod "liveness-68824938-7104-4b1c-969a-20f068db808f" in namespace "container-probe-5902" to be "not pending"
Aug 25 03:42:12.178: INFO: Pod "liveness-68824938-7104-4b1c-969a-20f068db808f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135902ms
Aug 25 03:42:14.184: INFO: Pod "liveness-68824938-7104-4b1c-969a-20f068db808f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008814271s
Aug 25 03:42:14.184: INFO: Pod "liveness-68824938-7104-4b1c-969a-20f068db808f" satisfied condition "not pending"
Aug 25 03:42:14.184: INFO: Started pod liveness-68824938-7104-4b1c-969a-20f068db808f in namespace container-probe-5902
STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:42:14.184
Aug 25 03:42:14.189: INFO: Initial restart count of pod liveness-68824938-7104-4b1c-969a-20f068db808f is 0
Aug 25 03:42:34.250: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 1 (20.061791141s elapsed)
Aug 25 03:42:54.307: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 2 (40.118507063s elapsed)
Aug 25 03:43:14.356: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 3 (1m0.167386522s elapsed)
Aug 25 03:43:34.408: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 4 (1m20.219930023s elapsed)
Aug 25 03:44:46.603: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 5 (2m32.414405272s elapsed)
STEP: deleting the pod 08/25/22 03:44:46.603
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 03:44:46.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5902" for this suite. 08/25/22 03:44:46.617
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":156,"skipped":2876,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [154.475 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:42:12.146
    Aug 25 03:42:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 03:42:12.147
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:42:12.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:42:12.164
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-68824938-7104-4b1c-969a-20f068db808f in namespace container-probe-5902 08/25/22 03:42:12.169
    Aug 25 03:42:12.175: INFO: Waiting up to 5m0s for pod "liveness-68824938-7104-4b1c-969a-20f068db808f" in namespace "container-probe-5902" to be "not pending"
    Aug 25 03:42:12.178: INFO: Pod "liveness-68824938-7104-4b1c-969a-20f068db808f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135902ms
    Aug 25 03:42:14.184: INFO: Pod "liveness-68824938-7104-4b1c-969a-20f068db808f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008814271s
    Aug 25 03:42:14.184: INFO: Pod "liveness-68824938-7104-4b1c-969a-20f068db808f" satisfied condition "not pending"
    Aug 25 03:42:14.184: INFO: Started pod liveness-68824938-7104-4b1c-969a-20f068db808f in namespace container-probe-5902
    STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 03:42:14.184
    Aug 25 03:42:14.189: INFO: Initial restart count of pod liveness-68824938-7104-4b1c-969a-20f068db808f is 0
    Aug 25 03:42:34.250: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 1 (20.061791141s elapsed)
    Aug 25 03:42:54.307: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 2 (40.118507063s elapsed)
    Aug 25 03:43:14.356: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 3 (1m0.167386522s elapsed)
    Aug 25 03:43:34.408: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 4 (1m20.219930023s elapsed)
    Aug 25 03:44:46.603: INFO: Restart count of pod container-probe-5902/liveness-68824938-7104-4b1c-969a-20f068db808f is now 5 (2m32.414405272s elapsed)
    STEP: deleting the pod 08/25/22 03:44:46.603
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 03:44:46.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5902" for this suite. 08/25/22 03:44:46.617
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:44:46.622
Aug 25 03:44:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 03:44:46.624
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:46.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:46.64
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 08/25/22 03:44:46.647
Aug 25 03:44:46.654: INFO: Waiting up to 5m0s for pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f" in namespace "emptydir-3570" to be "Succeeded or Failed"
Aug 25 03:44:46.657: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896104ms
Aug 25 03:44:48.661: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007480374s
Aug 25 03:44:50.662: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00852055s
STEP: Saw pod success 08/25/22 03:44:50.662
Aug 25 03:44:50.663: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f" satisfied condition "Succeeded or Failed"
Aug 25 03:44:50.667: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-bd1a2545-402b-44fe-9b0d-52863eabd44f container test-container: <nil>
STEP: delete the pod 08/25/22 03:44:50.687
Aug 25 03:44:50.694: INFO: Waiting for pod pod-bd1a2545-402b-44fe-9b0d-52863eabd44f to disappear
Aug 25 03:44:50.697: INFO: Pod pod-bd1a2545-402b-44fe-9b0d-52863eabd44f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 03:44:50.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3570" for this suite. 08/25/22 03:44:50.701
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":157,"skipped":2888,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:44:46.622
    Aug 25 03:44:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 03:44:46.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:46.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:46.64
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/25/22 03:44:46.647
    Aug 25 03:44:46.654: INFO: Waiting up to 5m0s for pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f" in namespace "emptydir-3570" to be "Succeeded or Failed"
    Aug 25 03:44:46.657: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896104ms
    Aug 25 03:44:48.661: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007480374s
    Aug 25 03:44:50.662: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00852055s
    STEP: Saw pod success 08/25/22 03:44:50.662
    Aug 25 03:44:50.663: INFO: Pod "pod-bd1a2545-402b-44fe-9b0d-52863eabd44f" satisfied condition "Succeeded or Failed"
    Aug 25 03:44:50.667: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-bd1a2545-402b-44fe-9b0d-52863eabd44f container test-container: <nil>
    STEP: delete the pod 08/25/22 03:44:50.687
    Aug 25 03:44:50.694: INFO: Waiting for pod pod-bd1a2545-402b-44fe-9b0d-52863eabd44f to disappear
    Aug 25 03:44:50.697: INFO: Pod pod-bd1a2545-402b-44fe-9b0d-52863eabd44f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 03:44:50.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3570" for this suite. 08/25/22 03:44:50.701
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:44:50.709
Aug 25 03:44:50.709: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-runtime 08/25/22 03:44:50.71
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:50.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:50.727
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 08/25/22 03:44:50.731
STEP: wait for the container to reach Succeeded 08/25/22 03:44:50.737
STEP: get the container status 08/25/22 03:44:54.759
STEP: the container should be terminated 08/25/22 03:44:54.763
STEP: the termination message should be set 08/25/22 03:44:54.763
Aug 25 03:44:54.763: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/25/22 03:44:54.763
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 25 03:44:54.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-301" for this suite. 08/25/22 03:44:54.779
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":158,"skipped":2925,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:44:50.709
    Aug 25 03:44:50.709: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-runtime 08/25/22 03:44:50.71
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:50.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:50.727
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 08/25/22 03:44:50.731
    STEP: wait for the container to reach Succeeded 08/25/22 03:44:50.737
    STEP: get the container status 08/25/22 03:44:54.759
    STEP: the container should be terminated 08/25/22 03:44:54.763
    STEP: the termination message should be set 08/25/22 03:44:54.763
    Aug 25 03:44:54.763: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/25/22 03:44:54.763
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 25 03:44:54.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-301" for this suite. 08/25/22 03:44:54.779
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:44:54.784
Aug 25 03:44:54.784: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 03:44:54.785
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:54.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:54.801
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6729.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6729.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/25/22 03:44:54.805
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6729.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6729.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/25/22 03:44:54.805
STEP: creating a pod to probe /etc/hosts 08/25/22 03:44:54.805
STEP: submitting the pod to kubernetes 08/25/22 03:44:54.805
Aug 25 03:44:54.811: INFO: Waiting up to 15m0s for pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91" in namespace "dns-6729" to be "running"
Aug 25 03:44:54.815: INFO: Pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818711ms
Aug 25 03:44:56.820: INFO: Pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91": Phase="Running", Reason="", readiness=true. Elapsed: 2.008567856s
Aug 25 03:44:56.820: INFO: Pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91" satisfied condition "running"
STEP: retrieving the pod 08/25/22 03:44:56.82
STEP: looking for the results for each expected name from probers 08/25/22 03:44:56.823
Aug 25 03:44:56.837: INFO: DNS probes using dns-6729/dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91 succeeded

STEP: deleting the pod 08/25/22 03:44:56.837
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 03:44:56.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6729" for this suite. 08/25/22 03:44:56.849
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":159,"skipped":2945,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.069 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:44:54.784
    Aug 25 03:44:54.784: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 03:44:54.785
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:54.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:54.801
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6729.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6729.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/25/22 03:44:54.805
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6729.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6729.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/25/22 03:44:54.805
    STEP: creating a pod to probe /etc/hosts 08/25/22 03:44:54.805
    STEP: submitting the pod to kubernetes 08/25/22 03:44:54.805
    Aug 25 03:44:54.811: INFO: Waiting up to 15m0s for pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91" in namespace "dns-6729" to be "running"
    Aug 25 03:44:54.815: INFO: Pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818711ms
    Aug 25 03:44:56.820: INFO: Pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91": Phase="Running", Reason="", readiness=true. Elapsed: 2.008567856s
    Aug 25 03:44:56.820: INFO: Pod "dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 03:44:56.82
    STEP: looking for the results for each expected name from probers 08/25/22 03:44:56.823
    Aug 25 03:44:56.837: INFO: DNS probes using dns-6729/dns-test-b722c41c-4f96-477b-ae6c-868144ab6e91 succeeded

    STEP: deleting the pod 08/25/22 03:44:56.837
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 03:44:56.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6729" for this suite. 08/25/22 03:44:56.849
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:44:56.854
Aug 25 03:44:56.854: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename subpath 08/25/22 03:44:56.856
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:56.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:56.873
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/25/22 03:44:56.879
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-hw7z 08/25/22 03:44:56.886
STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:44:56.886
Aug 25 03:44:56.892: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hw7z" in namespace "subpath-5926" to be "Succeeded or Failed"
Aug 25 03:44:56.895: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607516ms
Aug 25 03:44:58.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063844s
Aug 25 03:45:00.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 4.009065448s
Aug 25 03:45:02.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 6.008750568s
Aug 25 03:45:04.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 8.007257435s
Aug 25 03:45:06.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 10.007390173s
Aug 25 03:45:08.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.008798551s
Aug 25 03:45:10.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 14.007378243s
Aug 25 03:45:12.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 16.008558524s
Aug 25 03:45:14.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 18.00874759s
Aug 25 03:45:16.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 20.008525975s
Aug 25 03:45:18.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=false. Elapsed: 22.008774087s
Aug 25 03:45:20.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007615214s
STEP: Saw pod success 08/25/22 03:45:20.9
Aug 25 03:45:20.900: INFO: Pod "pod-subpath-test-projected-hw7z" satisfied condition "Succeeded or Failed"
Aug 25 03:45:20.904: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-projected-hw7z container test-container-subpath-projected-hw7z: <nil>
STEP: delete the pod 08/25/22 03:45:20.91
Aug 25 03:45:20.917: INFO: Waiting for pod pod-subpath-test-projected-hw7z to disappear
Aug 25 03:45:20.920: INFO: Pod pod-subpath-test-projected-hw7z no longer exists
STEP: Deleting pod pod-subpath-test-projected-hw7z 08/25/22 03:45:20.92
Aug 25 03:45:20.921: INFO: Deleting pod "pod-subpath-test-projected-hw7z" in namespace "subpath-5926"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 25 03:45:20.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5926" for this suite. 08/25/22 03:45:20.927
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":160,"skipped":2951,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [24.077 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:44:56.854
    Aug 25 03:44:56.854: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename subpath 08/25/22 03:44:56.856
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:44:56.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:44:56.873
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/25/22 03:44:56.879
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-hw7z 08/25/22 03:44:56.886
    STEP: Creating a pod to test atomic-volume-subpath 08/25/22 03:44:56.886
    Aug 25 03:44:56.892: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hw7z" in namespace "subpath-5926" to be "Succeeded or Failed"
    Aug 25 03:44:56.895: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607516ms
    Aug 25 03:44:58.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063844s
    Aug 25 03:45:00.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 4.009065448s
    Aug 25 03:45:02.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 6.008750568s
    Aug 25 03:45:04.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 8.007257435s
    Aug 25 03:45:06.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 10.007390173s
    Aug 25 03:45:08.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.008798551s
    Aug 25 03:45:10.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 14.007378243s
    Aug 25 03:45:12.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 16.008558524s
    Aug 25 03:45:14.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 18.00874759s
    Aug 25 03:45:16.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=true. Elapsed: 20.008525975s
    Aug 25 03:45:18.901: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Running", Reason="", readiness=false. Elapsed: 22.008774087s
    Aug 25 03:45:20.900: INFO: Pod "pod-subpath-test-projected-hw7z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007615214s
    STEP: Saw pod success 08/25/22 03:45:20.9
    Aug 25 03:45:20.900: INFO: Pod "pod-subpath-test-projected-hw7z" satisfied condition "Succeeded or Failed"
    Aug 25 03:45:20.904: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-projected-hw7z container test-container-subpath-projected-hw7z: <nil>
    STEP: delete the pod 08/25/22 03:45:20.91
    Aug 25 03:45:20.917: INFO: Waiting for pod pod-subpath-test-projected-hw7z to disappear
    Aug 25 03:45:20.920: INFO: Pod pod-subpath-test-projected-hw7z no longer exists
    STEP: Deleting pod pod-subpath-test-projected-hw7z 08/25/22 03:45:20.92
    Aug 25 03:45:20.921: INFO: Deleting pod "pod-subpath-test-projected-hw7z" in namespace "subpath-5926"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 25 03:45:20.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5926" for this suite. 08/25/22 03:45:20.927
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:45:20.934
Aug 25 03:45:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:45:20.935
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:20.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:20.948
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:45:20.962
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:45:21.248
STEP: Deploying the webhook pod 08/25/22 03:45:21.256
STEP: Wait for the deployment to be ready 08/25/22 03:45:21.267
Aug 25 03:45:21.274: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:45:23.287
STEP: Verifying the service has paired with the endpoint 08/25/22 03:45:23.297
Aug 25 03:45:24.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 08/25/22 03:45:24.301
STEP: Creating a custom resource definition that should be denied by the webhook 08/25/22 03:45:24.319
Aug 25 03:45:24.319: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:45:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2719" for this suite. 08/25/22 03:45:24.347
STEP: Destroying namespace "webhook-2719-markers" for this suite. 08/25/22 03:45:24.352
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":161,"skipped":3022,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.447 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:45:20.934
    Aug 25 03:45:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:45:20.935
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:20.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:20.948
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:45:20.962
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:45:21.248
    STEP: Deploying the webhook pod 08/25/22 03:45:21.256
    STEP: Wait for the deployment to be ready 08/25/22 03:45:21.267
    Aug 25 03:45:21.274: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:45:23.287
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:45:23.297
    Aug 25 03:45:24.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/25/22 03:45:24.301
    STEP: Creating a custom resource definition that should be denied by the webhook 08/25/22 03:45:24.319
    Aug 25 03:45:24.319: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:45:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2719" for this suite. 08/25/22 03:45:24.347
    STEP: Destroying namespace "webhook-2719-markers" for this suite. 08/25/22 03:45:24.352
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:45:24.381
Aug 25 03:45:24.381: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename proxy 08/25/22 03:45:24.382
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:24.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:24.397
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/25/22 03:45:24.406
STEP: creating replication controller proxy-service-xtrnb in namespace proxy-6006 08/25/22 03:45:24.406
I0825 03:45:24.409917      22 runners.go:193] Created replication controller with name: proxy-service-xtrnb, namespace: proxy-6006, replica count: 1
I0825 03:45:25.461564      22 runners.go:193] proxy-service-xtrnb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0825 03:45:26.461815      22 runners.go:193] proxy-service-xtrnb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 03:45:26.465: INFO: setup took 2.065478024s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/25/22 03:45:26.465
Aug 25 03:45:26.470: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.865023ms)
Aug 25 03:45:26.472: INFO: (0) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 5.662337ms)
Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 7.04798ms)
Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 7.058972ms)
Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 7.470157ms)
Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 7.148325ms)
Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 7.604262ms)
Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 7.215368ms)
Aug 25 03:45:26.474: INFO: (0) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 8.060498ms)
Aug 25 03:45:26.474: INFO: (0) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 8.333929ms)
Aug 25 03:45:26.474: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 8.487442ms)
Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 14.906266ms)
Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 14.855666ms)
Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 15.418105ms)
Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 15.13488ms)
Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 15.527964ms)
Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.215907ms)
Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.254236ms)
Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.284655ms)
Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.417041ms)
Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.450979ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.956163ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.323421ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.261385ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.581395ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 4.553877ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.742585ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 4.716986ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 4.719586ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.729337ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.854984ms)
Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.745352ms)
Aug 25 03:45:26.490: INFO: (2) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.01562ms)
Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.134544ms)
Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 4.190036ms)
Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.326561ms)
Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.331425ms)
Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.931478ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 5.194079ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 5.182971ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 5.320659ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 5.376097ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 5.336859ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 5.389751ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 5.41229ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 5.491535ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 5.914867ms)
Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 5.944658ms)
Aug 25 03:45:26.495: INFO: (3) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.796346ms)
Aug 25 03:45:26.495: INFO: (3) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 2.85607ms)
Aug 25 03:45:26.495: INFO: (3) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.853573ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 2.929477ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.180153ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.576098ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.730682ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.827202ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.902429ms)
Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.940365ms)
Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.926501ms)
Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.031228ms)
Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.982797ms)
Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 4.062569ms)
Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.207833ms)
Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.20639ms)
Aug 25 03:45:26.500: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.987709ms)
Aug 25 03:45:26.500: INFO: (4) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.268908ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.672324ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.655136ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.808987ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.711911ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.800294ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.853049ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.971577ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 4.044588ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.016445ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.047517ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 4.373134ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.25983ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 4.279547ms)
Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.355479ms)
Aug 25 03:45:26.504: INFO: (5) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.807072ms)
Aug 25 03:45:26.504: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.99327ms)
Aug 25 03:45:26.504: INFO: (5) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.033378ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.430342ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.488886ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.543741ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.648301ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.73847ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.980136ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.979826ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.093118ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.037525ms)
Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.189717ms)
Aug 25 03:45:26.506: INFO: (5) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 4.177815ms)
Aug 25 03:45:26.506: INFO: (5) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.215538ms)
Aug 25 03:45:26.506: INFO: (5) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.126192ms)
Aug 25 03:45:26.508: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.346921ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.071708ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.097477ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.118756ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.277821ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.428516ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.652669ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.588869ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.601272ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.74014ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.729204ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.721126ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.812009ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.871758ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.891053ms)
Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.813124ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.283588ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.27151ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.23946ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.386398ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.313501ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.732868ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.758755ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.783504ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.842303ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.863679ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.891211ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.838375ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.863496ms)
Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.849458ms)
Aug 25 03:45:26.514: INFO: (7) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.926468ms)
Aug 25 03:45:26.514: INFO: (7) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.924945ms)
Aug 25 03:45:26.515: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 1.896184ms)
Aug 25 03:45:26.516: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.612518ms)
Aug 25 03:45:26.516: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 2.766705ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.070736ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.288571ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.504494ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.555495ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.672397ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.580859ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.587848ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.69393ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.752134ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.714955ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.710628ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.812212ms)
Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.788438ms)
Aug 25 03:45:26.519: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.007639ms)
Aug 25 03:45:26.520: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 2.833259ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.042488ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.399375ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.497443ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.739243ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.72666ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.760638ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.789388ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.892408ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.868703ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.864892ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.831232ms)
Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.865157ms)
Aug 25 03:45:26.522: INFO: (9) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.065957ms)
Aug 25 03:45:26.522: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.125203ms)
Aug 25 03:45:26.524: INFO: (10) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 2.480541ms)
Aug 25 03:45:26.524: INFO: (10) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 2.636135ms)
Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.395015ms)
Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.545408ms)
Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.666773ms)
Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.721156ms)
Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.735574ms)
Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.648563ms)
Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.774992ms)
Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.886418ms)
Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.858129ms)
Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.053063ms)
Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 5.034579ms)
Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 5.202449ms)
Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 5.231926ms)
Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 5.3445ms)
Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.78843ms)
Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.859014ms)
Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.992329ms)
Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.110693ms)
Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.162324ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.259811ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.516477ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.584786ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.650279ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.734956ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.758983ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.777647ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.695472ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.748276ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.79086ms)
Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.852739ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 2.681311ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.892432ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 2.88593ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.918929ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.046576ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.114516ms)
Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.292692ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.413915ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.627532ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.606722ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.590289ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.733258ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.733029ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.818149ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.743651ms)
Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.819817ms)
Aug 25 03:45:26.537: INFO: (13) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.274764ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.503895ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.528244ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.531146ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.612862ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.766249ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.803568ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.749706ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.772403ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.778642ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.836852ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.85046ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.880419ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.888959ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.01588ms)
Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.015266ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.7723ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 2.819655ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 2.924107ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.073249ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.034356ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.045458ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.331152ms)
Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.383043ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.353907ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.578658ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.509986ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.523333ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.555136ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.636416ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.57089ms)
Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.585179ms)
Aug 25 03:45:26.545: INFO: (15) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 2.043426ms)
Aug 25 03:45:26.545: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 2.63227ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.844322ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.88591ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.06755ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.112624ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.107292ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.330675ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.414031ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.369583ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.474956ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.636741ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.646064ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.587472ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.632632ms)
Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.71428ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.021741ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.284464ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.311976ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.351739ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.359973ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.348438ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.417416ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.498726ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.535767ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.441511ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.612383ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.534622ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.59083ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.536587ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.674014ms)
Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.688297ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.598222ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.652919ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 2.781046ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.143946ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.137259ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.066413ms)
Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.175319ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.291502ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.507989ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.482905ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.47909ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.538792ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.739148ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.730137ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.849138ms)
Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.834339ms)
Aug 25 03:45:26.558: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.192666ms)
Aug 25 03:45:26.558: INFO: (18) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.210849ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 4.255609ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.452937ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 4.467122ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.522001ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 4.500641ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 4.587651ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.627799ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 4.582896ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.737595ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.733083ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.885072ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 4.814952ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.908947ms)
Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.940369ms)
Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 1.649147ms)
Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 1.994997ms)
Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.271996ms)
Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.310087ms)
Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.519629ms)
Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 2.91203ms)
Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.079213ms)
Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.128419ms)
Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.167492ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.258635ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.298968ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.276564ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.256725ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.34865ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.442574ms)
Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.435303ms)
STEP: deleting ReplicationController proxy-service-xtrnb in namespace proxy-6006, will wait for the garbage collector to delete the pods 08/25/22 03:45:26.563
Aug 25 03:45:26.621: INFO: Deleting ReplicationController proxy-service-xtrnb took: 5.201665ms
Aug 25 03:45:26.722: INFO: Terminating ReplicationController proxy-service-xtrnb pods took: 100.968495ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Aug 25 03:45:29.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6006" for this suite. 08/25/22 03:45:29.328
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":162,"skipped":3022,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.952 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:45:24.381
    Aug 25 03:45:24.381: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename proxy 08/25/22 03:45:24.382
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:24.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:24.397
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/25/22 03:45:24.406
    STEP: creating replication controller proxy-service-xtrnb in namespace proxy-6006 08/25/22 03:45:24.406
    I0825 03:45:24.409917      22 runners.go:193] Created replication controller with name: proxy-service-xtrnb, namespace: proxy-6006, replica count: 1
    I0825 03:45:25.461564      22 runners.go:193] proxy-service-xtrnb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0825 03:45:26.461815      22 runners.go:193] proxy-service-xtrnb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 03:45:26.465: INFO: setup took 2.065478024s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/25/22 03:45:26.465
    Aug 25 03:45:26.470: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.865023ms)
    Aug 25 03:45:26.472: INFO: (0) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 5.662337ms)
    Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 7.04798ms)
    Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 7.058972ms)
    Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 7.470157ms)
    Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 7.148325ms)
    Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 7.604262ms)
    Aug 25 03:45:26.473: INFO: (0) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 7.215368ms)
    Aug 25 03:45:26.474: INFO: (0) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 8.060498ms)
    Aug 25 03:45:26.474: INFO: (0) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 8.333929ms)
    Aug 25 03:45:26.474: INFO: (0) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 8.487442ms)
    Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 14.906266ms)
    Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 14.855666ms)
    Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 15.418105ms)
    Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 15.13488ms)
    Aug 25 03:45:26.481: INFO: (0) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 15.527964ms)
    Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.215907ms)
    Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.254236ms)
    Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.284655ms)
    Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.417041ms)
    Aug 25 03:45:26.485: INFO: (1) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.450979ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.956163ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.323421ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.261385ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.581395ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 4.553877ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.742585ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 4.716986ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 4.719586ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.729337ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.854984ms)
    Aug 25 03:45:26.486: INFO: (1) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.745352ms)
    Aug 25 03:45:26.490: INFO: (2) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.01562ms)
    Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.134544ms)
    Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 4.190036ms)
    Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.326561ms)
    Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.331425ms)
    Aug 25 03:45:26.491: INFO: (2) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.931478ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 5.194079ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 5.182971ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 5.320659ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 5.376097ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 5.336859ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 5.389751ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 5.41229ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 5.491535ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 5.914867ms)
    Aug 25 03:45:26.492: INFO: (2) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 5.944658ms)
    Aug 25 03:45:26.495: INFO: (3) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.796346ms)
    Aug 25 03:45:26.495: INFO: (3) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 2.85607ms)
    Aug 25 03:45:26.495: INFO: (3) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.853573ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 2.929477ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.180153ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.576098ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.730682ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.827202ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.902429ms)
    Aug 25 03:45:26.496: INFO: (3) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.940365ms)
    Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.926501ms)
    Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.031228ms)
    Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.982797ms)
    Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 4.062569ms)
    Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.207833ms)
    Aug 25 03:45:26.497: INFO: (3) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.20639ms)
    Aug 25 03:45:26.500: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.987709ms)
    Aug 25 03:45:26.500: INFO: (4) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.268908ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.672324ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.655136ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.808987ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.711911ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.800294ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.853049ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.971577ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 4.044588ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.016445ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.047517ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 4.373134ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.25983ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 4.279547ms)
    Aug 25 03:45:26.501: INFO: (4) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.355479ms)
    Aug 25 03:45:26.504: INFO: (5) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.807072ms)
    Aug 25 03:45:26.504: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.99327ms)
    Aug 25 03:45:26.504: INFO: (5) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.033378ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.430342ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.488886ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.543741ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.648301ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.73847ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.980136ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.979826ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.093118ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.037525ms)
    Aug 25 03:45:26.505: INFO: (5) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.189717ms)
    Aug 25 03:45:26.506: INFO: (5) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 4.177815ms)
    Aug 25 03:45:26.506: INFO: (5) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.215538ms)
    Aug 25 03:45:26.506: INFO: (5) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.126192ms)
    Aug 25 03:45:26.508: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.346921ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.071708ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.097477ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.118756ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.277821ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.428516ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.652669ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.588869ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.601272ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.74014ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.729204ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.721126ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.812009ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.871758ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.891053ms)
    Aug 25 03:45:26.509: INFO: (6) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.813124ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.283588ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.27151ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.23946ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.386398ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.313501ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.732868ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.758755ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.783504ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.842303ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.863679ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.891211ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.838375ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.863496ms)
    Aug 25 03:45:26.513: INFO: (7) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.849458ms)
    Aug 25 03:45:26.514: INFO: (7) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.926468ms)
    Aug 25 03:45:26.514: INFO: (7) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.924945ms)
    Aug 25 03:45:26.515: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 1.896184ms)
    Aug 25 03:45:26.516: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.612518ms)
    Aug 25 03:45:26.516: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 2.766705ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.070736ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.288571ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.504494ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.555495ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.672397ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.580859ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.587848ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.69393ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.752134ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.714955ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.710628ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.812212ms)
    Aug 25 03:45:26.517: INFO: (8) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.788438ms)
    Aug 25 03:45:26.519: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.007639ms)
    Aug 25 03:45:26.520: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 2.833259ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.042488ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.399375ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.497443ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.739243ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.72666ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.760638ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.789388ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.892408ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.868703ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.864892ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.831232ms)
    Aug 25 03:45:26.521: INFO: (9) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.865157ms)
    Aug 25 03:45:26.522: INFO: (9) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.065957ms)
    Aug 25 03:45:26.522: INFO: (9) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.125203ms)
    Aug 25 03:45:26.524: INFO: (10) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 2.480541ms)
    Aug 25 03:45:26.524: INFO: (10) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 2.636135ms)
    Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.395015ms)
    Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.545408ms)
    Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.666773ms)
    Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.721156ms)
    Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.735574ms)
    Aug 25 03:45:26.525: INFO: (10) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.648563ms)
    Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.774992ms)
    Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.886418ms)
    Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.858129ms)
    Aug 25 03:45:26.526: INFO: (10) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.053063ms)
    Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 5.034579ms)
    Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 5.202449ms)
    Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 5.231926ms)
    Aug 25 03:45:26.527: INFO: (10) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 5.3445ms)
    Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.78843ms)
    Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.859014ms)
    Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.992329ms)
    Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.110693ms)
    Aug 25 03:45:26.530: INFO: (11) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.162324ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.259811ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.516477ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.584786ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.650279ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.734956ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.758983ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.777647ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.695472ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.748276ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.79086ms)
    Aug 25 03:45:26.531: INFO: (11) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.852739ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 2.681311ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.892432ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 2.88593ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.918929ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.046576ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.114516ms)
    Aug 25 03:45:26.534: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.292692ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.413915ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.627532ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.606722ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.590289ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.733258ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.733029ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.818149ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.743651ms)
    Aug 25 03:45:26.535: INFO: (12) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.819817ms)
    Aug 25 03:45:26.537: INFO: (13) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.274764ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.503895ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.528244ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.531146ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.612862ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.766249ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.803568ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.749706ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.772403ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.778642ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.836852ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.85046ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.880419ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.888959ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.01588ms)
    Aug 25 03:45:26.539: INFO: (13) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.015266ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.7723ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 2.819655ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 2.924107ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.073249ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.034356ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.045458ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.331152ms)
    Aug 25 03:45:26.542: INFO: (14) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.383043ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.353907ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.578658ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.509986ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.523333ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.555136ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.636416ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.57089ms)
    Aug 25 03:45:26.543: INFO: (14) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.585179ms)
    Aug 25 03:45:26.545: INFO: (15) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 2.043426ms)
    Aug 25 03:45:26.545: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 2.63227ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.844322ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.88591ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.06755ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.112624ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.107292ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.330675ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.414031ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.369583ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.474956ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.636741ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.646064ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.587472ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.632632ms)
    Aug 25 03:45:26.546: INFO: (15) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.71428ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.021741ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.284464ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.311976ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.351739ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.359973ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 3.348438ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 3.417416ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.498726ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.535767ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.441511ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.612383ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.534622ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.59083ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.536587ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.674014ms)
    Aug 25 03:45:26.550: INFO: (16) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.688297ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.598222ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.652919ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 2.781046ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.143946ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.137259ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.066413ms)
    Aug 25 03:45:26.553: INFO: (17) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.175319ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.291502ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 3.507989ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 3.482905ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.47909ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.538792ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.739148ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.730137ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.849138ms)
    Aug 25 03:45:26.554: INFO: (17) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.834339ms)
    Aug 25 03:45:26.558: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.192666ms)
    Aug 25 03:45:26.558: INFO: (18) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 4.210849ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 4.255609ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 4.452937ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 4.467122ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 4.522001ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 4.500641ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 4.587651ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 4.627799ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 4.582896ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 4.737595ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 4.733083ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 4.885072ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 4.814952ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.908947ms)
    Aug 25 03:45:26.559: INFO: (18) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 4.940369ms)
    Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:443/proxy/tlsrewritem... (200; 1.649147ms)
    Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 1.994997ms)
    Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:160/proxy/: foo (200; 2.271996ms)
    Aug 25 03:45:26.561: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 2.310087ms)
    Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:460/proxy/: tls baz (200; 2.519629ms)
    Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname1/proxy/: foo (200; 2.91203ms)
    Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname1/proxy/: foo (200; 3.079213ms)
    Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5/proxy/rewriteme">test</a> (200; 3.128419ms)
    Aug 25 03:45:26.562: INFO: (19) /api/v1/namespaces/proxy-6006/services/proxy-service-xtrnb:portname2/proxy/: bar (200; 3.167492ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">test<... (200; 3.258635ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/https:proxy-service-xtrnb-5fjl5:462/proxy/: tls qux (200; 3.298968ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname2/proxy/: tls qux (200; 3.276564ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/services/https:proxy-service-xtrnb:tlsportname1/proxy/: tls baz (200; 3.256725ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:162/proxy/: bar (200; 3.34865ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/services/http:proxy-service-xtrnb:portname2/proxy/: bar (200; 3.442574ms)
    Aug 25 03:45:26.563: INFO: (19) /api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6006/pods/http:proxy-service-xtrnb-5fjl5:1080/proxy/rewriteme">... (200; 3.435303ms)
    STEP: deleting ReplicationController proxy-service-xtrnb in namespace proxy-6006, will wait for the garbage collector to delete the pods 08/25/22 03:45:26.563
    Aug 25 03:45:26.621: INFO: Deleting ReplicationController proxy-service-xtrnb took: 5.201665ms
    Aug 25 03:45:26.722: INFO: Terminating ReplicationController proxy-service-xtrnb pods took: 100.968495ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Aug 25 03:45:29.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6006" for this suite. 08/25/22 03:45:29.328
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:45:29.335
Aug 25 03:45:29.335: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:45:29.336
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:29.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:29.353
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:45:29.358
Aug 25 03:45:29.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660" in namespace "projected-307" to be "Succeeded or Failed"
Aug 25 03:45:29.388: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660": Phase="Pending", Reason="", readiness=false. Elapsed: 15.235272ms
Aug 25 03:45:31.393: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020127835s
Aug 25 03:45:33.392: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020068237s
STEP: Saw pod success 08/25/22 03:45:33.393
Aug 25 03:45:33.393: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660" satisfied condition "Succeeded or Failed"
Aug 25 03:45:33.397: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660 container client-container: <nil>
STEP: delete the pod 08/25/22 03:45:33.402
Aug 25 03:45:33.411: INFO: Waiting for pod downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660 to disappear
Aug 25 03:45:33.415: INFO: Pod downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:45:33.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-307" for this suite. 08/25/22 03:45:33.42
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":163,"skipped":3027,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:45:29.335
    Aug 25 03:45:29.335: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:45:29.336
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:29.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:29.353
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:45:29.358
    Aug 25 03:45:29.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660" in namespace "projected-307" to be "Succeeded or Failed"
    Aug 25 03:45:29.388: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660": Phase="Pending", Reason="", readiness=false. Elapsed: 15.235272ms
    Aug 25 03:45:31.393: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020127835s
    Aug 25 03:45:33.392: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020068237s
    STEP: Saw pod success 08/25/22 03:45:33.393
    Aug 25 03:45:33.393: INFO: Pod "downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660" satisfied condition "Succeeded or Failed"
    Aug 25 03:45:33.397: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:45:33.402
    Aug 25 03:45:33.411: INFO: Waiting for pod downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660 to disappear
    Aug 25 03:45:33.415: INFO: Pod downwardapi-volume-13312ab1-cbe1-4e87-92f4-9819a92b3660 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:45:33.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-307" for this suite. 08/25/22 03:45:33.42
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:45:33.426
Aug 25 03:45:33.426: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubelet-test 08/25/22 03:45:33.427
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:33.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:33.442
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 25 03:45:33.451: INFO: Waiting up to 5m0s for pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb" in namespace "kubelet-test-4987" to be "running and ready"
Aug 25 03:45:33.454: INFO: Pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712775ms
Aug 25 03:45:33.454: INFO: The phase of Pod busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:45:35.459: INFO: Pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007940293s
Aug 25 03:45:35.460: INFO: The phase of Pod busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb is Running (Ready = true)
Aug 25 03:45:35.460: INFO: Pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 25 03:45:35.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4987" for this suite. 08/25/22 03:45:35.474
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":164,"skipped":3035,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.053 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:45:33.426
    Aug 25 03:45:33.426: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubelet-test 08/25/22 03:45:33.427
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:33.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:33.442
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 25 03:45:33.451: INFO: Waiting up to 5m0s for pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb" in namespace "kubelet-test-4987" to be "running and ready"
    Aug 25 03:45:33.454: INFO: Pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712775ms
    Aug 25 03:45:33.454: INFO: The phase of Pod busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:45:35.459: INFO: Pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007940293s
    Aug 25 03:45:35.460: INFO: The phase of Pod busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb is Running (Ready = true)
    Aug 25 03:45:35.460: INFO: Pod "busybox-scheduling-028e7b36-19a2-4036-adae-0d3f823422eb" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 25 03:45:35.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4987" for this suite. 08/25/22 03:45:35.474
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:45:35.481
Aug 25 03:45:35.481: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename watch 08/25/22 03:45:35.482
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:35.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:35.5
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/25/22 03:45:35.503
STEP: creating a watch on configmaps with label B 08/25/22 03:45:35.505
STEP: creating a watch on configmaps with label A or B 08/25/22 03:45:35.506
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/25/22 03:45:35.508
Aug 25 03:45:35.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34744 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:45:35.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34744 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/25/22 03:45:35.511
Aug 25 03:45:35.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34745 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:45:35.518: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34745 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/25/22 03:45:35.518
Aug 25 03:45:35.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34746 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:45:35.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34746 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/25/22 03:45:35.526
Aug 25 03:45:35.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34747 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:45:35.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34747 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/25/22 03:45:35.53
Aug 25 03:45:35.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34748 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:45:35.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34748 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/25/22 03:45:45.534
Aug 25 03:45:45.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34828 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 03:45:45.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34828 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 25 03:45:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-511" for this suite. 08/25/22 03:45:55.546
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":165,"skipped":3061,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [20.071 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:45:35.481
    Aug 25 03:45:35.481: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename watch 08/25/22 03:45:35.482
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:35.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:35.5
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/25/22 03:45:35.503
    STEP: creating a watch on configmaps with label B 08/25/22 03:45:35.505
    STEP: creating a watch on configmaps with label A or B 08/25/22 03:45:35.506
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/25/22 03:45:35.508
    Aug 25 03:45:35.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34744 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:45:35.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34744 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/25/22 03:45:35.511
    Aug 25 03:45:35.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34745 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:45:35.518: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34745 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/25/22 03:45:35.518
    Aug 25 03:45:35.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34746 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:45:35.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34746 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/25/22 03:45:35.526
    Aug 25 03:45:35.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34747 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:45:35.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-511  2dbd14f5-5303-45b5-b677-62df6fa3448d 34747 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/25/22 03:45:35.53
    Aug 25 03:45:35.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34748 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:45:35.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34748 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/25/22 03:45:45.534
    Aug 25 03:45:45.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34828 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 03:45:45.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-511  3193f2b5-5869-40f4-ac42-0da1588318c8 34828 0 2022-08-25 03:45:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-08-25 03:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 25 03:45:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-511" for this suite. 08/25/22 03:45:55.546
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:45:55.559
Aug 25 03:45:55.559: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename prestop 08/25/22 03:45:55.56
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:55.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:55.576
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-3473 08/25/22 03:45:55.58
STEP: Waiting for pods to come up. 08/25/22 03:45:55.587
Aug 25 03:45:55.588: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3473" to be "running"
Aug 25 03:45:55.590: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434808ms
Aug 25 03:45:57.596: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007945209s
Aug 25 03:45:57.596: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-3473 08/25/22 03:45:57.6
Aug 25 03:45:57.605: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3473" to be "running"
Aug 25 03:45:57.608: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.335803ms
Aug 25 03:45:59.612: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007797403s
Aug 25 03:45:59.612: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/25/22 03:45:59.613
Aug 25 03:46:04.624: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/25/22 03:46:04.624
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Aug 25 03:46:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3473" for this suite. 08/25/22 03:46:04.635
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":166,"skipped":3145,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [9.080 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:45:55.559
    Aug 25 03:45:55.559: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename prestop 08/25/22 03:45:55.56
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:45:55.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:45:55.576
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-3473 08/25/22 03:45:55.58
    STEP: Waiting for pods to come up. 08/25/22 03:45:55.587
    Aug 25 03:45:55.588: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3473" to be "running"
    Aug 25 03:45:55.590: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434808ms
    Aug 25 03:45:57.596: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007945209s
    Aug 25 03:45:57.596: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-3473 08/25/22 03:45:57.6
    Aug 25 03:45:57.605: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3473" to be "running"
    Aug 25 03:45:57.608: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.335803ms
    Aug 25 03:45:59.612: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007797403s
    Aug 25 03:45:59.612: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/25/22 03:45:59.613
    Aug 25 03:46:04.624: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/25/22 03:46:04.624
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Aug 25 03:46:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-3473" for this suite. 08/25/22 03:46:04.635
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:46:04.64
Aug 25 03:46:04.640: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:46:04.641
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:04.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:04.658
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7108 08/25/22 03:46:04.661
STEP: changing the ExternalName service to type=ClusterIP 08/25/22 03:46:04.667
STEP: creating replication controller externalname-service in namespace services-7108 08/25/22 03:46:04.679
I0825 03:46:04.684329      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7108, replica count: 2
I0825 03:46:07.736116      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 03:46:07.736: INFO: Creating new exec pod
Aug 25 03:46:07.742: INFO: Waiting up to 5m0s for pod "execpod6fjhp" in namespace "services-7108" to be "running"
Aug 25 03:46:07.745: INFO: Pod "execpod6fjhp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763359ms
Aug 25 03:46:09.750: INFO: Pod "execpod6fjhp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008031742s
Aug 25 03:46:09.750: INFO: Pod "execpod6fjhp" satisfied condition "running"
Aug 25 03:46:10.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 25 03:46:10.892: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 25 03:46:10.892: INFO: stdout: ""
Aug 25 03:46:11.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 25 03:46:12.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 25 03:46:12.097: INFO: stdout: "externalname-service-qvv88"
Aug 25 03:46:12.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.170.200 80'
Aug 25 03:46:12.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.170.200 80\nConnection to 10.102.170.200 80 port [tcp/http] succeeded!\n"
Aug 25 03:46:12.269: INFO: stdout: ""
Aug 25 03:46:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.170.200 80'
Aug 25 03:46:13.464: INFO: stderr: "+ + nc -v -techo -w 2 hostName 10.102.170.200 80\n\nConnection to 10.102.170.200 80 port [tcp/http] succeeded!\n"
Aug 25 03:46:13.464: INFO: stdout: "externalname-service-qvv88"
Aug 25 03:46:13.464: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:46:13.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7108" for this suite. 08/25/22 03:46:13.481
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":167,"skipped":3151,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.846 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:46:04.64
    Aug 25 03:46:04.640: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:46:04.641
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:04.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:04.658
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7108 08/25/22 03:46:04.661
    STEP: changing the ExternalName service to type=ClusterIP 08/25/22 03:46:04.667
    STEP: creating replication controller externalname-service in namespace services-7108 08/25/22 03:46:04.679
    I0825 03:46:04.684329      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7108, replica count: 2
    I0825 03:46:07.736116      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 03:46:07.736: INFO: Creating new exec pod
    Aug 25 03:46:07.742: INFO: Waiting up to 5m0s for pod "execpod6fjhp" in namespace "services-7108" to be "running"
    Aug 25 03:46:07.745: INFO: Pod "execpod6fjhp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763359ms
    Aug 25 03:46:09.750: INFO: Pod "execpod6fjhp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008031742s
    Aug 25 03:46:09.750: INFO: Pod "execpod6fjhp" satisfied condition "running"
    Aug 25 03:46:10.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 25 03:46:10.892: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 25 03:46:10.892: INFO: stdout: ""
    Aug 25 03:46:11.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 25 03:46:12.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 25 03:46:12.097: INFO: stdout: "externalname-service-qvv88"
    Aug 25 03:46:12.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.170.200 80'
    Aug 25 03:46:12.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.170.200 80\nConnection to 10.102.170.200 80 port [tcp/http] succeeded!\n"
    Aug 25 03:46:12.269: INFO: stdout: ""
    Aug 25 03:46:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-7108 exec execpod6fjhp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.170.200 80'
    Aug 25 03:46:13.464: INFO: stderr: "+ + nc -v -techo -w 2 hostName 10.102.170.200 80\n\nConnection to 10.102.170.200 80 port [tcp/http] succeeded!\n"
    Aug 25 03:46:13.464: INFO: stdout: "externalname-service-qvv88"
    Aug 25 03:46:13.464: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:46:13.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7108" for this suite. 08/25/22 03:46:13.481
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:46:13.488
Aug 25 03:46:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:46:13.489
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:13.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:13.505
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-0eb9b4af-8a25-4518-8ae0-29cd03d09073 08/25/22 03:46:13.508
STEP: Creating a pod to test consume configMaps 08/25/22 03:46:13.512
Aug 25 03:46:13.518: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679" in namespace "projected-3028" to be "Succeeded or Failed"
Aug 25 03:46:13.520: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242268ms
Aug 25 03:46:15.526: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007288433s
Aug 25 03:46:17.527: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008781688s
STEP: Saw pod success 08/25/22 03:46:17.527
Aug 25 03:46:17.527: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679" satisfied condition "Succeeded or Failed"
Aug 25 03:46:17.531: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:46:17.537
Aug 25 03:46:17.544: INFO: Waiting for pod pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679 to disappear
Aug 25 03:46:17.548: INFO: Pod pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 03:46:17.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3028" for this suite. 08/25/22 03:46:17.552
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":168,"skipped":3178,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:46:13.488
    Aug 25 03:46:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:46:13.489
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:13.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:13.505
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-0eb9b4af-8a25-4518-8ae0-29cd03d09073 08/25/22 03:46:13.508
    STEP: Creating a pod to test consume configMaps 08/25/22 03:46:13.512
    Aug 25 03:46:13.518: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679" in namespace "projected-3028" to be "Succeeded or Failed"
    Aug 25 03:46:13.520: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242268ms
    Aug 25 03:46:15.526: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007288433s
    Aug 25 03:46:17.527: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008781688s
    STEP: Saw pod success 08/25/22 03:46:17.527
    Aug 25 03:46:17.527: INFO: Pod "pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679" satisfied condition "Succeeded or Failed"
    Aug 25 03:46:17.531: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:46:17.537
    Aug 25 03:46:17.544: INFO: Waiting for pod pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679 to disappear
    Aug 25 03:46:17.548: INFO: Pod pod-projected-configmaps-efe46f36-2e19-4af4-9e42-f114fc685679 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 03:46:17.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3028" for this suite. 08/25/22 03:46:17.552
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:46:17.556
Aug 25 03:46:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 03:46:17.557
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:17.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:17.568
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8551/secret-test-d393858d-d209-41c2-a0bd-b443cde3ec24 08/25/22 03:46:17.572
STEP: Creating a pod to test consume secrets 08/25/22 03:46:17.576
Aug 25 03:46:17.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97" in namespace "secrets-8551" to be "Succeeded or Failed"
Aug 25 03:46:17.586: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668787ms
Aug 25 03:46:19.591: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007375699s
Aug 25 03:46:21.591: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007657749s
STEP: Saw pod success 08/25/22 03:46:21.591
Aug 25 03:46:21.591: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97" satisfied condition "Succeeded or Failed"
Aug 25 03:46:21.595: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97 container env-test: <nil>
STEP: delete the pod 08/25/22 03:46:21.601
Aug 25 03:46:21.608: INFO: Waiting for pod pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97 to disappear
Aug 25 03:46:21.612: INFO: Pod pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 25 03:46:21.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8551" for this suite. 08/25/22 03:46:21.615
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":169,"skipped":3184,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.063 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:46:17.556
    Aug 25 03:46:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 03:46:17.557
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:17.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:17.568
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8551/secret-test-d393858d-d209-41c2-a0bd-b443cde3ec24 08/25/22 03:46:17.572
    STEP: Creating a pod to test consume secrets 08/25/22 03:46:17.576
    Aug 25 03:46:17.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97" in namespace "secrets-8551" to be "Succeeded or Failed"
    Aug 25 03:46:17.586: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668787ms
    Aug 25 03:46:19.591: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007375699s
    Aug 25 03:46:21.591: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007657749s
    STEP: Saw pod success 08/25/22 03:46:21.591
    Aug 25 03:46:21.591: INFO: Pod "pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97" satisfied condition "Succeeded or Failed"
    Aug 25 03:46:21.595: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97 container env-test: <nil>
    STEP: delete the pod 08/25/22 03:46:21.601
    Aug 25 03:46:21.608: INFO: Waiting for pod pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97 to disappear
    Aug 25 03:46:21.612: INFO: Pod pod-configmaps-55e25482-5006-41e9-816d-ddb1988edf97 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 03:46:21.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8551" for this suite. 08/25/22 03:46:21.615
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:46:21.624
Aug 25 03:46:21.625: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-preemption 08/25/22 03:46:21.626
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:21.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:21.644
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 25 03:46:21.658: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 25 03:47:21.742: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:47:21.747
Aug 25 03:47:21.747: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-preemption-path 08/25/22 03:47:21.748
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:47:21.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:47:21.765
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 08/25/22 03:47:21.768
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/25/22 03:47:21.769
Aug 25 03:47:21.774: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5502" to be "running"
Aug 25 03:47:21.777: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459132ms
Aug 25 03:47:23.782: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008100667s
Aug 25 03:47:23.782: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/25/22 03:47:23.786
Aug 25 03:47:23.793: INFO: found a healthy node: bobymcbobs-c849-control-plane-p55pp
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Aug 25 03:47:37.870: INFO: pods created so far: [1 1 1]
Aug 25 03:47:37.870: INFO: length of pods created so far: 3
Aug 25 03:47:39.879: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Aug 25 03:47:46.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5502" for this suite. 08/25/22 03:47:46.886
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:47:46.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9980" for this suite. 08/25/22 03:47:46.932
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":170,"skipped":3242,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [85.339 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:46:21.624
    Aug 25 03:46:21.625: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-preemption 08/25/22 03:46:21.626
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:46:21.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:46:21.644
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 25 03:46:21.658: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 25 03:47:21.742: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:47:21.747
    Aug 25 03:47:21.747: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-preemption-path 08/25/22 03:47:21.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:47:21.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:47:21.765
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 08/25/22 03:47:21.768
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/25/22 03:47:21.769
    Aug 25 03:47:21.774: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5502" to be "running"
    Aug 25 03:47:21.777: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459132ms
    Aug 25 03:47:23.782: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008100667s
    Aug 25 03:47:23.782: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/25/22 03:47:23.786
    Aug 25 03:47:23.793: INFO: found a healthy node: bobymcbobs-c849-control-plane-p55pp
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Aug 25 03:47:37.870: INFO: pods created so far: [1 1 1]
    Aug 25 03:47:37.870: INFO: length of pods created so far: 3
    Aug 25 03:47:39.879: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Aug 25 03:47:46.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-5502" for this suite. 08/25/22 03:47:46.886
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:47:46.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9980" for this suite. 08/25/22 03:47:46.932
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:47:46.965
Aug 25 03:47:46.965: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:47:46.966
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:47:46.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:47:46.983
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Aug 25 03:47:46.988: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/25/22 03:47:51.032
Aug 25 03:47:51.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
Aug 25 03:47:51.908: INFO: stderr: ""
Aug 25 03:47:51.908: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 25 03:47:51.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 delete e2e-test-crd-publish-openapi-381-crds test-foo'
Aug 25 03:47:51.985: INFO: stderr: ""
Aug 25 03:47:51.985: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 25 03:47:51.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 apply -f -'
Aug 25 03:47:52.269: INFO: stderr: ""
Aug 25 03:47:52.269: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 25 03:47:52.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 delete e2e-test-crd-publish-openapi-381-crds test-foo'
Aug 25 03:47:52.349: INFO: stderr: ""
Aug 25 03:47:52.349: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/25/22 03:47:52.349
Aug 25 03:47:52.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
Aug 25 03:47:52.617: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/25/22 03:47:52.617
Aug 25 03:47:52.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
Aug 25 03:47:52.879: INFO: rc: 1
Aug 25 03:47:52.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 apply -f -'
Aug 25 03:47:53.156: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/25/22 03:47:53.156
Aug 25 03:47:53.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
Aug 25 03:47:53.439: INFO: rc: 1
Aug 25 03:47:53.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 apply -f -'
Aug 25 03:47:53.699: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/25/22 03:47:53.699
Aug 25 03:47:53.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds'
Aug 25 03:47:53.972: INFO: stderr: ""
Aug 25 03:47:53.972: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/25/22 03:47:53.973
Aug 25 03:47:53.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.metadata'
Aug 25 03:47:54.234: INFO: stderr: ""
Aug 25 03:47:54.234: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 25 03:47:54.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.spec'
Aug 25 03:47:54.502: INFO: stderr: ""
Aug 25 03:47:54.502: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 25 03:47:54.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.spec.bars'
Aug 25 03:47:54.771: INFO: stderr: ""
Aug 25 03:47:54.771: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/25/22 03:47:54.771
Aug 25 03:47:54.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.spec.bars2'
Aug 25 03:47:55.022: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:47:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7025" for this suite. 08/25/22 03:47:59.1
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":171,"skipped":3249,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [12.140 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:47:46.965
    Aug 25 03:47:46.965: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:47:46.966
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:47:46.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:47:46.983
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Aug 25 03:47:46.988: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/25/22 03:47:51.032
    Aug 25 03:47:51.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
    Aug 25 03:47:51.908: INFO: stderr: ""
    Aug 25 03:47:51.908: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 25 03:47:51.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 delete e2e-test-crd-publish-openapi-381-crds test-foo'
    Aug 25 03:47:51.985: INFO: stderr: ""
    Aug 25 03:47:51.985: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 25 03:47:51.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 apply -f -'
    Aug 25 03:47:52.269: INFO: stderr: ""
    Aug 25 03:47:52.269: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 25 03:47:52.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 delete e2e-test-crd-publish-openapi-381-crds test-foo'
    Aug 25 03:47:52.349: INFO: stderr: ""
    Aug 25 03:47:52.349: INFO: stdout: "e2e-test-crd-publish-openapi-381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/25/22 03:47:52.349
    Aug 25 03:47:52.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
    Aug 25 03:47:52.617: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/25/22 03:47:52.617
    Aug 25 03:47:52.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
    Aug 25 03:47:52.879: INFO: rc: 1
    Aug 25 03:47:52.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 apply -f -'
    Aug 25 03:47:53.156: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/25/22 03:47:53.156
    Aug 25 03:47:53.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 create -f -'
    Aug 25 03:47:53.439: INFO: rc: 1
    Aug 25 03:47:53.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 --namespace=crd-publish-openapi-7025 apply -f -'
    Aug 25 03:47:53.699: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/25/22 03:47:53.699
    Aug 25 03:47:53.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds'
    Aug 25 03:47:53.972: INFO: stderr: ""
    Aug 25 03:47:53.972: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/25/22 03:47:53.973
    Aug 25 03:47:53.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.metadata'
    Aug 25 03:47:54.234: INFO: stderr: ""
    Aug 25 03:47:54.234: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 25 03:47:54.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.spec'
    Aug 25 03:47:54.502: INFO: stderr: ""
    Aug 25 03:47:54.502: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 25 03:47:54.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.spec.bars'
    Aug 25 03:47:54.771: INFO: stderr: ""
    Aug 25 03:47:54.771: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/25/22 03:47:54.771
    Aug 25 03:47:54.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-7025 explain e2e-test-crd-publish-openapi-381-crds.spec.bars2'
    Aug 25 03:47:55.022: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:47:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7025" for this suite. 08/25/22 03:47:59.1
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:47:59.105
Aug 25 03:47:59.105: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:47:59.106
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:47:59.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:47:59.12
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 25 03:47:59.123: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:48:05.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3876" for this suite. 08/25/22 03:48:05.524
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":172,"skipped":3249,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.424 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:47:59.105
    Aug 25 03:47:59.105: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:47:59.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:47:59.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:47:59.12
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 25 03:47:59.123: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:48:05.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3876" for this suite. 08/25/22 03:48:05.524
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:05.53
Aug 25 03:48:05.531: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 03:48:05.533
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:05.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:05.548
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/25/22 03:48:05.557
Aug 25 03:48:05.565: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7490" to be "running and ready"
Aug 25 03:48:05.569: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435422ms
Aug 25 03:48:05.569: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:48:07.575: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009402961s
Aug 25 03:48:07.575: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 25 03:48:07.575: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 08/25/22 03:48:07.579
Aug 25 03:48:07.585: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7490" to be "running and ready"
Aug 25 03:48:07.589: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.736494ms
Aug 25 03:48:07.589: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:48:09.593: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008422592s
Aug 25 03:48:09.593: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 25 03:48:09.593: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/25/22 03:48:09.597
STEP: delete the pod with lifecycle hook 08/25/22 03:48:09.613
Aug 25 03:48:09.618: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 25 03:48:09.623: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 25 03:48:11.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 25 03:48:11.628: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 25 03:48:13.625: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 25 03:48:13.629: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 25 03:48:13.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7490" for this suite. 08/25/22 03:48:13.634
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":173,"skipped":3256,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.109 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:05.53
    Aug 25 03:48:05.531: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 03:48:05.533
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:05.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:05.548
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/25/22 03:48:05.557
    Aug 25 03:48:05.565: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7490" to be "running and ready"
    Aug 25 03:48:05.569: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435422ms
    Aug 25 03:48:05.569: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:48:07.575: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009402961s
    Aug 25 03:48:07.575: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 25 03:48:07.575: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 08/25/22 03:48:07.579
    Aug 25 03:48:07.585: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7490" to be "running and ready"
    Aug 25 03:48:07.589: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.736494ms
    Aug 25 03:48:07.589: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:48:09.593: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008422592s
    Aug 25 03:48:09.593: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 25 03:48:09.593: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/25/22 03:48:09.597
    STEP: delete the pod with lifecycle hook 08/25/22 03:48:09.613
    Aug 25 03:48:09.618: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 25 03:48:09.623: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 25 03:48:11.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 25 03:48:11.628: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 25 03:48:13.625: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 25 03:48:13.629: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 25 03:48:13.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7490" for this suite. 08/25/22 03:48:13.634
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:13.643
Aug 25 03:48:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:48:13.645
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:13.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:13.66
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 08/25/22 03:48:13.667
STEP: waiting for available Endpoint 08/25/22 03:48:13.671
STEP: listing all Endpoints 08/25/22 03:48:13.678
STEP: updating the Endpoint 08/25/22 03:48:13.682
STEP: fetching the Endpoint 08/25/22 03:48:13.688
STEP: patching the Endpoint 08/25/22 03:48:13.69
STEP: fetching the Endpoint 08/25/22 03:48:13.7
STEP: deleting the Endpoint by Collection 08/25/22 03:48:13.702
STEP: waiting for Endpoint deletion 08/25/22 03:48:13.706
STEP: fetching the Endpoint 08/25/22 03:48:13.708
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:48:13.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1587" for this suite. 08/25/22 03:48:13.715
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":174,"skipped":3309,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.075 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:13.643
    Aug 25 03:48:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:48:13.645
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:13.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:13.66
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 08/25/22 03:48:13.667
    STEP: waiting for available Endpoint 08/25/22 03:48:13.671
    STEP: listing all Endpoints 08/25/22 03:48:13.678
    STEP: updating the Endpoint 08/25/22 03:48:13.682
    STEP: fetching the Endpoint 08/25/22 03:48:13.688
    STEP: patching the Endpoint 08/25/22 03:48:13.69
    STEP: fetching the Endpoint 08/25/22 03:48:13.7
    STEP: deleting the Endpoint by Collection 08/25/22 03:48:13.702
    STEP: waiting for Endpoint deletion 08/25/22 03:48:13.706
    STEP: fetching the Endpoint 08/25/22 03:48:13.708
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:48:13.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1587" for this suite. 08/25/22 03:48:13.715
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:13.719
Aug 25 03:48:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:48:13.721
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:13.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:13.736
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 08/25/22 03:48:13.74
Aug 25 03:48:13.746: INFO: Waiting up to 5m0s for pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3" in namespace "projected-1957" to be "running and ready"
Aug 25 03:48:13.749: INFO: Pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691103ms
Aug 25 03:48:13.749: INFO: The phase of Pod labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:48:15.754: INFO: Pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008430127s
Aug 25 03:48:15.754: INFO: The phase of Pod labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3 is Running (Ready = true)
Aug 25 03:48:15.754: INFO: Pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3" satisfied condition "running and ready"
Aug 25 03:48:16.277: INFO: Successfully updated pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:48:20.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1957" for this suite. 08/25/22 03:48:20.302
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":175,"skipped":3328,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.587 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:13.719
    Aug 25 03:48:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:48:13.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:13.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:13.736
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 08/25/22 03:48:13.74
    Aug 25 03:48:13.746: INFO: Waiting up to 5m0s for pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3" in namespace "projected-1957" to be "running and ready"
    Aug 25 03:48:13.749: INFO: Pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691103ms
    Aug 25 03:48:13.749: INFO: The phase of Pod labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:48:15.754: INFO: Pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008430127s
    Aug 25 03:48:15.754: INFO: The phase of Pod labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3 is Running (Ready = true)
    Aug 25 03:48:15.754: INFO: Pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3" satisfied condition "running and ready"
    Aug 25 03:48:16.277: INFO: Successfully updated pod "labelsupdate0b0162ec-aa95-4cb7-ba58-06272730a4e3"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:48:20.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1957" for this suite. 08/25/22 03:48:20.302
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:20.307
Aug 25 03:48:20.307: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:48:20.309
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:20.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:20.324
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:48:20.328
Aug 25 03:48:20.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959" in namespace "projected-4713" to be "Succeeded or Failed"
Aug 25 03:48:20.341: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579444ms
Aug 25 03:48:22.345: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007839155s
Aug 25 03:48:24.346: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008787945s
STEP: Saw pod success 08/25/22 03:48:24.346
Aug 25 03:48:24.346: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959" satisfied condition "Succeeded or Failed"
Aug 25 03:48:24.351: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959 container client-container: <nil>
STEP: delete the pod 08/25/22 03:48:24.356
Aug 25 03:48:24.363: INFO: Waiting for pod downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959 to disappear
Aug 25 03:48:24.366: INFO: Pod downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:48:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4713" for this suite. 08/25/22 03:48:24.37
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":176,"skipped":3328,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:20.307
    Aug 25 03:48:20.307: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:48:20.309
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:20.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:20.324
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:48:20.328
    Aug 25 03:48:20.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959" in namespace "projected-4713" to be "Succeeded or Failed"
    Aug 25 03:48:20.341: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579444ms
    Aug 25 03:48:22.345: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007839155s
    Aug 25 03:48:24.346: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008787945s
    STEP: Saw pod success 08/25/22 03:48:24.346
    Aug 25 03:48:24.346: INFO: Pod "downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959" satisfied condition "Succeeded or Failed"
    Aug 25 03:48:24.351: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:48:24.356
    Aug 25 03:48:24.363: INFO: Waiting for pod downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959 to disappear
    Aug 25 03:48:24.366: INFO: Pod downwardapi-volume-12be83c6-9f24-4fd2-8476-643a415f8959 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:48:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4713" for this suite. 08/25/22 03:48:24.37
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:24.377
Aug 25 03:48:24.377: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename disruption 08/25/22 03:48:24.378
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:24.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:24.394
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 08/25/22 03:48:24.402
STEP: Waiting for all pods to be running 08/25/22 03:48:26.427
Aug 25 03:48:26.430: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 25 03:48:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5234" for this suite. 08/25/22 03:48:28.444
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":177,"skipped":3345,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.072 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:24.377
    Aug 25 03:48:24.377: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename disruption 08/25/22 03:48:24.378
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:24.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:24.394
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 08/25/22 03:48:24.402
    STEP: Waiting for all pods to be running 08/25/22 03:48:26.427
    Aug 25 03:48:26.430: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 25 03:48:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5234" for this suite. 08/25/22 03:48:28.444
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:28.45
Aug 25 03:48:28.451: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:48:28.452
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:28.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:28.467
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-6341a364-a854-4e92-b03d-fe7258f7402c 08/25/22 03:48:28.472
STEP: Creating a pod to test consume secrets 08/25/22 03:48:28.475
Aug 25 03:48:28.481: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e" in namespace "projected-1286" to be "Succeeded or Failed"
Aug 25 03:48:28.483: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082952ms
Aug 25 03:48:30.488: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007346261s
Aug 25 03:48:32.490: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008799535s
STEP: Saw pod success 08/25/22 03:48:32.49
Aug 25 03:48:32.490: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e" satisfied condition "Succeeded or Failed"
Aug 25 03:48:32.494: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e container projected-secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:48:32.5
Aug 25 03:48:32.508: INFO: Waiting for pod pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e to disappear
Aug 25 03:48:32.511: INFO: Pod pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 03:48:32.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1286" for this suite. 08/25/22 03:48:32.515
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":3362,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.070 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:28.45
    Aug 25 03:48:28.451: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:48:28.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:28.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:28.467
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-6341a364-a854-4e92-b03d-fe7258f7402c 08/25/22 03:48:28.472
    STEP: Creating a pod to test consume secrets 08/25/22 03:48:28.475
    Aug 25 03:48:28.481: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e" in namespace "projected-1286" to be "Succeeded or Failed"
    Aug 25 03:48:28.483: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082952ms
    Aug 25 03:48:30.488: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007346261s
    Aug 25 03:48:32.490: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008799535s
    STEP: Saw pod success 08/25/22 03:48:32.49
    Aug 25 03:48:32.490: INFO: Pod "pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e" satisfied condition "Succeeded or Failed"
    Aug 25 03:48:32.494: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:48:32.5
    Aug 25 03:48:32.508: INFO: Waiting for pod pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e to disappear
    Aug 25 03:48:32.511: INFO: Pod pod-projected-secrets-177415ef-801f-493a-a891-914e03b60c3e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 03:48:32.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1286" for this suite. 08/25/22 03:48:32.515
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:32.524
Aug 25 03:48:32.524: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sysctl 08/25/22 03:48:32.525
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:32.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:32.539
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/25/22 03:48:32.542
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 03:48:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1473" for this suite. 08/25/22 03:48:32.55
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":179,"skipped":3428,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.030 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:32.524
    Aug 25 03:48:32.524: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sysctl 08/25/22 03:48:32.525
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:32.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:32.539
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/25/22 03:48:32.542
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 03:48:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1473" for this suite. 08/25/22 03:48:32.55
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:32.556
Aug 25 03:48:32.556: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pod-network-test 08/25/22 03:48:32.557
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:32.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:32.571
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-8205 08/25/22 03:48:32.576
STEP: creating a selector 08/25/22 03:48:32.576
STEP: Creating the service pods in kubernetes 08/25/22 03:48:32.577
Aug 25 03:48:32.577: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 25 03:48:32.588: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8205" to be "running and ready"
Aug 25 03:48:32.590: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67344ms
Aug 25 03:48:32.590: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:48:34.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007711372s
Aug 25 03:48:34.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:36.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009093698s
Aug 25 03:48:36.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:38.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007430406s
Aug 25 03:48:38.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:40.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008282623s
Aug 25 03:48:40.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:42.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007739891s
Aug 25 03:48:42.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:44.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008902292s
Aug 25 03:48:44.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:46.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008420968s
Aug 25 03:48:46.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:48.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009427953s
Aug 25 03:48:48.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:50.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008297506s
Aug 25 03:48:50.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:52.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007396363s
Aug 25 03:48:52.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 03:48:54.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008695317s
Aug 25 03:48:54.596: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 25 03:48:54.596: INFO: Pod "netserver-0" satisfied condition "running and ready"
STEP: Creating test pods 08/25/22 03:48:54.601
Aug 25 03:48:54.611: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8205" to be "running"
Aug 25 03:48:54.615: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.626345ms
Aug 25 03:48:56.621: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009361476s
Aug 25 03:48:56.621: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 25 03:48:56.624: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8205" to be "running"
Aug 25 03:48:56.628: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.197342ms
Aug 25 03:48:56.628: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 25 03:48:56.630: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
Aug 25 03:48:56.631: INFO: Going to poll 192.168.0.15 on port 8081 at least 0 times, with a maximum of 31 tries before failing
Aug 25 03:48:56.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.0.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8205 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:48:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:48:56.635: INFO: ExecWithOptions: Clientset creation
Aug 25 03:48:56.635: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8205/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.0.15+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 25 03:48:57.753: INFO: Found all 1 expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 25 03:48:57.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8205" for this suite. 08/25/22 03:48:57.758
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":3447,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [25.207 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:32.556
    Aug 25 03:48:32.556: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pod-network-test 08/25/22 03:48:32.557
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:32.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:32.571
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-8205 08/25/22 03:48:32.576
    STEP: creating a selector 08/25/22 03:48:32.576
    STEP: Creating the service pods in kubernetes 08/25/22 03:48:32.577
    Aug 25 03:48:32.577: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 25 03:48:32.588: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8205" to be "running and ready"
    Aug 25 03:48:32.590: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67344ms
    Aug 25 03:48:32.590: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:48:34.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007711372s
    Aug 25 03:48:34.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:36.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009093698s
    Aug 25 03:48:36.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:38.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007430406s
    Aug 25 03:48:38.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:40.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008282623s
    Aug 25 03:48:40.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:42.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007739891s
    Aug 25 03:48:42.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:44.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008902292s
    Aug 25 03:48:44.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:46.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008420968s
    Aug 25 03:48:46.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:48.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009427953s
    Aug 25 03:48:48.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:50.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008297506s
    Aug 25 03:48:50.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:52.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007396363s
    Aug 25 03:48:52.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 03:48:54.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008695317s
    Aug 25 03:48:54.596: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 25 03:48:54.596: INFO: Pod "netserver-0" satisfied condition "running and ready"
    STEP: Creating test pods 08/25/22 03:48:54.601
    Aug 25 03:48:54.611: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8205" to be "running"
    Aug 25 03:48:54.615: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.626345ms
    Aug 25 03:48:56.621: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009361476s
    Aug 25 03:48:56.621: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 25 03:48:56.624: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8205" to be "running"
    Aug 25 03:48:56.628: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.197342ms
    Aug 25 03:48:56.628: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 25 03:48:56.630: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
    Aug 25 03:48:56.631: INFO: Going to poll 192.168.0.15 on port 8081 at least 0 times, with a maximum of 31 tries before failing
    Aug 25 03:48:56.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.0.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8205 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:48:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:48:56.635: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:48:56.635: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8205/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.0.15+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 25 03:48:57.753: INFO: Found all 1 expected endpoints: [netserver-0]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 25 03:48:57.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8205" for this suite. 08/25/22 03:48:57.758
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:48:57.764
Aug 25 03:48:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 03:48:57.765
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:57.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:57.782
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Aug 25 03:48:57.795: INFO: Waiting up to 5m0s for pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f" in namespace "container-probe-296" to be "running and ready"
Aug 25 03:48:57.798: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.867989ms
Aug 25 03:48:57.798: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:48:59.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 2.009122896s
Aug 25 03:48:59.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:01.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 4.008491781s
Aug 25 03:49:01.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:03.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 6.009114097s
Aug 25 03:49:03.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:05.802: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 8.007480755s
Aug 25 03:49:05.802: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:07.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 10.008215878s
Aug 25 03:49:07.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:09.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 12.008707893s
Aug 25 03:49:09.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:11.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 14.008350573s
Aug 25 03:49:11.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:13.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 16.009114486s
Aug 25 03:49:13.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:15.802: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 18.007518567s
Aug 25 03:49:15.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:17.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 20.008486079s
Aug 25 03:49:17.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
Aug 25 03:49:19.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=true. Elapsed: 22.008695182s
Aug 25 03:49:19.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = true)
Aug 25 03:49:19.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f" satisfied condition "running and ready"
Aug 25 03:49:19.807: INFO: Container started at 2022-08-25 03:48:58 +0000 UTC, pod became ready at 2022-08-25 03:49:18 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 03:49:19.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-296" for this suite. 08/25/22 03:49:19.812
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":181,"skipped":3449,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [22.053 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:48:57.764
    Aug 25 03:48:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 03:48:57.765
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:48:57.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:48:57.782
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Aug 25 03:48:57.795: INFO: Waiting up to 5m0s for pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f" in namespace "container-probe-296" to be "running and ready"
    Aug 25 03:48:57.798: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.867989ms
    Aug 25 03:48:57.798: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:48:59.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 2.009122896s
    Aug 25 03:48:59.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:01.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 4.008491781s
    Aug 25 03:49:01.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:03.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 6.009114097s
    Aug 25 03:49:03.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:05.802: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 8.007480755s
    Aug 25 03:49:05.802: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:07.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 10.008215878s
    Aug 25 03:49:07.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:09.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 12.008707893s
    Aug 25 03:49:09.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:11.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 14.008350573s
    Aug 25 03:49:11.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:13.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 16.009114486s
    Aug 25 03:49:13.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:15.802: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 18.007518567s
    Aug 25 03:49:15.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:17.803: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=false. Elapsed: 20.008486079s
    Aug 25 03:49:17.803: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = false)
    Aug 25 03:49:19.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f": Phase="Running", Reason="", readiness=true. Elapsed: 22.008695182s
    Aug 25 03:49:19.804: INFO: The phase of Pod test-webserver-5e19060c-2a25-4703-aa77-332d37da215f is Running (Ready = true)
    Aug 25 03:49:19.804: INFO: Pod "test-webserver-5e19060c-2a25-4703-aa77-332d37da215f" satisfied condition "running and ready"
    Aug 25 03:49:19.807: INFO: Container started at 2022-08-25 03:48:58 +0000 UTC, pod became ready at 2022-08-25 03:49:18 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 03:49:19.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-296" for this suite. 08/25/22 03:49:19.812
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:49:19.819
Aug 25 03:49:19.819: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:49:19.82
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:49:19.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:49:19.835
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 08/25/22 03:49:19.838
Aug 25 03:49:19.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 create -f -'
Aug 25 03:49:20.213: INFO: stderr: ""
Aug 25 03:49:20.213: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 03:49:20.213
Aug 25 03:49:20.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 03:49:20.293: INFO: stderr: ""
Aug 25 03:49:20.293: INFO: stdout: "update-demo-nautilus-9ghcv update-demo-nautilus-b5ft4 "
Aug 25 03:49:20.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-9ghcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 03:49:20.376: INFO: stderr: ""
Aug 25 03:49:20.376: INFO: stdout: ""
Aug 25 03:49:20.376: INFO: update-demo-nautilus-9ghcv is created but not running
Aug 25 03:49:25.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 03:49:25.469: INFO: stderr: ""
Aug 25 03:49:25.469: INFO: stdout: "update-demo-nautilus-9ghcv update-demo-nautilus-b5ft4 "
Aug 25 03:49:25.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-9ghcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 03:49:25.544: INFO: stderr: ""
Aug 25 03:49:25.544: INFO: stdout: "true"
Aug 25 03:49:25.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-9ghcv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 03:49:25.625: INFO: stderr: ""
Aug 25 03:49:25.625: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 03:49:25.625: INFO: validating pod update-demo-nautilus-9ghcv
Aug 25 03:49:25.630: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 03:49:25.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 03:49:25.630: INFO: update-demo-nautilus-9ghcv is verified up and running
Aug 25 03:49:25.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-b5ft4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 03:49:25.713: INFO: stderr: ""
Aug 25 03:49:25.713: INFO: stdout: "true"
Aug 25 03:49:25.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-b5ft4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 03:49:25.787: INFO: stderr: ""
Aug 25 03:49:25.787: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 03:49:25.787: INFO: validating pod update-demo-nautilus-b5ft4
Aug 25 03:49:25.791: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 03:49:25.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 03:49:25.791: INFO: update-demo-nautilus-b5ft4 is verified up and running
STEP: using delete to clean up resources 08/25/22 03:49:25.791
Aug 25 03:49:25.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 delete --grace-period=0 --force -f -'
Aug 25 03:49:25.871: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 03:49:25.871: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 25 03:49:25.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get rc,svc -l name=update-demo --no-headers'
Aug 25 03:49:25.955: INFO: stderr: "No resources found in kubectl-1619 namespace.\n"
Aug 25 03:49:25.955: INFO: stdout: ""
Aug 25 03:49:25.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 25 03:49:26.032: INFO: stderr: ""
Aug 25 03:49:26.032: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:49:26.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1619" for this suite. 08/25/22 03:49:26.038
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":182,"skipped":3468,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.224 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:49:19.819
    Aug 25 03:49:19.819: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:49:19.82
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:49:19.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:49:19.835
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 08/25/22 03:49:19.838
    Aug 25 03:49:19.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 create -f -'
    Aug 25 03:49:20.213: INFO: stderr: ""
    Aug 25 03:49:20.213: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 03:49:20.213
    Aug 25 03:49:20.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 03:49:20.293: INFO: stderr: ""
    Aug 25 03:49:20.293: INFO: stdout: "update-demo-nautilus-9ghcv update-demo-nautilus-b5ft4 "
    Aug 25 03:49:20.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-9ghcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 03:49:20.376: INFO: stderr: ""
    Aug 25 03:49:20.376: INFO: stdout: ""
    Aug 25 03:49:20.376: INFO: update-demo-nautilus-9ghcv is created but not running
    Aug 25 03:49:25.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 03:49:25.469: INFO: stderr: ""
    Aug 25 03:49:25.469: INFO: stdout: "update-demo-nautilus-9ghcv update-demo-nautilus-b5ft4 "
    Aug 25 03:49:25.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-9ghcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 03:49:25.544: INFO: stderr: ""
    Aug 25 03:49:25.544: INFO: stdout: "true"
    Aug 25 03:49:25.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-9ghcv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 03:49:25.625: INFO: stderr: ""
    Aug 25 03:49:25.625: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 03:49:25.625: INFO: validating pod update-demo-nautilus-9ghcv
    Aug 25 03:49:25.630: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 03:49:25.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 03:49:25.630: INFO: update-demo-nautilus-9ghcv is verified up and running
    Aug 25 03:49:25.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-b5ft4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 03:49:25.713: INFO: stderr: ""
    Aug 25 03:49:25.713: INFO: stdout: "true"
    Aug 25 03:49:25.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods update-demo-nautilus-b5ft4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 03:49:25.787: INFO: stderr: ""
    Aug 25 03:49:25.787: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 03:49:25.787: INFO: validating pod update-demo-nautilus-b5ft4
    Aug 25 03:49:25.791: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 03:49:25.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 03:49:25.791: INFO: update-demo-nautilus-b5ft4 is verified up and running
    STEP: using delete to clean up resources 08/25/22 03:49:25.791
    Aug 25 03:49:25.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 delete --grace-period=0 --force -f -'
    Aug 25 03:49:25.871: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 03:49:25.871: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 25 03:49:25.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get rc,svc -l name=update-demo --no-headers'
    Aug 25 03:49:25.955: INFO: stderr: "No resources found in kubectl-1619 namespace.\n"
    Aug 25 03:49:25.955: INFO: stdout: ""
    Aug 25 03:49:25.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-1619 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 25 03:49:26.032: INFO: stderr: ""
    Aug 25 03:49:26.032: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:49:26.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1619" for this suite. 08/25/22 03:49:26.038
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:49:26.044
Aug 25 03:49:26.044: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:49:26.045
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:49:26.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:49:26.063
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:49:26.077
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:49:26.529
STEP: Deploying the webhook pod 08/25/22 03:49:26.539
STEP: Wait for the deployment to be ready 08/25/22 03:49:26.549
Aug 25 03:49:26.556: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 03:49:28.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:49:30.572
STEP: Verifying the service has paired with the endpoint 08/25/22 03:49:30.585
Aug 25 03:49:31.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/25/22 03:49:31.589
STEP: create a namespace for the webhook 08/25/22 03:49:31.61
STEP: create a configmap should be unconditionally rejected by the webhook 08/25/22 03:49:31.617
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:49:31.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4753" for this suite. 08/25/22 03:49:31.655
STEP: Destroying namespace "webhook-4753-markers" for this suite. 08/25/22 03:49:31.66
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":183,"skipped":3476,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.644 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:49:26.044
    Aug 25 03:49:26.044: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:49:26.045
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:49:26.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:49:26.063
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:49:26.077
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:49:26.529
    STEP: Deploying the webhook pod 08/25/22 03:49:26.539
    STEP: Wait for the deployment to be ready 08/25/22 03:49:26.549
    Aug 25 03:49:26.556: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 03:49:28.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 49, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:49:30.572
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:49:30.585
    Aug 25 03:49:31.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/25/22 03:49:31.589
    STEP: create a namespace for the webhook 08/25/22 03:49:31.61
    STEP: create a configmap should be unconditionally rejected by the webhook 08/25/22 03:49:31.617
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:49:31.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4753" for this suite. 08/25/22 03:49:31.655
    STEP: Destroying namespace "webhook-4753-markers" for this suite. 08/25/22 03:49:31.66
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:49:31.696
Aug 25 03:49:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 03:49:31.697
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:49:31.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:49:31.71
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 03:50:31.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5737" for this suite. 08/25/22 03:50:31.731
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":184,"skipped":3571,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [60.040 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:49:31.696
    Aug 25 03:49:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 03:49:31.697
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:49:31.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:49:31.71
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 03:50:31.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5737" for this suite. 08/25/22 03:50:31.731
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:50:31.738
Aug 25 03:50:31.738: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svcaccounts 08/25/22 03:50:31.739
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:50:31.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:50:31.756
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Aug 25 03:50:31.769: INFO: created pod
Aug 25 03:50:31.769: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6587" to be "Succeeded or Failed"
Aug 25 03:50:31.771: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365564ms
Aug 25 03:50:33.777: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.007921138s
Aug 25 03:50:35.776: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007372438s
STEP: Saw pod success 08/25/22 03:50:35.776
Aug 25 03:50:35.776: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 25 03:51:05.776: INFO: polling logs
Aug 25 03:51:05.798: INFO: Pod logs: 
I0825 03:50:32.550459       1 log.go:195] OK: Got token
I0825 03:50:32.550482       1 log.go:195] validating with in-cluster discovery
I0825 03:50:32.550723       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0825 03:50:32.550743       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6587:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661400031, NotBefore:1661399431, IssuedAt:1661399431, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6587", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"01b4ca15-769f-4c77-927f-2d176e0b89f9"}}}
I0825 03:50:32.564048       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0825 03:50:32.571706       1 log.go:195] OK: Validated signature on JWT
I0825 03:50:32.571801       1 log.go:195] OK: Got valid claims from token!
I0825 03:50:32.571826       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6587:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661400031, NotBefore:1661399431, IssuedAt:1661399431, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6587", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"01b4ca15-769f-4c77-927f-2d176e0b89f9"}}}

Aug 25 03:51:05.798: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 25 03:51:05.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6587" for this suite. 08/25/22 03:51:05.807
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":185,"skipped":3583,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [34.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:50:31.738
    Aug 25 03:50:31.738: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svcaccounts 08/25/22 03:50:31.739
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:50:31.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:50:31.756
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Aug 25 03:50:31.769: INFO: created pod
    Aug 25 03:50:31.769: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6587" to be "Succeeded or Failed"
    Aug 25 03:50:31.771: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365564ms
    Aug 25 03:50:33.777: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.007921138s
    Aug 25 03:50:35.776: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007372438s
    STEP: Saw pod success 08/25/22 03:50:35.776
    Aug 25 03:50:35.776: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 25 03:51:05.776: INFO: polling logs
    Aug 25 03:51:05.798: INFO: Pod logs: 
    I0825 03:50:32.550459       1 log.go:195] OK: Got token
    I0825 03:50:32.550482       1 log.go:195] validating with in-cluster discovery
    I0825 03:50:32.550723       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0825 03:50:32.550743       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6587:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661400031, NotBefore:1661399431, IssuedAt:1661399431, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6587", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"01b4ca15-769f-4c77-927f-2d176e0b89f9"}}}
    I0825 03:50:32.564048       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0825 03:50:32.571706       1 log.go:195] OK: Validated signature on JWT
    I0825 03:50:32.571801       1 log.go:195] OK: Got valid claims from token!
    I0825 03:50:32.571826       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6587:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661400031, NotBefore:1661399431, IssuedAt:1661399431, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6587", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"01b4ca15-769f-4c77-927f-2d176e0b89f9"}}}

    Aug 25 03:51:05.798: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 25 03:51:05.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6587" for this suite. 08/25/22 03:51:05.807
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:05.813
Aug 25 03:51:05.813: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:51:05.814
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:05.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:05.83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:51:05.834
Aug 25 03:51:05.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612" in namespace "downward-api-8643" to be "Succeeded or Failed"
Aug 25 03:51:05.843: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855018ms
Aug 25 03:51:07.849: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008475271s
Aug 25 03:51:09.849: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177917s
STEP: Saw pod success 08/25/22 03:51:09.849
Aug 25 03:51:09.850: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612" satisfied condition "Succeeded or Failed"
Aug 25 03:51:09.854: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612 container client-container: <nil>
STEP: delete the pod 08/25/22 03:51:09.859
Aug 25 03:51:09.866: INFO: Waiting for pod downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612 to disappear
Aug 25 03:51:09.870: INFO: Pod downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 03:51:09.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8643" for this suite. 08/25/22 03:51:09.874
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":186,"skipped":3587,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:05.813
    Aug 25 03:51:05.813: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:51:05.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:05.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:05.83
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:51:05.834
    Aug 25 03:51:05.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612" in namespace "downward-api-8643" to be "Succeeded or Failed"
    Aug 25 03:51:05.843: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855018ms
    Aug 25 03:51:07.849: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008475271s
    Aug 25 03:51:09.849: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177917s
    STEP: Saw pod success 08/25/22 03:51:09.849
    Aug 25 03:51:09.850: INFO: Pod "downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612" satisfied condition "Succeeded or Failed"
    Aug 25 03:51:09.854: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:51:09.859
    Aug 25 03:51:09.866: INFO: Waiting for pod downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612 to disappear
    Aug 25 03:51:09.870: INFO: Pod downwardapi-volume-63c52155-f875-4b52-853d-923a19a14612 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 03:51:09.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8643" for this suite. 08/25/22 03:51:09.874
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:09.879
Aug 25 03:51:09.879: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename init-container 08/25/22 03:51:09.88
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:09.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:09.895
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 08/25/22 03:51:09.9
Aug 25 03:51:09.900: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 03:51:14.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1550" for this suite. 08/25/22 03:51:14.915
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":187,"skipped":3589,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.040 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:09.879
    Aug 25 03:51:09.879: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename init-container 08/25/22 03:51:09.88
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:09.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:09.895
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 08/25/22 03:51:09.9
    Aug 25 03:51:09.900: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 03:51:14.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1550" for this suite. 08/25/22 03:51:14.915
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:14.921
Aug 25 03:51:14.921: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename certificates 08/25/22 03:51:14.922
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:14.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:14.935
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/25/22 03:51:15.436
STEP: getting /apis/certificates.k8s.io 08/25/22 03:51:15.439
STEP: getting /apis/certificates.k8s.io/v1 08/25/22 03:51:15.441
STEP: creating 08/25/22 03:51:15.443
STEP: getting 08/25/22 03:51:15.457
STEP: listing 08/25/22 03:51:15.46
STEP: watching 08/25/22 03:51:15.464
Aug 25 03:51:15.464: INFO: starting watch
STEP: patching 08/25/22 03:51:15.466
STEP: updating 08/25/22 03:51:15.471
Aug 25 03:51:15.476: INFO: waiting for watch events with expected annotations
Aug 25 03:51:15.476: INFO: saw patched and updated annotations
STEP: getting /approval 08/25/22 03:51:15.476
STEP: patching /approval 08/25/22 03:51:15.479
STEP: updating /approval 08/25/22 03:51:15.484
STEP: getting /status 08/25/22 03:51:15.489
STEP: patching /status 08/25/22 03:51:15.493
STEP: updating /status 08/25/22 03:51:15.501
STEP: deleting 08/25/22 03:51:15.506
STEP: deleting a collection 08/25/22 03:51:15.517
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:51:15.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4817" for this suite. 08/25/22 03:51:15.532
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":188,"skipped":3627,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.616 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:14.921
    Aug 25 03:51:14.921: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename certificates 08/25/22 03:51:14.922
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:14.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:14.935
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/25/22 03:51:15.436
    STEP: getting /apis/certificates.k8s.io 08/25/22 03:51:15.439
    STEP: getting /apis/certificates.k8s.io/v1 08/25/22 03:51:15.441
    STEP: creating 08/25/22 03:51:15.443
    STEP: getting 08/25/22 03:51:15.457
    STEP: listing 08/25/22 03:51:15.46
    STEP: watching 08/25/22 03:51:15.464
    Aug 25 03:51:15.464: INFO: starting watch
    STEP: patching 08/25/22 03:51:15.466
    STEP: updating 08/25/22 03:51:15.471
    Aug 25 03:51:15.476: INFO: waiting for watch events with expected annotations
    Aug 25 03:51:15.476: INFO: saw patched and updated annotations
    STEP: getting /approval 08/25/22 03:51:15.476
    STEP: patching /approval 08/25/22 03:51:15.479
    STEP: updating /approval 08/25/22 03:51:15.484
    STEP: getting /status 08/25/22 03:51:15.489
    STEP: patching /status 08/25/22 03:51:15.493
    STEP: updating /status 08/25/22 03:51:15.501
    STEP: deleting 08/25/22 03:51:15.506
    STEP: deleting a collection 08/25/22 03:51:15.517
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:51:15.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-4817" for this suite. 08/25/22 03:51:15.532
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:15.538
Aug 25 03:51:15.538: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename endpointslice 08/25/22 03:51:15.539
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:15.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:15.554
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 08/25/22 03:51:20.602
STEP: referencing matching pods with named port 08/25/22 03:51:25.611
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/25/22 03:51:30.62
STEP: recreating EndpointSlices after they've been deleted 08/25/22 03:51:35.63
Aug 25 03:51:35.647: INFO: EndpointSlice for Service endpointslice-3842/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 25 03:51:45.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3842" for this suite. 08/25/22 03:51:45.662
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":189,"skipped":3642,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [30.129 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:15.538
    Aug 25 03:51:15.538: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename endpointslice 08/25/22 03:51:15.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:15.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:15.554
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 08/25/22 03:51:20.602
    STEP: referencing matching pods with named port 08/25/22 03:51:25.611
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/25/22 03:51:30.62
    STEP: recreating EndpointSlices after they've been deleted 08/25/22 03:51:35.63
    Aug 25 03:51:35.647: INFO: EndpointSlice for Service endpointslice-3842/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 25 03:51:45.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3842" for this suite. 08/25/22 03:51:45.662
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:45.667
Aug 25 03:51:45.667: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 03:51:45.668
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:45.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:45.684
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-fb06a9eb-fd29-476c-b005-78bb51164d25 08/25/22 03:51:45.689
STEP: Creating a pod to test consume configMaps 08/25/22 03:51:45.692
Aug 25 03:51:45.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e" in namespace "configmap-9256" to be "Succeeded or Failed"
Aug 25 03:51:45.698: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.33484ms
Aug 25 03:51:47.704: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008157558s
Aug 25 03:51:49.705: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008584728s
STEP: Saw pod success 08/25/22 03:51:49.705
Aug 25 03:51:49.705: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e" satisfied condition "Succeeded or Failed"
Aug 25 03:51:49.709: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e container agnhost-container: <nil>
STEP: delete the pod 08/25/22 03:51:49.715
Aug 25 03:51:49.722: INFO: Waiting for pod pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e to disappear
Aug 25 03:51:49.726: INFO: Pod pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 03:51:49.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9256" for this suite. 08/25/22 03:51:49.73
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":190,"skipped":3642,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:45.667
    Aug 25 03:51:45.667: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 03:51:45.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:45.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:45.684
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-fb06a9eb-fd29-476c-b005-78bb51164d25 08/25/22 03:51:45.689
    STEP: Creating a pod to test consume configMaps 08/25/22 03:51:45.692
    Aug 25 03:51:45.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e" in namespace "configmap-9256" to be "Succeeded or Failed"
    Aug 25 03:51:45.698: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.33484ms
    Aug 25 03:51:47.704: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008157558s
    Aug 25 03:51:49.705: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008584728s
    STEP: Saw pod success 08/25/22 03:51:49.705
    Aug 25 03:51:49.705: INFO: Pod "pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e" satisfied condition "Succeeded or Failed"
    Aug 25 03:51:49.709: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 03:51:49.715
    Aug 25 03:51:49.722: INFO: Waiting for pod pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e to disappear
    Aug 25 03:51:49.726: INFO: Pod pod-configmaps-4dce6c88-3e90-45bc-9981-8ec570121b1e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 03:51:49.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9256" for this suite. 08/25/22 03:51:49.73
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:49.736
Aug 25 03:51:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename security-context-test 08/25/22 03:51:49.738
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:49.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:49.752
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Aug 25 03:51:49.776: INFO: Waiting up to 5m0s for pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e" in namespace "security-context-test-9772" to be "Succeeded or Failed"
Aug 25 03:51:49.779: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.424701ms
Aug 25 03:51:51.784: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008430307s
Aug 25 03:51:53.785: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009065663s
Aug 25 03:51:53.785: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 25 03:51:53.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9772" for this suite. 08/25/22 03:51:53.79
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3644,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.059 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:49.736
    Aug 25 03:51:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename security-context-test 08/25/22 03:51:49.738
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:49.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:49.752
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Aug 25 03:51:49.776: INFO: Waiting up to 5m0s for pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e" in namespace "security-context-test-9772" to be "Succeeded or Failed"
    Aug 25 03:51:49.779: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.424701ms
    Aug 25 03:51:51.784: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008430307s
    Aug 25 03:51:53.785: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009065663s
    Aug 25 03:51:53.785: INFO: Pod "busybox-user-65534-768a48e3-e66b-4dfd-8c72-ec8a8cec669e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 25 03:51:53.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9772" for this suite. 08/25/22 03:51:53.79
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:51:53.797
Aug 25 03:51:53.797: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:51:53.798
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:53.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:53.814
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Aug 25 03:51:53.820: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/25/22 03:51:57.969
Aug 25 03:51:57.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 create -f -'
Aug 25 03:51:58.757: INFO: stderr: ""
Aug 25 03:51:58.757: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 25 03:51:58.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 delete e2e-test-crd-publish-openapi-6471-crds test-cr'
Aug 25 03:51:58.840: INFO: stderr: ""
Aug 25 03:51:58.840: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 25 03:51:58.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 apply -f -'
Aug 25 03:51:59.113: INFO: stderr: ""
Aug 25 03:51:59.113: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 25 03:51:59.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 delete e2e-test-crd-publish-openapi-6471-crds test-cr'
Aug 25 03:51:59.208: INFO: stderr: ""
Aug 25 03:51:59.208: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/25/22 03:51:59.208
Aug 25 03:51:59.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 explain e2e-test-crd-publish-openapi-6471-crds'
Aug 25 03:51:59.436: INFO: stderr: ""
Aug 25 03:51:59.436: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6471-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:52:03.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4295" for this suite. 08/25/22 03:52:03.575
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":192,"skipped":3653,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [9.783 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:51:53.797
    Aug 25 03:51:53.797: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 03:51:53.798
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:51:53.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:51:53.814
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Aug 25 03:51:53.820: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/25/22 03:51:57.969
    Aug 25 03:51:57.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 create -f -'
    Aug 25 03:51:58.757: INFO: stderr: ""
    Aug 25 03:51:58.757: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 25 03:51:58.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 delete e2e-test-crd-publish-openapi-6471-crds test-cr'
    Aug 25 03:51:58.840: INFO: stderr: ""
    Aug 25 03:51:58.840: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 25 03:51:58.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 apply -f -'
    Aug 25 03:51:59.113: INFO: stderr: ""
    Aug 25 03:51:59.113: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 25 03:51:59.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 --namespace=crd-publish-openapi-4295 delete e2e-test-crd-publish-openapi-6471-crds test-cr'
    Aug 25 03:51:59.208: INFO: stderr: ""
    Aug 25 03:51:59.208: INFO: stdout: "e2e-test-crd-publish-openapi-6471-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/25/22 03:51:59.208
    Aug 25 03:51:59.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-4295 explain e2e-test-crd-publish-openapi-6471-crds'
    Aug 25 03:51:59.436: INFO: stderr: ""
    Aug 25 03:51:59.436: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6471-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:52:03.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4295" for this suite. 08/25/22 03:52:03.575
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:52:03.583
Aug 25 03:52:03.583: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 03:52:03.584
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:52:03.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:52:03.6
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5302 08/25/22 03:52:03.603
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 08/25/22 03:52:03.607
Aug 25 03:52:03.616: INFO: Found 0 stateful pods, waiting for 3
Aug 25 03:52:13.622: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:52:13.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:52:13.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/25/22 03:52:13.634
Aug 25 03:52:13.654: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/25/22 03:52:13.654
STEP: Not applying an update when the partition is greater than the number of replicas 08/25/22 03:52:23.671
STEP: Performing a canary update 08/25/22 03:52:23.671
Aug 25 03:52:23.692: INFO: Updating stateful set ss2
Aug 25 03:52:23.700: INFO: Waiting for Pod statefulset-5302/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 08/25/22 03:52:33.712
Aug 25 03:52:33.732: INFO: Found 1 stateful pods, waiting for 3
Aug 25 03:52:43.741: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:52:43.741: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:52:43.741: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/25/22 03:52:43.749
Aug 25 03:52:43.770: INFO: Updating stateful set ss2
Aug 25 03:52:43.777: INFO: Waiting for Pod statefulset-5302/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Aug 25 03:52:53.809: INFO: Updating stateful set ss2
Aug 25 03:52:53.817: INFO: Waiting for StatefulSet statefulset-5302/ss2 to complete update
Aug 25 03:52:53.817: INFO: Waiting for Pod statefulset-5302/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 03:53:03.830: INFO: Deleting all statefulset in ns statefulset-5302
Aug 25 03:53:03.834: INFO: Scaling statefulset ss2 to 0
Aug 25 03:53:13.855: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:53:13.858: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 03:53:13.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5302" for this suite. 08/25/22 03:53:13.873
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":193,"skipped":3673,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [70.295 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:52:03.583
    Aug 25 03:52:03.583: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 03:52:03.584
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:52:03.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:52:03.6
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5302 08/25/22 03:52:03.603
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 08/25/22 03:52:03.607
    Aug 25 03:52:03.616: INFO: Found 0 stateful pods, waiting for 3
    Aug 25 03:52:13.622: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:52:13.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:52:13.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/25/22 03:52:13.634
    Aug 25 03:52:13.654: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/25/22 03:52:13.654
    STEP: Not applying an update when the partition is greater than the number of replicas 08/25/22 03:52:23.671
    STEP: Performing a canary update 08/25/22 03:52:23.671
    Aug 25 03:52:23.692: INFO: Updating stateful set ss2
    Aug 25 03:52:23.700: INFO: Waiting for Pod statefulset-5302/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 08/25/22 03:52:33.712
    Aug 25 03:52:33.732: INFO: Found 1 stateful pods, waiting for 3
    Aug 25 03:52:43.741: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:52:43.741: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:52:43.741: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/25/22 03:52:43.749
    Aug 25 03:52:43.770: INFO: Updating stateful set ss2
    Aug 25 03:52:43.777: INFO: Waiting for Pod statefulset-5302/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Aug 25 03:52:53.809: INFO: Updating stateful set ss2
    Aug 25 03:52:53.817: INFO: Waiting for StatefulSet statefulset-5302/ss2 to complete update
    Aug 25 03:52:53.817: INFO: Waiting for Pod statefulset-5302/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 03:53:03.830: INFO: Deleting all statefulset in ns statefulset-5302
    Aug 25 03:53:03.834: INFO: Scaling statefulset ss2 to 0
    Aug 25 03:53:13.855: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:53:13.858: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 03:53:13.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5302" for this suite. 08/25/22 03:53:13.873
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:13.877
Aug 25 03:53:13.878: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:53:13.879
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:13.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:13.892
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 08/25/22 03:53:13.9
STEP: watching for the Service to be added 08/25/22 03:53:13.907
Aug 25 03:53:13.910: INFO: Found Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 25 03:53:13.910: INFO: Service test-service-flnp7 created
STEP: Getting /status 08/25/22 03:53:13.91
Aug 25 03:53:13.914: INFO: Service test-service-flnp7 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/25/22 03:53:13.914
STEP: watching for the Service to be patched 08/25/22 03:53:13.918
Aug 25 03:53:13.922: INFO: observed Service test-service-flnp7 in namespace services-3237 with annotations: map[] & LoadBalancer: {[]}
Aug 25 03:53:13.922: INFO: Found Service test-service-flnp7 in namespace services-3237 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 25 03:53:13.922: INFO: Service test-service-flnp7 has service status patched
STEP: updating the ServiceStatus 08/25/22 03:53:13.922
Aug 25 03:53:13.931: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/25/22 03:53:13.931
Aug 25 03:53:13.933: INFO: Observed Service test-service-flnp7 in namespace services-3237 with annotations: map[] & Conditions: {[]}
Aug 25 03:53:13.933: INFO: Observed event: &Service{ObjectMeta:{test-service-flnp7  services-3237  7b37f189-89ef-42be-8731-8f1e1cd291ac 38743 0 2022-08-25 03:53:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.102.26,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.102.26],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 25 03:53:13.933: INFO: Observed event: &Service{ObjectMeta:{test-service-flnp7  services-3237  7b37f189-89ef-42be-8731-8f1e1cd291ac 38744 0 2022-08-25 03:53:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.102.26,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.102.26],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
Aug 25 03:53:13.933: INFO: Found Service test-service-flnp7 in namespace services-3237 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 25 03:53:13.933: INFO: Service test-service-flnp7 has service status updated
STEP: patching the service 08/25/22 03:53:13.933
STEP: watching for the Service to be patched 08/25/22 03:53:13.94
Aug 25 03:53:13.942: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
Aug 25 03:53:13.942: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
Aug 25 03:53:13.942: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
Aug 25 03:53:13.943: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
Aug 25 03:53:13.943: INFO: Found Service test-service-flnp7 in namespace services-3237 with labels: map[test-service:patched test-service-static:true]
Aug 25 03:53:13.943: INFO: Service test-service-flnp7 patched
STEP: deleting the service 08/25/22 03:53:13.943
STEP: watching for the Service to be deleted 08/25/22 03:53:13.952
Aug 25 03:53:13.954: INFO: Observed event: ADDED
Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
Aug 25 03:53:13.954: INFO: Found Service test-service-flnp7 in namespace services-3237 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 25 03:53:13.954: INFO: Service test-service-flnp7 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:53:13.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3237" for this suite. 08/25/22 03:53:13.957
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":194,"skipped":3673,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.084 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:13.877
    Aug 25 03:53:13.878: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:53:13.879
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:13.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:13.892
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 08/25/22 03:53:13.9
    STEP: watching for the Service to be added 08/25/22 03:53:13.907
    Aug 25 03:53:13.910: INFO: Found Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 25 03:53:13.910: INFO: Service test-service-flnp7 created
    STEP: Getting /status 08/25/22 03:53:13.91
    Aug 25 03:53:13.914: INFO: Service test-service-flnp7 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/25/22 03:53:13.914
    STEP: watching for the Service to be patched 08/25/22 03:53:13.918
    Aug 25 03:53:13.922: INFO: observed Service test-service-flnp7 in namespace services-3237 with annotations: map[] & LoadBalancer: {[]}
    Aug 25 03:53:13.922: INFO: Found Service test-service-flnp7 in namespace services-3237 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 25 03:53:13.922: INFO: Service test-service-flnp7 has service status patched
    STEP: updating the ServiceStatus 08/25/22 03:53:13.922
    Aug 25 03:53:13.931: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/25/22 03:53:13.931
    Aug 25 03:53:13.933: INFO: Observed Service test-service-flnp7 in namespace services-3237 with annotations: map[] & Conditions: {[]}
    Aug 25 03:53:13.933: INFO: Observed event: &Service{ObjectMeta:{test-service-flnp7  services-3237  7b37f189-89ef-42be-8731-8f1e1cd291ac 38743 0 2022-08-25 03:53:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.102.26,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.102.26],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 25 03:53:13.933: INFO: Observed event: &Service{ObjectMeta:{test-service-flnp7  services-3237  7b37f189-89ef-42be-8731-8f1e1cd291ac 38744 0 2022-08-25 03:53:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-25 03:53:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.102.26,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.102.26],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
    Aug 25 03:53:13.933: INFO: Found Service test-service-flnp7 in namespace services-3237 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 25 03:53:13.933: INFO: Service test-service-flnp7 has service status updated
    STEP: patching the service 08/25/22 03:53:13.933
    STEP: watching for the Service to be patched 08/25/22 03:53:13.94
    Aug 25 03:53:13.942: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
    Aug 25 03:53:13.942: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
    Aug 25 03:53:13.942: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
    Aug 25 03:53:13.943: INFO: observed Service test-service-flnp7 in namespace services-3237 with labels: map[test-service-static:true]
    Aug 25 03:53:13.943: INFO: Found Service test-service-flnp7 in namespace services-3237 with labels: map[test-service:patched test-service-static:true]
    Aug 25 03:53:13.943: INFO: Service test-service-flnp7 patched
    STEP: deleting the service 08/25/22 03:53:13.943
    STEP: watching for the Service to be deleted 08/25/22 03:53:13.952
    Aug 25 03:53:13.954: INFO: Observed event: ADDED
    Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
    Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
    Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
    Aug 25 03:53:13.954: INFO: Observed event: MODIFIED
    Aug 25 03:53:13.954: INFO: Found Service test-service-flnp7 in namespace services-3237 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 25 03:53:13.954: INFO: Service test-service-flnp7 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:53:13.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3237" for this suite. 08/25/22 03:53:13.957
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:13.962
Aug 25 03:53:13.962: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename ingressclass 08/25/22 03:53:13.963
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:13.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:13.977
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/25/22 03:53:13.981
STEP: getting /apis/networking.k8s.io 08/25/22 03:53:13.984
STEP: getting /apis/networking.k8s.iov1 08/25/22 03:53:13.985
STEP: creating 08/25/22 03:53:13.986
STEP: getting 08/25/22 03:53:13.997
STEP: listing 08/25/22 03:53:13.999
STEP: watching 08/25/22 03:53:14.002
Aug 25 03:53:14.002: INFO: starting watch
STEP: patching 08/25/22 03:53:14.004
STEP: updating 08/25/22 03:53:14.009
Aug 25 03:53:14.014: INFO: waiting for watch events with expected annotations
Aug 25 03:53:14.014: INFO: saw patched and updated annotations
STEP: deleting 08/25/22 03:53:14.014
STEP: deleting a collection 08/25/22 03:53:14.023
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Aug 25 03:53:14.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-814" for this suite. 08/25/22 03:53:14.036
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":195,"skipped":3681,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.079 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:13.962
    Aug 25 03:53:13.962: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename ingressclass 08/25/22 03:53:13.963
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:13.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:13.977
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/25/22 03:53:13.981
    STEP: getting /apis/networking.k8s.io 08/25/22 03:53:13.984
    STEP: getting /apis/networking.k8s.iov1 08/25/22 03:53:13.985
    STEP: creating 08/25/22 03:53:13.986
    STEP: getting 08/25/22 03:53:13.997
    STEP: listing 08/25/22 03:53:13.999
    STEP: watching 08/25/22 03:53:14.002
    Aug 25 03:53:14.002: INFO: starting watch
    STEP: patching 08/25/22 03:53:14.004
    STEP: updating 08/25/22 03:53:14.009
    Aug 25 03:53:14.014: INFO: waiting for watch events with expected annotations
    Aug 25 03:53:14.014: INFO: saw patched and updated annotations
    STEP: deleting 08/25/22 03:53:14.014
    STEP: deleting a collection 08/25/22 03:53:14.023
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Aug 25 03:53:14.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-814" for this suite. 08/25/22 03:53:14.036
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:14.046
Aug 25 03:53:14.046: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 03:53:14.047
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:14.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:14.06
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-4e55c8f8-85b6-4689-bb0e-4d5861481b41 08/25/22 03:53:14.063
STEP: Creating a pod to test consume secrets 08/25/22 03:53:14.067
Aug 25 03:53:14.072: INFO: Waiting up to 5m0s for pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804" in namespace "secrets-3932" to be "Succeeded or Failed"
Aug 25 03:53:14.075: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20034ms
Aug 25 03:53:16.079: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00700299s
Aug 25 03:53:18.081: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008391869s
STEP: Saw pod success 08/25/22 03:53:18.081
Aug 25 03:53:18.081: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804" satisfied condition "Succeeded or Failed"
Aug 25 03:53:18.085: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:53:18.091
Aug 25 03:53:18.099: INFO: Waiting for pod pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804 to disappear
Aug 25 03:53:18.102: INFO: Pod pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 03:53:18.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3932" for this suite. 08/25/22 03:53:18.106
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":196,"skipped":3777,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.065 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:14.046
    Aug 25 03:53:14.046: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 03:53:14.047
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:14.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:14.06
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-4e55c8f8-85b6-4689-bb0e-4d5861481b41 08/25/22 03:53:14.063
    STEP: Creating a pod to test consume secrets 08/25/22 03:53:14.067
    Aug 25 03:53:14.072: INFO: Waiting up to 5m0s for pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804" in namespace "secrets-3932" to be "Succeeded or Failed"
    Aug 25 03:53:14.075: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20034ms
    Aug 25 03:53:16.079: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00700299s
    Aug 25 03:53:18.081: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008391869s
    STEP: Saw pod success 08/25/22 03:53:18.081
    Aug 25 03:53:18.081: INFO: Pod "pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804" satisfied condition "Succeeded or Failed"
    Aug 25 03:53:18.085: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:53:18.091
    Aug 25 03:53:18.099: INFO: Waiting for pod pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804 to disappear
    Aug 25 03:53:18.102: INFO: Pod pod-secrets-22971cb0-26dd-4ed0-92e5-dfe148485804 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 03:53:18.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3932" for this suite. 08/25/22 03:53:18.106
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:18.112
Aug 25 03:53:18.112: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:53:18.113
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:18.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:18.129
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 08/25/22 03:53:18.133
Aug 25 03:53:18.139: INFO: Waiting up to 5m0s for pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba" in namespace "pods-9279" to be "running and ready"
Aug 25 03:53:18.142: INFO: Pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06247ms
Aug 25 03:53:18.142: INFO: The phase of Pod pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:53:20.147: INFO: Pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.008728631s
Aug 25 03:53:20.148: INFO: The phase of Pod pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba is Running (Ready = true)
Aug 25 03:53:20.148: INFO: Pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba" satisfied condition "running and ready"
Aug 25 03:53:20.156: INFO: Pod pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba has hostIP: 86.109.11.217
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:53:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9279" for this suite. 08/25/22 03:53:20.16
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":197,"skipped":3785,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.053 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:18.112
    Aug 25 03:53:18.112: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:53:18.113
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:18.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:18.129
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 08/25/22 03:53:18.133
    Aug 25 03:53:18.139: INFO: Waiting up to 5m0s for pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba" in namespace "pods-9279" to be "running and ready"
    Aug 25 03:53:18.142: INFO: Pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06247ms
    Aug 25 03:53:18.142: INFO: The phase of Pod pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:53:20.147: INFO: Pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.008728631s
    Aug 25 03:53:20.148: INFO: The phase of Pod pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba is Running (Ready = true)
    Aug 25 03:53:20.148: INFO: Pod "pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba" satisfied condition "running and ready"
    Aug 25 03:53:20.156: INFO: Pod pod-hostip-2c69163d-25a7-46ce-a892-9f7d7bc363ba has hostIP: 86.109.11.217
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:53:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9279" for this suite. 08/25/22 03:53:20.16
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:20.166
Aug 25 03:53:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 03:53:20.167
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:20.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:20.183
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 08/25/22 03:53:20.188
Aug 25 03:53:20.195: INFO: Waiting up to 5m0s for pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b" in namespace "var-expansion-9742" to be "Succeeded or Failed"
Aug 25 03:53:20.199: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829111ms
Aug 25 03:53:22.205: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009632456s
Aug 25 03:53:24.205: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009629177s
STEP: Saw pod success 08/25/22 03:53:24.205
Aug 25 03:53:24.205: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b" satisfied condition "Succeeded or Failed"
Aug 25 03:53:24.209: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b container dapi-container: <nil>
STEP: delete the pod 08/25/22 03:53:24.215
Aug 25 03:53:24.223: INFO: Waiting for pod var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b to disappear
Aug 25 03:53:24.226: INFO: Pod var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 03:53:24.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9742" for this suite. 08/25/22 03:53:24.23
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":198,"skipped":3787,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.069 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:20.166
    Aug 25 03:53:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 03:53:20.167
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:20.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:20.183
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 08/25/22 03:53:20.188
    Aug 25 03:53:20.195: INFO: Waiting up to 5m0s for pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b" in namespace "var-expansion-9742" to be "Succeeded or Failed"
    Aug 25 03:53:20.199: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829111ms
    Aug 25 03:53:22.205: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009632456s
    Aug 25 03:53:24.205: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009629177s
    STEP: Saw pod success 08/25/22 03:53:24.205
    Aug 25 03:53:24.205: INFO: Pod "var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b" satisfied condition "Succeeded or Failed"
    Aug 25 03:53:24.209: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b container dapi-container: <nil>
    STEP: delete the pod 08/25/22 03:53:24.215
    Aug 25 03:53:24.223: INFO: Waiting for pod var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b to disappear
    Aug 25 03:53:24.226: INFO: Pod var-expansion-9997b4f7-6565-4151-ba84-fc5c68b91f4b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 03:53:24.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9742" for this suite. 08/25/22 03:53:24.23
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:24.237
Aug 25 03:53:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 03:53:24.238
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:24.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:24.254
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/25/22 03:53:24.261
STEP: delete the rc 08/25/22 03:53:29.268
STEP: wait for the rc to be deleted 08/25/22 03:53:29.272
Aug 25 03:53:30.285: INFO: 35 pods remaining
Aug 25 03:53:30.285: INFO: 35 pods has nil DeletionTimestamp
Aug 25 03:53:30.285: INFO: 
Aug 25 03:53:31.286: INFO: 26 pods remaining
Aug 25 03:53:31.286: INFO: 25 pods has nil DeletionTimestamp
Aug 25 03:53:31.286: INFO: 
Aug 25 03:53:32.284: INFO: 15 pods remaining
Aug 25 03:53:32.284: INFO: 15 pods has nil DeletionTimestamp
Aug 25 03:53:32.284: INFO: 
STEP: Gathering metrics 08/25/22 03:53:33.28
Aug 25 03:53:33.292: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
Aug 25 03:53:33.297: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.68213ms
Aug 25 03:53:33.297: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
Aug 25 03:53:33.297: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
Aug 25 03:53:33.449: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 03:53:33.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-641" for this suite. 08/25/22 03:53:33.452
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":199,"skipped":3821,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [9.220 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:24.237
    Aug 25 03:53:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 03:53:24.238
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:24.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:24.254
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/25/22 03:53:24.261
    STEP: delete the rc 08/25/22 03:53:29.268
    STEP: wait for the rc to be deleted 08/25/22 03:53:29.272
    Aug 25 03:53:30.285: INFO: 35 pods remaining
    Aug 25 03:53:30.285: INFO: 35 pods has nil DeletionTimestamp
    Aug 25 03:53:30.285: INFO: 
    Aug 25 03:53:31.286: INFO: 26 pods remaining
    Aug 25 03:53:31.286: INFO: 25 pods has nil DeletionTimestamp
    Aug 25 03:53:31.286: INFO: 
    Aug 25 03:53:32.284: INFO: 15 pods remaining
    Aug 25 03:53:32.284: INFO: 15 pods has nil DeletionTimestamp
    Aug 25 03:53:32.284: INFO: 
    STEP: Gathering metrics 08/25/22 03:53:33.28
    Aug 25 03:53:33.292: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
    Aug 25 03:53:33.297: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 4.68213ms
    Aug 25 03:53:33.297: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
    Aug 25 03:53:33.297: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
    Aug 25 03:53:33.449: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 03:53:33.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-641" for this suite. 08/25/22 03:53:33.452
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:53:33.458
Aug 25 03:53:33.458: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:53:33.459
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:33.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:33.475
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:53:33.487
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:53:33.83
STEP: Deploying the webhook pod 08/25/22 03:53:33.84
STEP: Wait for the deployment to be ready 08/25/22 03:53:33.846
Aug 25 03:53:33.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 03:53:35.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:37.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:39.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:41.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:43.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:45.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:47.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:49.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:51.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:53.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:55.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:53:57.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:53:59.868
STEP: Verifying the service has paired with the endpoint 08/25/22 03:53:59.878
Aug 25 03:54:00.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 08/25/22 03:54:00.882
STEP: create a pod that should be denied by the webhook 08/25/22 03:54:00.901
STEP: create a pod that causes the webhook to hang 08/25/22 03:54:00.915
STEP: create a configmap that should be denied by the webhook 08/25/22 03:54:10.923
STEP: create a configmap that should be admitted by the webhook 08/25/22 03:54:10.937
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/25/22 03:54:10.95
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/25/22 03:54:10.958
STEP: create a namespace that bypass the webhook 08/25/22 03:54:10.963
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/25/22 03:54:10.968
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:54:10.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2448" for this suite. 08/25/22 03:54:10.995
STEP: Destroying namespace "webhook-2448-markers" for this suite. 08/25/22 03:54:10.998
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":200,"skipped":3830,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [37.567 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:53:33.458
    Aug 25 03:53:33.458: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:53:33.459
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:53:33.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:53:33.475
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:53:33.487
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:53:33.83
    STEP: Deploying the webhook pod 08/25/22 03:53:33.84
    STEP: Wait for the deployment to be ready 08/25/22 03:53:33.846
    Aug 25 03:53:33.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 03:53:35.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:37.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:39.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:41.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:43.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:45.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:47.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:49.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:51.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:53.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:55.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:53:57.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 53, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:53:59.868
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:53:59.878
    Aug 25 03:54:00.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 08/25/22 03:54:00.882
    STEP: create a pod that should be denied by the webhook 08/25/22 03:54:00.901
    STEP: create a pod that causes the webhook to hang 08/25/22 03:54:00.915
    STEP: create a configmap that should be denied by the webhook 08/25/22 03:54:10.923
    STEP: create a configmap that should be admitted by the webhook 08/25/22 03:54:10.937
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/25/22 03:54:10.95
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/25/22 03:54:10.958
    STEP: create a namespace that bypass the webhook 08/25/22 03:54:10.963
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/25/22 03:54:10.968
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:54:10.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2448" for this suite. 08/25/22 03:54:10.995
    STEP: Destroying namespace "webhook-2448-markers" for this suite. 08/25/22 03:54:10.998
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:11.026
Aug 25 03:54:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:54:11.028
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:11.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:11.04
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-1629 08/25/22 03:54:11.043
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[] 08/25/22 03:54:11.054
Aug 25 03:54:11.061: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1629 08/25/22 03:54:11.061
Aug 25 03:54:11.066: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1629" to be "running and ready"
Aug 25 03:54:11.068: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472995ms
Aug 25 03:54:11.068: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:54:13.074: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007564178s
Aug 25 03:54:13.074: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 25 03:54:13.074: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[pod1:[80]] 08/25/22 03:54:13.078
Aug 25 03:54:13.091: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/25/22 03:54:13.091
Aug 25 03:54:13.091: INFO: Creating new exec pod
Aug 25 03:54:13.095: INFO: Waiting up to 5m0s for pod "execpodhfzs5" in namespace "services-1629" to be "running"
Aug 25 03:54:13.100: INFO: Pod "execpodhfzs5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474113ms
Aug 25 03:54:15.106: INFO: Pod "execpodhfzs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010723553s
Aug 25 03:54:15.106: INFO: Pod "execpodhfzs5" satisfied condition "running"
Aug 25 03:54:16.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 25 03:54:16.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 25 03:54:16.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:54:16.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.207.197 80'
Aug 25 03:54:16.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.207.197 80\nConnection to 10.98.207.197 80 port [tcp/http] succeeded!\n"
Aug 25 03:54:16.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1629 08/25/22 03:54:16.476
Aug 25 03:54:16.482: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1629" to be "running and ready"
Aug 25 03:54:16.486: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.933037ms
Aug 25 03:54:16.486: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:54:18.491: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008557153s
Aug 25 03:54:18.491: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 25 03:54:18.491: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[pod1:[80] pod2:[80]] 08/25/22 03:54:18.494
Aug 25 03:54:18.510: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/25/22 03:54:18.51
Aug 25 03:54:19.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 25 03:54:19.729: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 25 03:54:19.729: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:54:19.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.207.197 80'
Aug 25 03:54:19.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.207.197 80\nConnection to 10.98.207.197 80 port [tcp/http] succeeded!\n"
Aug 25 03:54:19.913: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1629 08/25/22 03:54:19.913
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[pod2:[80]] 08/25/22 03:54:19.921
Aug 25 03:54:19.934: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/25/22 03:54:19.934
Aug 25 03:54:20.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 25 03:54:21.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 25 03:54:21.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:54:21.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.207.197 80'
Aug 25 03:54:21.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.207.197 80\nConnection to 10.98.207.197 80 port [tcp/http] succeeded!\n"
Aug 25 03:54:21.315: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1629 08/25/22 03:54:21.315
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[] 08/25/22 03:54:21.322
Aug 25 03:54:21.331: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:54:21.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1629" for this suite. 08/25/22 03:54:21.344
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":201,"skipped":3834,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.322 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:11.026
    Aug 25 03:54:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:54:11.028
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:11.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:11.04
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-1629 08/25/22 03:54:11.043
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[] 08/25/22 03:54:11.054
    Aug 25 03:54:11.061: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1629 08/25/22 03:54:11.061
    Aug 25 03:54:11.066: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1629" to be "running and ready"
    Aug 25 03:54:11.068: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472995ms
    Aug 25 03:54:11.068: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:54:13.074: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007564178s
    Aug 25 03:54:13.074: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 25 03:54:13.074: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[pod1:[80]] 08/25/22 03:54:13.078
    Aug 25 03:54:13.091: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/25/22 03:54:13.091
    Aug 25 03:54:13.091: INFO: Creating new exec pod
    Aug 25 03:54:13.095: INFO: Waiting up to 5m0s for pod "execpodhfzs5" in namespace "services-1629" to be "running"
    Aug 25 03:54:13.100: INFO: Pod "execpodhfzs5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474113ms
    Aug 25 03:54:15.106: INFO: Pod "execpodhfzs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010723553s
    Aug 25 03:54:15.106: INFO: Pod "execpodhfzs5" satisfied condition "running"
    Aug 25 03:54:16.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Aug 25 03:54:16.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 25 03:54:16.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:54:16.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.207.197 80'
    Aug 25 03:54:16.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.207.197 80\nConnection to 10.98.207.197 80 port [tcp/http] succeeded!\n"
    Aug 25 03:54:16.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-1629 08/25/22 03:54:16.476
    Aug 25 03:54:16.482: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1629" to be "running and ready"
    Aug 25 03:54:16.486: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.933037ms
    Aug 25 03:54:16.486: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:54:18.491: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008557153s
    Aug 25 03:54:18.491: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 25 03:54:18.491: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[pod1:[80] pod2:[80]] 08/25/22 03:54:18.494
    Aug 25 03:54:18.510: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/25/22 03:54:18.51
    Aug 25 03:54:19.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Aug 25 03:54:19.729: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 25 03:54:19.729: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:54:19.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.207.197 80'
    Aug 25 03:54:19.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.207.197 80\nConnection to 10.98.207.197 80 port [tcp/http] succeeded!\n"
    Aug 25 03:54:19.913: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1629 08/25/22 03:54:19.913
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[pod2:[80]] 08/25/22 03:54:19.921
    Aug 25 03:54:19.934: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/25/22 03:54:19.934
    Aug 25 03:54:20.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Aug 25 03:54:21.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 25 03:54:21.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:54:21.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-1629 exec execpodhfzs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.207.197 80'
    Aug 25 03:54:21.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.207.197 80\nConnection to 10.98.207.197 80 port [tcp/http] succeeded!\n"
    Aug 25 03:54:21.315: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-1629 08/25/22 03:54:21.315
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1629 to expose endpoints map[] 08/25/22 03:54:21.322
    Aug 25 03:54:21.331: INFO: successfully validated that service endpoint-test2 in namespace services-1629 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:54:21.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1629" for this suite. 08/25/22 03:54:21.344
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:21.348
Aug 25 03:54:21.348: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename runtimeclass 08/25/22 03:54:21.349
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:21.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:21.362
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/25/22 03:54:21.365
STEP: getting /apis/node.k8s.io 08/25/22 03:54:21.368
STEP: getting /apis/node.k8s.io/v1 08/25/22 03:54:21.369
STEP: creating 08/25/22 03:54:21.37
STEP: watching 08/25/22 03:54:21.388
Aug 25 03:54:21.388: INFO: starting watch
STEP: getting 08/25/22 03:54:21.393
STEP: listing 08/25/22 03:54:21.396
STEP: patching 08/25/22 03:54:21.399
STEP: updating 08/25/22 03:54:21.403
Aug 25 03:54:21.407: INFO: waiting for watch events with expected annotations
STEP: deleting 08/25/22 03:54:21.408
STEP: deleting a collection 08/25/22 03:54:21.418
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 25 03:54:21.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-61" for this suite. 08/25/22 03:54:21.433
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":202,"skipped":3843,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.089 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:21.348
    Aug 25 03:54:21.348: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename runtimeclass 08/25/22 03:54:21.349
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:21.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:21.362
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/25/22 03:54:21.365
    STEP: getting /apis/node.k8s.io 08/25/22 03:54:21.368
    STEP: getting /apis/node.k8s.io/v1 08/25/22 03:54:21.369
    STEP: creating 08/25/22 03:54:21.37
    STEP: watching 08/25/22 03:54:21.388
    Aug 25 03:54:21.388: INFO: starting watch
    STEP: getting 08/25/22 03:54:21.393
    STEP: listing 08/25/22 03:54:21.396
    STEP: patching 08/25/22 03:54:21.399
    STEP: updating 08/25/22 03:54:21.403
    Aug 25 03:54:21.407: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/25/22 03:54:21.408
    STEP: deleting a collection 08/25/22 03:54:21.418
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 25 03:54:21.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-61" for this suite. 08/25/22 03:54:21.433
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:21.438
Aug 25 03:54:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 03:54:21.44
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:21.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:21.455
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 03:54:21.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1144" for this suite. 08/25/22 03:54:21.491
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":203,"skipped":3844,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.055 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:21.438
    Aug 25 03:54:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 03:54:21.44
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:21.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:21.455
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 03:54:21.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1144" for this suite. 08/25/22 03:54:21.491
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:21.494
Aug 25 03:54:21.495: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:54:21.495
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:21.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:21.505
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 08/25/22 03:54:21.508
STEP: submitting the pod to kubernetes 08/25/22 03:54:21.508
Aug 25 03:54:21.513: INFO: Waiting up to 5m0s for pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" in namespace "pods-9112" to be "running and ready"
Aug 25 03:54:21.515: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.889997ms
Aug 25 03:54:21.515: INFO: The phase of Pod pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:54:23.520: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006639726s
Aug 25 03:54:23.520: INFO: The phase of Pod pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8 is Running (Ready = true)
Aug 25 03:54:23.520: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/25/22 03:54:23.524
STEP: updating the pod 08/25/22 03:54:23.528
Aug 25 03:54:24.040: INFO: Successfully updated pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8"
Aug 25 03:54:24.040: INFO: Waiting up to 5m0s for pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" in namespace "pods-9112" to be "running"
Aug 25 03:54:24.044: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8": Phase="Running", Reason="", readiness=true. Elapsed: 3.589049ms
Aug 25 03:54:24.044: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/25/22 03:54:24.044
Aug 25 03:54:24.048: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:54:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9112" for this suite. 08/25/22 03:54:24.052
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":204,"skipped":3869,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.562 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:21.494
    Aug 25 03:54:21.495: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:54:21.495
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:21.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:21.505
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 08/25/22 03:54:21.508
    STEP: submitting the pod to kubernetes 08/25/22 03:54:21.508
    Aug 25 03:54:21.513: INFO: Waiting up to 5m0s for pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" in namespace "pods-9112" to be "running and ready"
    Aug 25 03:54:21.515: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.889997ms
    Aug 25 03:54:21.515: INFO: The phase of Pod pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:54:23.520: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006639726s
    Aug 25 03:54:23.520: INFO: The phase of Pod pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8 is Running (Ready = true)
    Aug 25 03:54:23.520: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/25/22 03:54:23.524
    STEP: updating the pod 08/25/22 03:54:23.528
    Aug 25 03:54:24.040: INFO: Successfully updated pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8"
    Aug 25 03:54:24.040: INFO: Waiting up to 5m0s for pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" in namespace "pods-9112" to be "running"
    Aug 25 03:54:24.044: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8": Phase="Running", Reason="", readiness=true. Elapsed: 3.589049ms
    Aug 25 03:54:24.044: INFO: Pod "pod-update-92ee5b0c-2f0c-48ed-b98c-8cc53305afe8" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/25/22 03:54:24.044
    Aug 25 03:54:24.048: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:54:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9112" for this suite. 08/25/22 03:54:24.052
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:24.057
Aug 25 03:54:24.057: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:54:24.059
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:24.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:24.075
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:54:24.088
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:54:24.505
STEP: Deploying the webhook pod 08/25/22 03:54:24.513
STEP: Wait for the deployment to be ready 08/25/22 03:54:24.524
Aug 25 03:54:24.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 03:54:26.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:54:28.546
STEP: Verifying the service has paired with the endpoint 08/25/22 03:54:28.556
Aug 25 03:54:29.557: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Aug 25 03:54:29.561: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/25/22 03:54:30.073
STEP: Creating a custom resource that should be denied by the webhook 08/25/22 03:54:30.092
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/25/22 03:54:32.138
STEP: Updating the custom resource with disallowed data should be denied 08/25/22 03:54:32.148
STEP: Deleting the custom resource should be denied 08/25/22 03:54:32.158
STEP: Remove the offending key and value from the custom resource data 08/25/22 03:54:32.165
STEP: Deleting the updated custom resource should be successful 08/25/22 03:54:32.174
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:54:32.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2779" for this suite. 08/25/22 03:54:32.697
STEP: Destroying namespace "webhook-2779-markers" for this suite. 08/25/22 03:54:32.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":205,"skipped":3870,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.675 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:24.057
    Aug 25 03:54:24.057: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:54:24.059
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:24.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:24.075
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:54:24.088
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:54:24.505
    STEP: Deploying the webhook pod 08/25/22 03:54:24.513
    STEP: Wait for the deployment to be ready 08/25/22 03:54:24.524
    Aug 25 03:54:24.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 03:54:26.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:54:28.546
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:54:28.556
    Aug 25 03:54:29.557: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Aug 25 03:54:29.561: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/25/22 03:54:30.073
    STEP: Creating a custom resource that should be denied by the webhook 08/25/22 03:54:30.092
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/25/22 03:54:32.138
    STEP: Updating the custom resource with disallowed data should be denied 08/25/22 03:54:32.148
    STEP: Deleting the custom resource should be denied 08/25/22 03:54:32.158
    STEP: Remove the offending key and value from the custom resource data 08/25/22 03:54:32.165
    STEP: Deleting the updated custom resource should be successful 08/25/22 03:54:32.174
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:54:32.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2779" for this suite. 08/25/22 03:54:32.697
    STEP: Destroying namespace "webhook-2779-markers" for this suite. 08/25/22 03:54:32.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:32.732
Aug 25 03:54:32.732: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 03:54:32.733
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:32.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:32.747
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 08/25/22 03:54:32.75
Aug 25 03:54:32.755: INFO: Waiting up to 5m0s for pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df" in namespace "downward-api-2792" to be "running and ready"
Aug 25 03:54:32.758: INFO: Pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886538ms
Aug 25 03:54:32.758: INFO: The phase of Pod annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:54:34.763: INFO: Pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df": Phase="Running", Reason="", readiness=true. Elapsed: 2.007746981s
Aug 25 03:54:34.763: INFO: The phase of Pod annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df is Running (Ready = true)
Aug 25 03:54:34.763: INFO: Pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df" satisfied condition "running and ready"
Aug 25 03:54:35.285: INFO: Successfully updated pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 03:54:39.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2792" for this suite. 08/25/22 03:54:39.31
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":206,"skipped":3870,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.583 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:32.732
    Aug 25 03:54:32.732: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 03:54:32.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:32.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:32.747
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 08/25/22 03:54:32.75
    Aug 25 03:54:32.755: INFO: Waiting up to 5m0s for pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df" in namespace "downward-api-2792" to be "running and ready"
    Aug 25 03:54:32.758: INFO: Pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886538ms
    Aug 25 03:54:32.758: INFO: The phase of Pod annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:54:34.763: INFO: Pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df": Phase="Running", Reason="", readiness=true. Elapsed: 2.007746981s
    Aug 25 03:54:34.763: INFO: The phase of Pod annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df is Running (Ready = true)
    Aug 25 03:54:34.763: INFO: Pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df" satisfied condition "running and ready"
    Aug 25 03:54:35.285: INFO: Successfully updated pod "annotationupdated072d4ea-476a-47d3-b058-f26ba15ee5df"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 03:54:39.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2792" for this suite. 08/25/22 03:54:39.31
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:54:39.317
Aug 25 03:54:39.317: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 03:54:39.319
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:39.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:39.339
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 25 03:54:39.345: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 25 03:54:44.361: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/25/22 03:54:44.361
Aug 25 03:54:44.362: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 25 03:54:46.366: INFO: Creating deployment "test-rollover-deployment"
Aug 25 03:54:46.376: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 25 03:54:48.384: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 25 03:54:48.393: INFO: Ensure that both replica sets have 1 created replica
Aug 25 03:54:48.399: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 25 03:54:48.408: INFO: Updating deployment test-rollover-deployment
Aug 25 03:54:48.408: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 25 03:54:50.416: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 25 03:54:50.424: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 25 03:54:50.432: INFO: all replica sets need to contain the pod-template-hash label
Aug 25 03:54:50.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:54:52.443: INFO: all replica sets need to contain the pod-template-hash label
Aug 25 03:54:52.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:54:54.442: INFO: all replica sets need to contain the pod-template-hash label
Aug 25 03:54:54.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:54:56.442: INFO: all replica sets need to contain the pod-template-hash label
Aug 25 03:54:56.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:54:58.442: INFO: all replica sets need to contain the pod-template-hash label
Aug 25 03:54:58.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 25 03:55:00.442: INFO: 
Aug 25 03:55:00.442: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 03:55:00.454: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7461  dc97efaf-f8f6-4312-b11d-f1928f5f22d6 40387 2 2022-08-25 03:54:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070cd498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-25 03:54:46 +0000 UTC,LastTransitionTime:2022-08-25 03:54:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-08-25 03:55:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 25 03:55:00.459: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7461  919bc19f-2ac4-4a09-b923-ae0961dc644e 40378 2 2022-08-25 03:54:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment dc97efaf-f8f6-4312-b11d-f1928f5f22d6 0xc007068e37 0xc007068e38}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc97efaf-f8f6-4312-b11d-f1928f5f22d6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007068ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:55:00.459: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 25 03:55:00.459: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7461  32b2c2bd-a8da-481b-ac74-73ac2a5135a4 40386 2 2022-08-25 03:54:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment dc97efaf-f8f6-4312-b11d-f1928f5f22d6 0xc007068be7 0xc007068be8}] [] [{e2e.test Update apps/v1 2022-08-25 03:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc97efaf-f8f6-4312-b11d-f1928f5f22d6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007068ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:55:00.459: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7461  950d3f65-ba6b-4b4c-bb93-e9a41fbc0a1d 40286 2 2022-08-25 03:54:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment dc97efaf-f8f6-4312-b11d-f1928f5f22d6 0xc007068d17 0xc007068d18}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc97efaf-f8f6-4312-b11d-f1928f5f22d6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007068dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:55:00.463: INFO: Pod "test-rollover-deployment-6d45fd857b-x9nvc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-x9nvc test-rollover-deployment-6d45fd857b- deployment-7461  3bf4e9d0-086d-49e3-8c89-c10cdfc5f9aa 40305 0 2022-08-25 03:54:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 919bc19f-2ac4-4a09-b923-ae0961dc644e 0xc0052fb907 0xc0052fb908}] [] [{kube-controller-manager Update v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"919bc19f-2ac4-4a09-b923-ae0961dc644e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:54:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5vgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5vgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.14,StartTime:2022-08-25 03:54:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:54:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://403c292e1a91041f115f723f5452da214c8668cf4d3de584061ba12dadc15493,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 03:55:00.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7461" for this suite. 08/25/22 03:55:00.467
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":207,"skipped":3898,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [21.154 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:54:39.317
    Aug 25 03:54:39.317: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 03:54:39.319
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:54:39.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:54:39.339
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 25 03:54:39.345: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 25 03:54:44.361: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/25/22 03:54:44.361
    Aug 25 03:54:44.362: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 25 03:54:46.366: INFO: Creating deployment "test-rollover-deployment"
    Aug 25 03:54:46.376: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 25 03:54:48.384: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 25 03:54:48.393: INFO: Ensure that both replica sets have 1 created replica
    Aug 25 03:54:48.399: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 25 03:54:48.408: INFO: Updating deployment test-rollover-deployment
    Aug 25 03:54:48.408: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 25 03:54:50.416: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 25 03:54:50.424: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 25 03:54:50.432: INFO: all replica sets need to contain the pod-template-hash label
    Aug 25 03:54:50.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:54:52.443: INFO: all replica sets need to contain the pod-template-hash label
    Aug 25 03:54:52.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:54:54.442: INFO: all replica sets need to contain the pod-template-hash label
    Aug 25 03:54:54.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:54:56.442: INFO: all replica sets need to contain the pod-template-hash label
    Aug 25 03:54:56.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:54:58.442: INFO: all replica sets need to contain the pod-template-hash label
    Aug 25 03:54:58.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 54, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 25 03:55:00.442: INFO: 
    Aug 25 03:55:00.442: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 03:55:00.454: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7461  dc97efaf-f8f6-4312-b11d-f1928f5f22d6 40387 2 2022-08-25 03:54:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070cd498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-25 03:54:46 +0000 UTC,LastTransitionTime:2022-08-25 03:54:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-08-25 03:55:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 25 03:55:00.459: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7461  919bc19f-2ac4-4a09-b923-ae0961dc644e 40378 2 2022-08-25 03:54:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment dc97efaf-f8f6-4312-b11d-f1928f5f22d6 0xc007068e37 0xc007068e38}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc97efaf-f8f6-4312-b11d-f1928f5f22d6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007068ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:55:00.459: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 25 03:55:00.459: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7461  32b2c2bd-a8da-481b-ac74-73ac2a5135a4 40386 2 2022-08-25 03:54:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment dc97efaf-f8f6-4312-b11d-f1928f5f22d6 0xc007068be7 0xc007068be8}] [] [{e2e.test Update apps/v1 2022-08-25 03:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc97efaf-f8f6-4312-b11d-f1928f5f22d6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:55:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007068ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:55:00.459: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7461  950d3f65-ba6b-4b4c-bb93-e9a41fbc0a1d 40286 2 2022-08-25 03:54:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment dc97efaf-f8f6-4312-b11d-f1928f5f22d6 0xc007068d17 0xc007068d18}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc97efaf-f8f6-4312-b11d-f1928f5f22d6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007068dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:55:00.463: INFO: Pod "test-rollover-deployment-6d45fd857b-x9nvc" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-x9nvc test-rollover-deployment-6d45fd857b- deployment-7461  3bf4e9d0-086d-49e3-8c89-c10cdfc5f9aa 40305 0 2022-08-25 03:54:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 919bc19f-2ac4-4a09-b923-ae0961dc644e 0xc0052fb907 0xc0052fb908}] [] [{kube-controller-manager Update v1 2022-08-25 03:54:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"919bc19f-2ac4-4a09-b923-ae0961dc644e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:54:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5vgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5vgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:54:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.14,StartTime:2022-08-25 03:54:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:54:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://403c292e1a91041f115f723f5452da214c8668cf4d3de584061ba12dadc15493,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 03:55:00.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7461" for this suite. 08/25/22 03:55:00.467
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:55:00.473
Aug 25 03:55:00.473: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:55:00.474
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:55:00.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:55:00.49
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-9878 08/25/22 03:55:00.493
Aug 25 03:55:00.498: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9878" to be "running and ready"
Aug 25 03:55:00.502: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.225691ms
Aug 25 03:55:00.502: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:55:02.514: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015857362s
Aug 25 03:55:02.514: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 25 03:55:02.514: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Aug 25 03:55:02.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 25 03:55:02.707: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 25 03:55:02.707: INFO: stdout: "iptables"
Aug 25 03:55:02.707: INFO: proxyMode: iptables
Aug 25 03:55:02.715: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 25 03:55:02.719: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9878 08/25/22 03:55:02.719
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9878 08/25/22 03:55:02.73
I0825 03:55:02.736025      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9878, replica count: 3
I0825 03:55:05.786799      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 03:55:05.796: INFO: Creating new exec pod
Aug 25 03:55:05.800: INFO: Waiting up to 5m0s for pod "execpod-affinity699qh" in namespace "services-9878" to be "running"
Aug 25 03:55:05.803: INFO: Pod "execpod-affinity699qh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.230327ms
Aug 25 03:55:07.809: INFO: Pod "execpod-affinity699qh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008835159s
Aug 25 03:55:07.809: INFO: Pod "execpod-affinity699qh" satisfied condition "running"
Aug 25 03:55:08.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Aug 25 03:55:09.019: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 25 03:55:09.019: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:55:09.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.20.117 80'
Aug 25 03:55:09.196: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.20.117 80\nConnection to 10.106.20.117 80 port [tcp/http] succeeded!\n"
Aug 25 03:55:09.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:55:09.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 31356'
Aug 25 03:55:09.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 31356\nConnection to 86.109.11.217 31356 port [tcp/*] succeeded!\n"
Aug 25 03:55:09.397: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 03:55:09.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:31356/ ; done'
Aug 25 03:55:09.695: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
Aug 25 03:55:09.695: INFO: stdout: "\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z"
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
Aug 25 03:55:09.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://86.109.11.217:31356/'
Aug 25 03:55:09.888: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
Aug 25 03:55:09.888: INFO: stdout: "affinity-nodeport-timeout-49t2z"
Aug 25 03:55:29.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://86.109.11.217:31356/'
Aug 25 03:55:30.080: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
Aug 25 03:55:30.080: INFO: stdout: "affinity-nodeport-timeout-49t2z"
Aug 25 03:55:50.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://86.109.11.217:31356/'
Aug 25 03:55:50.274: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
Aug 25 03:55:50.274: INFO: stdout: "affinity-nodeport-timeout-2xr74"
Aug 25 03:55:50.274: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9878, will wait for the garbage collector to delete the pods 08/25/22 03:55:50.282
Aug 25 03:55:50.346: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.734728ms
Aug 25 03:55:50.447: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.072944ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:55:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9878" for this suite. 08/25/22 03:55:52.665
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":208,"skipped":3904,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [52.196 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:55:00.473
    Aug 25 03:55:00.473: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:55:00.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:55:00.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:55:00.49
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-9878 08/25/22 03:55:00.493
    Aug 25 03:55:00.498: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9878" to be "running and ready"
    Aug 25 03:55:00.502: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.225691ms
    Aug 25 03:55:00.502: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:55:02.514: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015857362s
    Aug 25 03:55:02.514: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Aug 25 03:55:02.514: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Aug 25 03:55:02.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Aug 25 03:55:02.707: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Aug 25 03:55:02.707: INFO: stdout: "iptables"
    Aug 25 03:55:02.707: INFO: proxyMode: iptables
    Aug 25 03:55:02.715: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Aug 25 03:55:02.719: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-9878 08/25/22 03:55:02.719
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-9878 08/25/22 03:55:02.73
    I0825 03:55:02.736025      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9878, replica count: 3
    I0825 03:55:05.786799      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 03:55:05.796: INFO: Creating new exec pod
    Aug 25 03:55:05.800: INFO: Waiting up to 5m0s for pod "execpod-affinity699qh" in namespace "services-9878" to be "running"
    Aug 25 03:55:05.803: INFO: Pod "execpod-affinity699qh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.230327ms
    Aug 25 03:55:07.809: INFO: Pod "execpod-affinity699qh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008835159s
    Aug 25 03:55:07.809: INFO: Pod "execpod-affinity699qh" satisfied condition "running"
    Aug 25 03:55:08.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Aug 25 03:55:09.019: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Aug 25 03:55:09.019: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:55:09.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.20.117 80'
    Aug 25 03:55:09.196: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.20.117 80\nConnection to 10.106.20.117 80 port [tcp/http] succeeded!\n"
    Aug 25 03:55:09.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:55:09.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 31356'
    Aug 25 03:55:09.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 31356\nConnection to 86.109.11.217 31356 port [tcp/*] succeeded!\n"
    Aug 25 03:55:09.397: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 03:55:09.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:31356/ ; done'
    Aug 25 03:55:09.695: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
    Aug 25 03:55:09.695: INFO: stdout: "\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z\naffinity-nodeport-timeout-49t2z"
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Received response from host: affinity-nodeport-timeout-49t2z
    Aug 25 03:55:09.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://86.109.11.217:31356/'
    Aug 25 03:55:09.888: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
    Aug 25 03:55:09.888: INFO: stdout: "affinity-nodeport-timeout-49t2z"
    Aug 25 03:55:29.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://86.109.11.217:31356/'
    Aug 25 03:55:30.080: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
    Aug 25 03:55:30.080: INFO: stdout: "affinity-nodeport-timeout-49t2z"
    Aug 25 03:55:50.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9878 exec execpod-affinity699qh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://86.109.11.217:31356/'
    Aug 25 03:55:50.274: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://86.109.11.217:31356/\n"
    Aug 25 03:55:50.274: INFO: stdout: "affinity-nodeport-timeout-2xr74"
    Aug 25 03:55:50.274: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9878, will wait for the garbage collector to delete the pods 08/25/22 03:55:50.282
    Aug 25 03:55:50.346: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.734728ms
    Aug 25 03:55:50.447: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.072944ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:55:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9878" for this suite. 08/25/22 03:55:52.665
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:55:52.673
Aug 25 03:55:52.673: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 03:55:52.674
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:55:52.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:55:52.687
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9538 08/25/22 03:55:52.692
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 08/25/22 03:55:52.696
Aug 25 03:55:52.702: INFO: Found 0 stateful pods, waiting for 3
Aug 25 03:56:02.708: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:56:02.708: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:56:02.708: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 03:56:02.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 03:56:02.917: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 03:56:02.917: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 03:56:02.917: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/25/22 03:56:12.936
Aug 25 03:56:12.956: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/25/22 03:56:12.956
STEP: Updating Pods in reverse ordinal order 08/25/22 03:56:22.975
Aug 25 03:56:22.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 03:56:23.173: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 03:56:23.173: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 03:56:23.173: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/25/22 03:56:33.201
Aug 25 03:56:33.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 03:56:33.401: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 03:56:33.401: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 03:56:33.401: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 03:56:43.443: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/25/22 03:56:53.461
Aug 25 03:56:53.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 03:56:53.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 03:56:53.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 03:56:53.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 03:57:03.691: INFO: Deleting all statefulset in ns statefulset-9538
Aug 25 03:57:03.694: INFO: Scaling statefulset ss2 to 0
Aug 25 03:57:13.713: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 03:57:13.716: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 03:57:13.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9538" for this suite. 08/25/22 03:57:13.732
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":209,"skipped":3950,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [81.063 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:55:52.673
    Aug 25 03:55:52.673: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 03:55:52.674
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:55:52.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:55:52.687
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9538 08/25/22 03:55:52.692
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 08/25/22 03:55:52.696
    Aug 25 03:55:52.702: INFO: Found 0 stateful pods, waiting for 3
    Aug 25 03:56:02.708: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:56:02.708: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:56:02.708: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 03:56:02.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 03:56:02.917: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 03:56:02.917: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 03:56:02.917: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/25/22 03:56:12.936
    Aug 25 03:56:12.956: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/25/22 03:56:12.956
    STEP: Updating Pods in reverse ordinal order 08/25/22 03:56:22.975
    Aug 25 03:56:22.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 03:56:23.173: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 03:56:23.173: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 03:56:23.173: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/25/22 03:56:33.201
    Aug 25 03:56:33.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 03:56:33.401: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 03:56:33.401: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 03:56:33.401: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 03:56:43.443: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/25/22 03:56:53.461
    Aug 25 03:56:53.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-9538 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 03:56:53.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 03:56:53.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 03:56:53.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 03:57:03.691: INFO: Deleting all statefulset in ns statefulset-9538
    Aug 25 03:57:03.694: INFO: Scaling statefulset ss2 to 0
    Aug 25 03:57:13.713: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 03:57:13.716: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 03:57:13.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9538" for this suite. 08/25/22 03:57:13.732
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:57:13.738
Aug 25 03:57:13.738: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename watch 08/25/22 03:57:13.74
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:13.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:13.755
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/25/22 03:57:13.759
STEP: starting a background goroutine to produce watch events 08/25/22 03:57:13.761
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/25/22 03:57:13.761
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 25 03:57:16.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1809" for this suite. 08/25/22 03:57:16.595
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":210,"skipped":3972,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:57:13.738
    Aug 25 03:57:13.738: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename watch 08/25/22 03:57:13.74
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:13.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:13.755
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/25/22 03:57:13.759
    STEP: starting a background goroutine to produce watch events 08/25/22 03:57:13.761
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/25/22 03:57:13.761
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 25 03:57:16.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1809" for this suite. 08/25/22 03:57:16.595
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:57:16.648
Aug 25 03:57:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 03:57:16.65
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:16.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:16.665
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 08/25/22 03:57:16.67
Aug 25 03:57:16.675: INFO: created test-pod-1
Aug 25 03:57:16.678: INFO: created test-pod-2
Aug 25 03:57:16.683: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/25/22 03:57:16.683
Aug 25 03:57:16.683: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5228' to be running and ready
Aug 25 03:57:16.692: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 25 03:57:16.692: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 25 03:57:16.692: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 25 03:57:16.692: INFO: 0 / 3 pods in namespace 'pods-5228' are running and ready (0 seconds elapsed)
Aug 25 03:57:16.693: INFO: expected 0 pod replicas in namespace 'pods-5228', 0 are Running and Ready.
Aug 25 03:57:16.693: INFO: POD         NODE                                 PHASE    GRACE  CONDITIONS
Aug 25 03:57:16.693: INFO: test-pod-1  bobymcbobs-c849-control-plane-p55pp  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  }]
Aug 25 03:57:16.693: INFO: test-pod-2  bobymcbobs-c849-control-plane-p55pp  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  }]
Aug 25 03:57:16.693: INFO: test-pod-3  bobymcbobs-c849-control-plane-p55pp  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  }]
Aug 25 03:57:16.693: INFO: 
Aug 25 03:57:18.705: INFO: 3 / 3 pods in namespace 'pods-5228' are running and ready (2 seconds elapsed)
Aug 25 03:57:18.705: INFO: expected 0 pod replicas in namespace 'pods-5228', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/25/22 03:57:18.717
Aug 25 03:57:18.721: INFO: Pod quantity 3 is different from expected quantity 0
Aug 25 03:57:19.727: INFO: Pod quantity 3 is different from expected quantity 0
Aug 25 03:57:20.725: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 03:57:21.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5228" for this suite. 08/25/22 03:57:21.729
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":211,"skipped":4016,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.086 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:57:16.648
    Aug 25 03:57:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 03:57:16.65
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:16.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:16.665
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 08/25/22 03:57:16.67
    Aug 25 03:57:16.675: INFO: created test-pod-1
    Aug 25 03:57:16.678: INFO: created test-pod-2
    Aug 25 03:57:16.683: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/25/22 03:57:16.683
    Aug 25 03:57:16.683: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5228' to be running and ready
    Aug 25 03:57:16.692: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 25 03:57:16.692: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 25 03:57:16.692: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 25 03:57:16.692: INFO: 0 / 3 pods in namespace 'pods-5228' are running and ready (0 seconds elapsed)
    Aug 25 03:57:16.693: INFO: expected 0 pod replicas in namespace 'pods-5228', 0 are Running and Ready.
    Aug 25 03:57:16.693: INFO: POD         NODE                                 PHASE    GRACE  CONDITIONS
    Aug 25 03:57:16.693: INFO: test-pod-1  bobymcbobs-c849-control-plane-p55pp  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  }]
    Aug 25 03:57:16.693: INFO: test-pod-2  bobymcbobs-c849-control-plane-p55pp  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  }]
    Aug 25 03:57:16.693: INFO: test-pod-3  bobymcbobs-c849-control-plane-p55pp  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 03:57:16 +0000 UTC  }]
    Aug 25 03:57:16.693: INFO: 
    Aug 25 03:57:18.705: INFO: 3 / 3 pods in namespace 'pods-5228' are running and ready (2 seconds elapsed)
    Aug 25 03:57:18.705: INFO: expected 0 pod replicas in namespace 'pods-5228', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/25/22 03:57:18.717
    Aug 25 03:57:18.721: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 25 03:57:19.727: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 25 03:57:20.725: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 03:57:21.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5228" for this suite. 08/25/22 03:57:21.729
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:57:21.736
Aug 25 03:57:21.736: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename csistoragecapacity 08/25/22 03:57:21.737
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:21.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:21.754
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/25/22 03:57:21.759
STEP: getting /apis/storage.k8s.io 08/25/22 03:57:21.763
STEP: getting /apis/storage.k8s.io/v1 08/25/22 03:57:21.765
STEP: creating 08/25/22 03:57:21.768
STEP: watching 08/25/22 03:57:21.779
Aug 25 03:57:21.779: INFO: starting watch
STEP: getting 08/25/22 03:57:21.785
STEP: listing in namespace 08/25/22 03:57:21.789
STEP: listing across namespaces 08/25/22 03:57:21.791
STEP: patching 08/25/22 03:57:21.794
STEP: updating 08/25/22 03:57:21.797
Aug 25 03:57:21.800: INFO: waiting for watch events with expected annotations in namespace
Aug 25 03:57:21.800: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/25/22 03:57:21.8
STEP: deleting a collection 08/25/22 03:57:21.81
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Aug 25 03:57:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-3579" for this suite. 08/25/22 03:57:21.821
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":212,"skipped":4028,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.089 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:57:21.736
    Aug 25 03:57:21.736: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename csistoragecapacity 08/25/22 03:57:21.737
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:21.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:21.754
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/25/22 03:57:21.759
    STEP: getting /apis/storage.k8s.io 08/25/22 03:57:21.763
    STEP: getting /apis/storage.k8s.io/v1 08/25/22 03:57:21.765
    STEP: creating 08/25/22 03:57:21.768
    STEP: watching 08/25/22 03:57:21.779
    Aug 25 03:57:21.779: INFO: starting watch
    STEP: getting 08/25/22 03:57:21.785
    STEP: listing in namespace 08/25/22 03:57:21.789
    STEP: listing across namespaces 08/25/22 03:57:21.791
    STEP: patching 08/25/22 03:57:21.794
    STEP: updating 08/25/22 03:57:21.797
    Aug 25 03:57:21.800: INFO: waiting for watch events with expected annotations in namespace
    Aug 25 03:57:21.800: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/25/22 03:57:21.8
    STEP: deleting a collection 08/25/22 03:57:21.81
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Aug 25 03:57:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-3579" for this suite. 08/25/22 03:57:21.821
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:57:21.825
Aug 25 03:57:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-preemption 08/25/22 03:57:21.826
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:21.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:21.849
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 25 03:57:21.860: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 25 03:58:21.938: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:21.943
Aug 25 03:58:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-preemption-path 08/25/22 03:58:21.945
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:21.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:21.962
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Aug 25 03:58:21.981: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 25 03:58:21.985: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Aug 25 03:58:21.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2023" for this suite. 08/25/22 03:58:22.002
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:58:22.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7975" for this suite. 08/25/22 03:58:22.016
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":213,"skipped":4028,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [60.227 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:57:21.825
    Aug 25 03:57:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-preemption 08/25/22 03:57:21.826
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:57:21.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:57:21.849
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 25 03:57:21.860: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 25 03:58:21.938: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:21.943
    Aug 25 03:58:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-preemption-path 08/25/22 03:58:21.945
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:21.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:21.962
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Aug 25 03:58:21.981: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 25 03:58:21.985: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Aug 25 03:58:21.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2023" for this suite. 08/25/22 03:58:22.002
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:58:22.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7975" for this suite. 08/25/22 03:58:22.016
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:22.053
Aug 25 03:58:22.053: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 03:58:22.054
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:22.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:22.07
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Aug 25 03:58:22.079: INFO: Waiting up to 2m0s for pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" in namespace "var-expansion-1806" to be "container 0 failed with reason CreateContainerConfigError"
Aug 25 03:58:22.081: INFO: Pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279896ms
Aug 25 03:58:24.086: INFO: Pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007577771s
Aug 25 03:58:24.086: INFO: Pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 25 03:58:24.086: INFO: Deleting pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" in namespace "var-expansion-1806"
Aug 25 03:58:24.092: INFO: Wait up to 5m0s for pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 03:58:26.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1806" for this suite. 08/25/22 03:58:26.105
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":214,"skipped":4036,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.059 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:22.053
    Aug 25 03:58:22.053: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 03:58:22.054
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:22.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:22.07
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Aug 25 03:58:22.079: INFO: Waiting up to 2m0s for pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" in namespace "var-expansion-1806" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 25 03:58:22.081: INFO: Pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279896ms
    Aug 25 03:58:24.086: INFO: Pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007577771s
    Aug 25 03:58:24.086: INFO: Pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 25 03:58:24.086: INFO: Deleting pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" in namespace "var-expansion-1806"
    Aug 25 03:58:24.092: INFO: Wait up to 5m0s for pod "var-expansion-8dcd4dac-7d6f-46ba-800d-909fcd31a7ac" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 03:58:26.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1806" for this suite. 08/25/22 03:58:26.105
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:26.112
Aug 25 03:58:26.112: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename hostport 08/25/22 03:58:26.114
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:26.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:26.131
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/25/22 03:58:26.144
Aug 25 03:58:26.150: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5885" to be "running and ready"
Aug 25 03:58:26.154: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146818ms
Aug 25 03:58:26.154: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:58:28.160: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009618807s
Aug 25 03:58:28.160: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 25 03:58:28.160: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 86.109.11.217 on the node which pod1 resides and expect scheduled 08/25/22 03:58:28.16
Aug 25 03:58:28.166: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5885" to be "running and ready"
Aug 25 03:58:28.169: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833536ms
Aug 25 03:58:28.169: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:58:30.174: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.00836902s
Aug 25 03:58:30.174: INFO: The phase of Pod pod2 is Running (Ready = false)
Aug 25 03:58:32.173: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007585011s
Aug 25 03:58:32.174: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 25 03:58:32.174: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 86.109.11.217 but use UDP protocol on the node which pod2 resides 08/25/22 03:58:32.174
Aug 25 03:58:32.178: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5885" to be "running and ready"
Aug 25 03:58:32.181: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.958873ms
Aug 25 03:58:32.181: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:58:34.187: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.0083566s
Aug 25 03:58:34.187: INFO: The phase of Pod pod3 is Running (Ready = false)
Aug 25 03:58:36.187: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008811067s
Aug 25 03:58:36.187: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 25 03:58:36.187: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 25 03:58:36.192: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5885" to be "running and ready"
Aug 25 03:58:36.196: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902075ms
Aug 25 03:58:36.196: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:58:38.202: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009581689s
Aug 25 03:58:38.202: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 25 03:58:38.202: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/25/22 03:58:38.206
Aug 25 03:58:38.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 86.109.11.217 http://127.0.0.1:54323/hostname] Namespace:hostport-5885 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:58:38.206: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:58:38.207: INFO: ExecWithOptions: Clientset creation
Aug 25 03:58:38.207: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5885/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+86.109.11.217+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 86.109.11.217, port: 54323 08/25/22 03:58:38.339
Aug 25 03:58:38.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://86.109.11.217:54323/hostname] Namespace:hostport-5885 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:58:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:58:38.341: INFO: ExecWithOptions: Clientset creation
Aug 25 03:58:38.341: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5885/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F86.109.11.217%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 86.109.11.217, port: 54323 UDP 08/25/22 03:58:38.417
Aug 25 03:58:38.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 86.109.11.217 54323] Namespace:hostport-5885 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 03:58:38.418: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 03:58:38.418: INFO: ExecWithOptions: Clientset creation
Aug 25 03:58:38.418: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5885/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+86.109.11.217+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Aug 25 03:58:43.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5885" for this suite. 08/25/22 03:58:43.524
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":215,"skipped":4038,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [17.417 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:26.112
    Aug 25 03:58:26.112: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename hostport 08/25/22 03:58:26.114
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:26.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:26.131
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/25/22 03:58:26.144
    Aug 25 03:58:26.150: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5885" to be "running and ready"
    Aug 25 03:58:26.154: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146818ms
    Aug 25 03:58:26.154: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:58:28.160: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009618807s
    Aug 25 03:58:28.160: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 25 03:58:28.160: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 86.109.11.217 on the node which pod1 resides and expect scheduled 08/25/22 03:58:28.16
    Aug 25 03:58:28.166: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5885" to be "running and ready"
    Aug 25 03:58:28.169: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833536ms
    Aug 25 03:58:28.169: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:58:30.174: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.00836902s
    Aug 25 03:58:30.174: INFO: The phase of Pod pod2 is Running (Ready = false)
    Aug 25 03:58:32.173: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007585011s
    Aug 25 03:58:32.174: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 25 03:58:32.174: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 86.109.11.217 but use UDP protocol on the node which pod2 resides 08/25/22 03:58:32.174
    Aug 25 03:58:32.178: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5885" to be "running and ready"
    Aug 25 03:58:32.181: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.958873ms
    Aug 25 03:58:32.181: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:58:34.187: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.0083566s
    Aug 25 03:58:34.187: INFO: The phase of Pod pod3 is Running (Ready = false)
    Aug 25 03:58:36.187: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008811067s
    Aug 25 03:58:36.187: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 25 03:58:36.187: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 25 03:58:36.192: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5885" to be "running and ready"
    Aug 25 03:58:36.196: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902075ms
    Aug 25 03:58:36.196: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:58:38.202: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009581689s
    Aug 25 03:58:38.202: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 25 03:58:38.202: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/25/22 03:58:38.206
    Aug 25 03:58:38.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 86.109.11.217 http://127.0.0.1:54323/hostname] Namespace:hostport-5885 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:58:38.206: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:58:38.207: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:58:38.207: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5885/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+86.109.11.217+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 86.109.11.217, port: 54323 08/25/22 03:58:38.339
    Aug 25 03:58:38.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://86.109.11.217:54323/hostname] Namespace:hostport-5885 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:58:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:58:38.341: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:58:38.341: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5885/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F86.109.11.217%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 86.109.11.217, port: 54323 UDP 08/25/22 03:58:38.417
    Aug 25 03:58:38.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 86.109.11.217 54323] Namespace:hostport-5885 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 03:58:38.418: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 03:58:38.418: INFO: ExecWithOptions: Clientset creation
    Aug 25 03:58:38.418: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5885/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+86.109.11.217+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Aug 25 03:58:43.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-5885" for this suite. 08/25/22 03:58:43.524
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:43.534
Aug 25 03:58:43.534: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubelet-test 08/25/22 03:58:43.535
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:43.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:43.555
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/25/22 03:58:43.575
Aug 25 03:58:43.575: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c" in namespace "kubelet-test-3429" to be "completed"
Aug 25 03:58:43.577: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160388ms
Aug 25 03:58:45.582: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006832025s
Aug 25 03:58:47.584: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008524388s
Aug 25 03:58:47.584: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 25 03:58:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3429" for this suite. 08/25/22 03:58:47.606
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":216,"skipped":4096,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.077 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:43.534
    Aug 25 03:58:43.534: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubelet-test 08/25/22 03:58:43.535
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:43.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:43.555
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/25/22 03:58:43.575
    Aug 25 03:58:43.575: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c" in namespace "kubelet-test-3429" to be "completed"
    Aug 25 03:58:43.577: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160388ms
    Aug 25 03:58:45.582: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006832025s
    Aug 25 03:58:47.584: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008524388s
    Aug 25 03:58:47.584: INFO: Pod "agnhost-host-aliases791ca99b-cf97-4ac9-943e-5feffb38dd7c" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 25 03:58:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3429" for this suite. 08/25/22 03:58:47.606
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:47.622
Aug 25 03:58:47.622: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename security-context-test 08/25/22 03:58:47.625
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:47.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:47.637
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Aug 25 03:58:47.645: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6" in namespace "security-context-test-9833" to be "Succeeded or Failed"
Aug 25 03:58:47.648: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340779ms
Aug 25 03:58:49.653: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007942473s
Aug 25 03:58:51.652: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006651218s
Aug 25 03:58:53.652: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006962934s
Aug 25 03:58:53.652: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 25 03:58:53.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9833" for this suite. 08/25/22 03:58:53.662
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":217,"skipped":4102,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:47.622
    Aug 25 03:58:47.622: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename security-context-test 08/25/22 03:58:47.625
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:47.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:47.637
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Aug 25 03:58:47.645: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6" in namespace "security-context-test-9833" to be "Succeeded or Failed"
    Aug 25 03:58:47.648: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340779ms
    Aug 25 03:58:49.653: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007942473s
    Aug 25 03:58:51.652: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006651218s
    Aug 25 03:58:53.652: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006962934s
    Aug 25 03:58:53.652: INFO: Pod "alpine-nnp-false-c9fa25f4-ded6-4f37-a5c5-25fb6196f7a6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 25 03:58:53.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9833" for this suite. 08/25/22 03:58:53.662
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:53.668
Aug 25 03:58:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 03:58:53.67
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:53.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:53.687
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:58:53.692
Aug 25 03:58:53.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3054 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Aug 25 03:58:53.779: INFO: stderr: ""
Aug 25 03:58:53.779: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/25/22 03:58:53.779
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Aug 25 03:58:53.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3054 delete pods e2e-test-httpd-pod'
Aug 25 03:58:56.358: INFO: stderr: ""
Aug 25 03:58:56.358: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 03:58:56.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3054" for this suite. 08/25/22 03:58:56.363
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":218,"skipped":4109,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.699 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:53.668
    Aug 25 03:58:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 03:58:53.67
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:53.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:53.687
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/25/22 03:58:53.692
    Aug 25 03:58:53.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3054 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Aug 25 03:58:53.779: INFO: stderr: ""
    Aug 25 03:58:53.779: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/25/22 03:58:53.779
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Aug 25 03:58:53.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-3054 delete pods e2e-test-httpd-pod'
    Aug 25 03:58:56.358: INFO: stderr: ""
    Aug 25 03:58:56.358: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 03:58:56.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3054" for this suite. 08/25/22 03:58:56.363
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:56.368
Aug 25 03:58:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 03:58:56.369
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:56.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:56.384
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/25/22 03:58:56.387
STEP: Wait for the Deployment to create new ReplicaSet 08/25/22 03:58:56.391
STEP: delete the deployment 08/25/22 03:58:56.502
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/25/22 03:58:56.508
STEP: Gathering metrics 08/25/22 03:58:57.027
Aug 25 03:58:57.037: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
Aug 25 03:58:57.041: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 3.272466ms
Aug 25 03:58:57.041: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
Aug 25 03:58:57.041: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
Aug 25 03:58:57.117: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 03:58:57.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6954" for this suite. 08/25/22 03:58:57.121
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":219,"skipped":4112,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.758 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:56.368
    Aug 25 03:58:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 03:58:56.369
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:56.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:56.384
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/25/22 03:58:56.387
    STEP: Wait for the Deployment to create new ReplicaSet 08/25/22 03:58:56.391
    STEP: delete the deployment 08/25/22 03:58:56.502
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/25/22 03:58:56.508
    STEP: Gathering metrics 08/25/22 03:58:57.027
    Aug 25 03:58:57.037: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
    Aug 25 03:58:57.041: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 3.272466ms
    Aug 25 03:58:57.041: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
    Aug 25 03:58:57.041: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
    Aug 25 03:58:57.117: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 03:58:57.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6954" for this suite. 08/25/22 03:58:57.121
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:58:57.126
Aug 25 03:58:57.126: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 03:58:57.127
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:57.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:57.141
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 08/25/22 03:58:57.146
Aug 25 03:58:57.152: INFO: Waiting up to 5m0s for pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5" in namespace "var-expansion-3212" to be "Succeeded or Failed"
Aug 25 03:58:57.155: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292423ms
Aug 25 03:58:59.162: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009488976s
Aug 25 03:59:01.161: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008486634s
STEP: Saw pod success 08/25/22 03:59:01.161
Aug 25 03:59:01.161: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5" satisfied condition "Succeeded or Failed"
Aug 25 03:59:01.165: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5 container dapi-container: <nil>
STEP: delete the pod 08/25/22 03:59:01.17
Aug 25 03:59:01.177: INFO: Waiting for pod var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5 to disappear
Aug 25 03:59:01.180: INFO: Pod var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 03:59:01.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3212" for this suite. 08/25/22 03:59:01.185
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":220,"skipped":4114,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.063 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:58:57.126
    Aug 25 03:58:57.126: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 03:58:57.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:58:57.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:58:57.141
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 08/25/22 03:58:57.146
    Aug 25 03:58:57.152: INFO: Waiting up to 5m0s for pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5" in namespace "var-expansion-3212" to be "Succeeded or Failed"
    Aug 25 03:58:57.155: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292423ms
    Aug 25 03:58:59.162: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009488976s
    Aug 25 03:59:01.161: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008486634s
    STEP: Saw pod success 08/25/22 03:59:01.161
    Aug 25 03:59:01.161: INFO: Pod "var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5" satisfied condition "Succeeded or Failed"
    Aug 25 03:59:01.165: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 03:59:01.17
    Aug 25 03:59:01.177: INFO: Waiting for pod var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5 to disappear
    Aug 25 03:59:01.180: INFO: Pod var-expansion-e11e7316-8cf0-46a1-8f03-77bcdec0c6b5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 03:59:01.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3212" for this suite. 08/25/22 03:59:01.185
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:01.19
Aug 25 03:59:01.191: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replicaset 08/25/22 03:59:01.192
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:01.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:01.208
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/25/22 03:59:01.212
Aug 25 03:59:01.218: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7294" to be "running and ready"
Aug 25 03:59:01.220: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258352ms
Aug 25 03:59:01.220: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:59:03.225: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007172994s
Aug 25 03:59:03.225: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 25 03:59:03.225: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/25/22 03:59:03.229
STEP: Then the orphan pod is adopted 08/25/22 03:59:03.232
STEP: When the matched label of one of its pods change 08/25/22 03:59:04.24
Aug 25 03:59:04.244: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/25/22 03:59:04.256
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 25 03:59:05.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7294" for this suite. 08/25/22 03:59:05.268
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":221,"skipped":4131,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.081 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:01.19
    Aug 25 03:59:01.191: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replicaset 08/25/22 03:59:01.192
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:01.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:01.208
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/25/22 03:59:01.212
    Aug 25 03:59:01.218: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7294" to be "running and ready"
    Aug 25 03:59:01.220: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258352ms
    Aug 25 03:59:01.220: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:59:03.225: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007172994s
    Aug 25 03:59:03.225: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 25 03:59:03.225: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/25/22 03:59:03.229
    STEP: Then the orphan pod is adopted 08/25/22 03:59:03.232
    STEP: When the matched label of one of its pods change 08/25/22 03:59:04.24
    Aug 25 03:59:04.244: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/25/22 03:59:04.256
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 25 03:59:05.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7294" for this suite. 08/25/22 03:59:05.268
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:05.278
Aug 25 03:59:05.278: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:59:05.28
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:05.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:05.292
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:59:05.307
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:59:05.707
STEP: Deploying the webhook pod 08/25/22 03:59:05.716
STEP: Wait for the deployment to be ready 08/25/22 03:59:05.728
Aug 25 03:59:05.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 03:59:07.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 03:59:09.753
STEP: Verifying the service has paired with the endpoint 08/25/22 03:59:09.763
Aug 25 03:59:10.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Aug 25 03:59:10.768: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4366-crds.webhook.example.com via the AdmissionRegistration API 08/25/22 03:59:11.282
STEP: Creating a custom resource while v1 is storage version 08/25/22 03:59:11.303
STEP: Patching Custom Resource Definition to set v2 as storage 08/25/22 03:59:13.377
STEP: Patching the custom resource while v2 is storage version 08/25/22 03:59:13.384
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:59:13.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5894" for this suite. 08/25/22 03:59:13.969
STEP: Destroying namespace "webhook-5894-markers" for this suite. 08/25/22 03:59:13.974
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":222,"skipped":4231,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.728 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:05.278
    Aug 25 03:59:05.278: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:59:05.28
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:05.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:05.292
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:59:05.307
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:59:05.707
    STEP: Deploying the webhook pod 08/25/22 03:59:05.716
    STEP: Wait for the deployment to be ready 08/25/22 03:59:05.728
    Aug 25 03:59:05.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 03:59:07.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 03:59:09.753
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:59:09.763
    Aug 25 03:59:10.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Aug 25 03:59:10.768: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4366-crds.webhook.example.com via the AdmissionRegistration API 08/25/22 03:59:11.282
    STEP: Creating a custom resource while v1 is storage version 08/25/22 03:59:11.303
    STEP: Patching Custom Resource Definition to set v2 as storage 08/25/22 03:59:13.377
    STEP: Patching the custom resource while v2 is storage version 08/25/22 03:59:13.384
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:59:13.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5894" for this suite. 08/25/22 03:59:13.969
    STEP: Destroying namespace "webhook-5894-markers" for this suite. 08/25/22 03:59:13.974
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:14.007
Aug 25 03:59:14.007: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename namespaces 08/25/22 03:59:14.007
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:14.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:14.019
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 08/25/22 03:59:14.023
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:14.031
STEP: Creating a service in the namespace 08/25/22 03:59:14.034
STEP: Deleting the namespace 08/25/22 03:59:14.043
STEP: Waiting for the namespace to be removed. 08/25/22 03:59:14.047
STEP: Recreating the namespace 08/25/22 03:59:20.053
STEP: Verifying there is no service in the namespace 08/25/22 03:59:20.066
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 25 03:59:20.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8418" for this suite. 08/25/22 03:59:20.073
STEP: Destroying namespace "nsdeletetest-1585" for this suite. 08/25/22 03:59:20.076
Aug 25 03:59:20.080: INFO: Namespace nsdeletetest-1585 was already deleted
STEP: Destroying namespace "nsdeletetest-7710" for this suite. 08/25/22 03:59:20.08
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":223,"skipped":4239,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:14.007
    Aug 25 03:59:14.007: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename namespaces 08/25/22 03:59:14.007
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:14.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:14.019
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 08/25/22 03:59:14.023
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:14.031
    STEP: Creating a service in the namespace 08/25/22 03:59:14.034
    STEP: Deleting the namespace 08/25/22 03:59:14.043
    STEP: Waiting for the namespace to be removed. 08/25/22 03:59:14.047
    STEP: Recreating the namespace 08/25/22 03:59:20.053
    STEP: Verifying there is no service in the namespace 08/25/22 03:59:20.066
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 03:59:20.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8418" for this suite. 08/25/22 03:59:20.073
    STEP: Destroying namespace "nsdeletetest-1585" for this suite. 08/25/22 03:59:20.076
    Aug 25 03:59:20.080: INFO: Namespace nsdeletetest-1585 was already deleted
    STEP: Destroying namespace "nsdeletetest-7710" for this suite. 08/25/22 03:59:20.08
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:20.083
Aug 25 03:59:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 03:59:20.084
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:20.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:20.097
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 03:59:20.109
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:59:20.825
STEP: Deploying the webhook pod 08/25/22 03:59:20.831
STEP: Wait for the deployment to be ready 08/25/22 03:59:20.841
Aug 25 03:59:20.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 03:59:22.86
STEP: Verifying the service has paired with the endpoint 08/25/22 03:59:22.869
Aug 25 03:59:23.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 08/25/22 03:59:23.873
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/25/22 03:59:23.876
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/25/22 03:59:23.876
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/25/22 03:59:23.876
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/25/22 03:59:23.878
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/25/22 03:59:23.879
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/25/22 03:59:23.881
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:59:23.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5459" for this suite. 08/25/22 03:59:23.886
STEP: Destroying namespace "webhook-5459-markers" for this suite. 08/25/22 03:59:23.891
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":224,"skipped":4244,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.839 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:20.083
    Aug 25 03:59:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 03:59:20.084
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:20.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:20.097
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 03:59:20.109
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 03:59:20.825
    STEP: Deploying the webhook pod 08/25/22 03:59:20.831
    STEP: Wait for the deployment to be ready 08/25/22 03:59:20.841
    Aug 25 03:59:20.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 03:59:22.86
    STEP: Verifying the service has paired with the endpoint 08/25/22 03:59:22.869
    Aug 25 03:59:23.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 08/25/22 03:59:23.873
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/25/22 03:59:23.876
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/25/22 03:59:23.876
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/25/22 03:59:23.876
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/25/22 03:59:23.878
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/25/22 03:59:23.879
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/25/22 03:59:23.881
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:59:23.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5459" for this suite. 08/25/22 03:59:23.886
    STEP: Destroying namespace "webhook-5459-markers" for this suite. 08/25/22 03:59:23.891
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:23.923
Aug 25 03:59:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 03:59:23.924
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:23.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:23.937
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/25/22 03:59:23.947
Aug 25 03:59:23.947: INFO: Creating simple deployment test-deployment-g672p
Aug 25 03:59:23.957: INFO: deployment "test-deployment-g672p" doesn't have the required revision set
STEP: Getting /status 08/25/22 03:59:25.971
Aug 25 03:59:25.977: INFO: Deployment test-deployment-g672p has Conditions: [{Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 08/25/22 03:59:25.977
Aug 25 03:59:25.987: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 23, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-g672p-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/25/22 03:59:25.987
Aug 25 03:59:25.990: INFO: Observed &Deployment event: ADDED
Aug 25 03:59:25.990: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g672p-777898ffcc" is progressing.}
Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
Aug 25 03:59:25.991: INFO: Found Deployment test-deployment-g672p in namespace deployment-3616 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 25 03:59:25.991: INFO: Deployment test-deployment-g672p has an updated status
STEP: patching the Statefulset Status 08/25/22 03:59:25.991
Aug 25 03:59:25.991: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 25 03:59:25.998: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/25/22 03:59:25.998
Aug 25 03:59:26.001: INFO: Observed &Deployment event: ADDED
Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
Aug 25 03:59:26.001: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 25 03:59:26.001: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g672p-777898ffcc" is progressing.}
Aug 25 03:59:26.002: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
Aug 25 03:59:26.002: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 25 03:59:26.002: INFO: Observed &Deployment event: MODIFIED
Aug 25 03:59:26.002: INFO: Found deployment test-deployment-g672p in namespace deployment-3616 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 25 03:59:26.002: INFO: Deployment test-deployment-g672p has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 03:59:26.006: INFO: Deployment "test-deployment-g672p":
&Deployment{ObjectMeta:{test-deployment-g672p  deployment-3616  8d1c0f2e-be83-4ab3-903a-b21571f35a93 43159 1 2022-08-25 03:59:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-08-25 03:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-08-25 03:59:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-08-25 03:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004321568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-g672p-777898ffcc",LastUpdateTime:2022-08-25 03:59:25 +0000 UTC,LastTransitionTime:2022-08-25 03:59:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 25 03:59:26.010: INFO: New ReplicaSet "test-deployment-g672p-777898ffcc" of Deployment "test-deployment-g672p":
&ReplicaSet{ObjectMeta:{test-deployment-g672p-777898ffcc  deployment-3616  ebd55761-2087-4eca-823d-353c547ad35b 43153 1 2022-08-25 03:59:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-g672p 8d1c0f2e-be83-4ab3-903a-b21571f35a93 0xc0005ed3d0 0xc0005ed3d1}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d1c0f2e-be83-4ab3-903a-b21571f35a93\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0005ed478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:59:26.013: INFO: Pod "test-deployment-g672p-777898ffcc-7hw2d" is available:
&Pod{ObjectMeta:{test-deployment-g672p-777898ffcc-7hw2d test-deployment-g672p-777898ffcc- deployment-3616  30dfb039-ad85-4754-99bf-79f59f63b091 43152 0 2022-08-25 03:59:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-g672p-777898ffcc ebd55761-2087-4eca-823d-353c547ad35b 0xc005eb95f0 0xc005eb95f1}] [] [{kube-controller-manager Update v1 2022-08-25 03:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebd55761-2087-4eca-823d-353c547ad35b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:59:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xkjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xkjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.13,StartTime:2022-08-25 03:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:59:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23905dd632220ca9dcf8890a4019fcb2c70bcfe7412df7f3bee84f7f4d4e643f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 03:59:26.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3616" for this suite. 08/25/22 03:59:26.016
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":225,"skipped":4245,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.097 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:23.923
    Aug 25 03:59:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 03:59:23.924
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:23.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:23.937
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/25/22 03:59:23.947
    Aug 25 03:59:23.947: INFO: Creating simple deployment test-deployment-g672p
    Aug 25 03:59:23.957: INFO: deployment "test-deployment-g672p" doesn't have the required revision set
    STEP: Getting /status 08/25/22 03:59:25.971
    Aug 25 03:59:25.977: INFO: Deployment test-deployment-g672p has Conditions: [{Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 08/25/22 03:59:25.977
    Aug 25 03:59:25.987: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 3, 59, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 3, 59, 23, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-g672p-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/25/22 03:59:25.987
    Aug 25 03:59:25.990: INFO: Observed &Deployment event: ADDED
    Aug 25 03:59:25.990: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
    Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g672p-777898ffcc" is progressing.}
    Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
    Aug 25 03:59:25.991: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 25 03:59:25.991: INFO: Observed Deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
    Aug 25 03:59:25.991: INFO: Found Deployment test-deployment-g672p in namespace deployment-3616 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 25 03:59:25.991: INFO: Deployment test-deployment-g672p has an updated status
    STEP: patching the Statefulset Status 08/25/22 03:59:25.991
    Aug 25 03:59:25.991: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 25 03:59:25.998: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/25/22 03:59:25.998
    Aug 25 03:59:26.001: INFO: Observed &Deployment event: ADDED
    Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
    Aug 25 03:59:26.001: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-g672p-777898ffcc"}
    Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 25 03:59:26.001: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:26.001: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:23 +0000 UTC 2022-08-25 03:59:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-g672p-777898ffcc" is progressing.}
    Aug 25 03:59:26.002: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
    Aug 25 03:59:26.002: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-25 03:59:25 +0000 UTC 2022-08-25 03:59:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-g672p-777898ffcc" has successfully progressed.}
    Aug 25 03:59:26.002: INFO: Observed deployment test-deployment-g672p in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 25 03:59:26.002: INFO: Observed &Deployment event: MODIFIED
    Aug 25 03:59:26.002: INFO: Found deployment test-deployment-g672p in namespace deployment-3616 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 25 03:59:26.002: INFO: Deployment test-deployment-g672p has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 03:59:26.006: INFO: Deployment "test-deployment-g672p":
    &Deployment{ObjectMeta:{test-deployment-g672p  deployment-3616  8d1c0f2e-be83-4ab3-903a-b21571f35a93 43159 1 2022-08-25 03:59:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-08-25 03:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-08-25 03:59:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-08-25 03:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004321568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-g672p-777898ffcc",LastUpdateTime:2022-08-25 03:59:25 +0000 UTC,LastTransitionTime:2022-08-25 03:59:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 25 03:59:26.010: INFO: New ReplicaSet "test-deployment-g672p-777898ffcc" of Deployment "test-deployment-g672p":
    &ReplicaSet{ObjectMeta:{test-deployment-g672p-777898ffcc  deployment-3616  ebd55761-2087-4eca-823d-353c547ad35b 43153 1 2022-08-25 03:59:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-g672p 8d1c0f2e-be83-4ab3-903a-b21571f35a93 0xc0005ed3d0 0xc0005ed3d1}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d1c0f2e-be83-4ab3-903a-b21571f35a93\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0005ed478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:59:26.013: INFO: Pod "test-deployment-g672p-777898ffcc-7hw2d" is available:
    &Pod{ObjectMeta:{test-deployment-g672p-777898ffcc-7hw2d test-deployment-g672p-777898ffcc- deployment-3616  30dfb039-ad85-4754-99bf-79f59f63b091 43152 0 2022-08-25 03:59:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-g672p-777898ffcc ebd55761-2087-4eca-823d-353c547ad35b 0xc005eb95f0 0xc005eb95f1}] [] [{kube-controller-manager Update v1 2022-08-25 03:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebd55761-2087-4eca-823d-353c547ad35b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:59:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xkjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xkjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.13,StartTime:2022-08-25 03:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:59:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23905dd632220ca9dcf8890a4019fcb2c70bcfe7412df7f3bee84f7f4d4e643f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 03:59:26.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3616" for this suite. 08/25/22 03:59:26.016
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:26.022
Aug 25 03:59:26.022: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename disruption 08/25/22 03:59:26.024
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:26.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:26.037
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 08/25/22 03:59:26.043
STEP: Updating PodDisruptionBudget status 08/25/22 03:59:28.051
STEP: Waiting for all pods to be running 08/25/22 03:59:28.059
Aug 25 03:59:28.063: INFO: running pods: 0 < 1
STEP: locating a running pod 08/25/22 03:59:30.069
STEP: Waiting for the pdb to be processed 08/25/22 03:59:30.08
STEP: Patching PodDisruptionBudget status 08/25/22 03:59:30.087
STEP: Waiting for the pdb to be processed 08/25/22 03:59:30.095
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 25 03:59:30.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6099" for this suite. 08/25/22 03:59:30.103
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":226,"skipped":4273,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.085 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:26.022
    Aug 25 03:59:26.022: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename disruption 08/25/22 03:59:26.024
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:26.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:26.037
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 08/25/22 03:59:26.043
    STEP: Updating PodDisruptionBudget status 08/25/22 03:59:28.051
    STEP: Waiting for all pods to be running 08/25/22 03:59:28.059
    Aug 25 03:59:28.063: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/25/22 03:59:30.069
    STEP: Waiting for the pdb to be processed 08/25/22 03:59:30.08
    STEP: Patching PodDisruptionBudget status 08/25/22 03:59:30.087
    STEP: Waiting for the pdb to be processed 08/25/22 03:59:30.095
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 25 03:59:30.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6099" for this suite. 08/25/22 03:59:30.103
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:30.111
Aug 25 03:59:30.112: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 03:59:30.113
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:30.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:30.126
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 25 03:59:30.129: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 25 03:59:30.136: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 25 03:59:35.140: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/25/22 03:59:35.14
Aug 25 03:59:35.140: INFO: Creating deployment "test-rolling-update-deployment"
Aug 25 03:59:35.143: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 25 03:59:35.149: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 25 03:59:37.157: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 25 03:59:37.160: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 03:59:37.170: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9254  656dfa1a-c103-4fbe-a988-596d6b22eb2c 43328 1 2022-08-25 03:59:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-08-25 03:59:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00114a568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-25 03:59:35 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-08-25 03:59:36 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 25 03:59:37.174: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9254  347b5824-997c-4d29-932e-0a03e1ec94db 43314 1 2022-08-25 03:59:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 656dfa1a-c103-4fbe-a988-596d6b22eb2c 0xc004081a77 0xc004081a78}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:59:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"656dfa1a-c103-4fbe-a988-596d6b22eb2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004081b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:59:37.174: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 25 03:59:37.174: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9254  bbc0c8b4-3d63-4884-baa5-174204747dc9 43326 2 2022-08-25 03:59:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 656dfa1a-c103-4fbe-a988-596d6b22eb2c 0xc004081947 0xc004081948}] [] [{e2e.test Update apps/v1 2022-08-25 03:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"656dfa1a-c103-4fbe-a988-596d6b22eb2c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004081a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 25 03:59:37.178: INFO: Pod "test-rolling-update-deployment-78f575d8ff-q7cxt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-q7cxt test-rolling-update-deployment-78f575d8ff- deployment-9254  be8a432e-6934-4c1d-89bf-ce9511bef8f4 43313 0 2022-08-25 03:59:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 347b5824-997c-4d29-932e-0a03e1ec94db 0xc0027ed9b7 0xc0027ed9b8}] [] [{kube-controller-manager Update v1 2022-08-25 03:59:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"347b5824-997c-4d29-932e-0a03e1ec94db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvr48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvr48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.13,StartTime:2022-08-25 03:59:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:59:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9f58c4418ce94cbb4c32c3b549e91c85363290ffa8f4412f1846f71c5ab04955,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 03:59:37.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9254" for this suite. 08/25/22 03:59:37.183
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":227,"skipped":4329,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.079 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:30.111
    Aug 25 03:59:30.112: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 03:59:30.113
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:30.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:30.126
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 25 03:59:30.129: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 25 03:59:30.136: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 25 03:59:35.140: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/25/22 03:59:35.14
    Aug 25 03:59:35.140: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 25 03:59:35.143: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 25 03:59:35.149: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 25 03:59:37.157: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 25 03:59:37.160: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 03:59:37.170: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9254  656dfa1a-c103-4fbe-a988-596d6b22eb2c 43328 1 2022-08-25 03:59:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-08-25 03:59:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00114a568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-25 03:59:35 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-08-25 03:59:36 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 25 03:59:37.174: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9254  347b5824-997c-4d29-932e-0a03e1ec94db 43314 1 2022-08-25 03:59:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 656dfa1a-c103-4fbe-a988-596d6b22eb2c 0xc004081a77 0xc004081a78}] [] [{kube-controller-manager Update apps/v1 2022-08-25 03:59:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"656dfa1a-c103-4fbe-a988-596d6b22eb2c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004081b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:59:37.174: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 25 03:59:37.174: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9254  bbc0c8b4-3d63-4884-baa5-174204747dc9 43326 2 2022-08-25 03:59:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 656dfa1a-c103-4fbe-a988-596d6b22eb2c 0xc004081947 0xc004081948}] [] [{e2e.test Update apps/v1 2022-08-25 03:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"656dfa1a-c103-4fbe-a988-596d6b22eb2c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004081a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 03:59:37.178: INFO: Pod "test-rolling-update-deployment-78f575d8ff-q7cxt" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-q7cxt test-rolling-update-deployment-78f575d8ff- deployment-9254  be8a432e-6934-4c1d-89bf-ce9511bef8f4 43313 0 2022-08-25 03:59:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 347b5824-997c-4d29-932e-0a03e1ec94db 0xc0027ed9b7 0xc0027ed9b8}] [] [{kube-controller-manager Update v1 2022-08-25 03:59:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"347b5824-997c-4d29-932e-0a03e1ec94db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 03:59:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvr48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvr48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 03:59:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.13,StartTime:2022-08-25 03:59:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 03:59:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9f58c4418ce94cbb4c32c3b549e91c85363290ffa8f4412f1846f71c5ab04955,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 03:59:37.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9254" for this suite. 08/25/22 03:59:37.183
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:37.194
Aug 25 03:59:37.194: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:59:37.195
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:37.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:37.208
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 08/25/22 03:59:37.211
Aug 25 03:59:37.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144" in namespace "projected-4896" to be "Succeeded or Failed"
Aug 25 03:59:37.222: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849479ms
Aug 25 03:59:39.227: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009691207s
Aug 25 03:59:41.226: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008604169s
STEP: Saw pod success 08/25/22 03:59:41.226
Aug 25 03:59:41.226: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144" satisfied condition "Succeeded or Failed"
Aug 25 03:59:41.231: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144 container client-container: <nil>
STEP: delete the pod 08/25/22 03:59:41.236
Aug 25 03:59:41.244: INFO: Waiting for pod downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144 to disappear
Aug 25 03:59:41.247: INFO: Pod downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 03:59:41.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4896" for this suite. 08/25/22 03:59:41.251
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":228,"skipped":4359,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.062 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:37.194
    Aug 25 03:59:37.194: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:59:37.195
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:37.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:37.208
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 08/25/22 03:59:37.211
    Aug 25 03:59:37.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144" in namespace "projected-4896" to be "Succeeded or Failed"
    Aug 25 03:59:37.222: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849479ms
    Aug 25 03:59:39.227: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009691207s
    Aug 25 03:59:41.226: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008604169s
    STEP: Saw pod success 08/25/22 03:59:41.226
    Aug 25 03:59:41.226: INFO: Pod "downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144" satisfied condition "Succeeded or Failed"
    Aug 25 03:59:41.231: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144 container client-container: <nil>
    STEP: delete the pod 08/25/22 03:59:41.236
    Aug 25 03:59:41.244: INFO: Waiting for pod downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144 to disappear
    Aug 25 03:59:41.247: INFO: Pod downwardapi-volume-4115b46f-1974-4e80-9269-fd0168ba9144 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 03:59:41.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4896" for this suite. 08/25/22 03:59:41.251
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:41.257
Aug 25 03:59:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 03:59:41.258
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:41.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:41.276
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Aug 25 03:59:41.287: INFO: Waiting up to 2m0s for pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" in namespace "var-expansion-1342" to be "container 0 failed with reason CreateContainerConfigError"
Aug 25 03:59:41.290: INFO: Pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.544103ms
Aug 25 03:59:43.293: INFO: Pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00625241s
Aug 25 03:59:43.293: INFO: Pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 25 03:59:43.293: INFO: Deleting pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" in namespace "var-expansion-1342"
Aug 25 03:59:43.298: INFO: Wait up to 5m0s for pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 03:59:45.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1342" for this suite. 08/25/22 03:59:45.307
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":229,"skipped":4360,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.054 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:41.257
    Aug 25 03:59:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 03:59:41.258
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:41.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:41.276
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Aug 25 03:59:41.287: INFO: Waiting up to 2m0s for pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" in namespace "var-expansion-1342" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 25 03:59:41.290: INFO: Pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.544103ms
    Aug 25 03:59:43.293: INFO: Pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00625241s
    Aug 25 03:59:43.293: INFO: Pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 25 03:59:43.293: INFO: Deleting pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" in namespace "var-expansion-1342"
    Aug 25 03:59:43.298: INFO: Wait up to 5m0s for pod "var-expansion-76f50662-cc96-4e35-948c-48641a274580" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 03:59:45.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1342" for this suite. 08/25/22 03:59:45.307
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:45.313
Aug 25 03:59:45.314: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename security-context 08/25/22 03:59:45.315
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:45.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:45.331
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/25/22 03:59:45.335
Aug 25 03:59:45.340: INFO: Waiting up to 5m0s for pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde" in namespace "security-context-1063" to be "Succeeded or Failed"
Aug 25 03:59:45.342: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.304978ms
Aug 25 03:59:47.347: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007137459s
Aug 25 03:59:49.348: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008349993s
STEP: Saw pod success 08/25/22 03:59:49.348
Aug 25 03:59:49.348: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde" satisfied condition "Succeeded or Failed"
Aug 25 03:59:49.352: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde container test-container: <nil>
STEP: delete the pod 08/25/22 03:59:49.357
Aug 25 03:59:49.364: INFO: Waiting for pod security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde to disappear
Aug 25 03:59:49.368: INFO: Pod security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 25 03:59:49.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1063" for this suite. 08/25/22 03:59:49.372
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":230,"skipped":4384,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.063 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:45.313
    Aug 25 03:59:45.314: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename security-context 08/25/22 03:59:45.315
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:45.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:45.331
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/25/22 03:59:45.335
    Aug 25 03:59:45.340: INFO: Waiting up to 5m0s for pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde" in namespace "security-context-1063" to be "Succeeded or Failed"
    Aug 25 03:59:45.342: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.304978ms
    Aug 25 03:59:47.347: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007137459s
    Aug 25 03:59:49.348: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008349993s
    STEP: Saw pod success 08/25/22 03:59:49.348
    Aug 25 03:59:49.348: INFO: Pod "security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde" satisfied condition "Succeeded or Failed"
    Aug 25 03:59:49.352: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde container test-container: <nil>
    STEP: delete the pod 08/25/22 03:59:49.357
    Aug 25 03:59:49.364: INFO: Waiting for pod security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde to disappear
    Aug 25 03:59:49.368: INFO: Pod security-context-5c61642f-6fa8-4c8e-a9a4-ddbaf8272bde no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 25 03:59:49.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1063" for this suite. 08/25/22 03:59:49.372
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:49.377
Aug 25 03:59:49.378: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:59:49.379
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:49.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:49.395
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 25 03:59:49.398: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 03:59:50.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1925" for this suite. 08/25/22 03:59:50.423
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":231,"skipped":4389,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [1.049 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:49.377
    Aug 25 03:59:49.378: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename custom-resource-definition 08/25/22 03:59:49.379
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:49.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:49.395
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 25 03:59:49.398: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 03:59:50.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1925" for this suite. 08/25/22 03:59:50.423
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:50.428
Aug 25 03:59:50.428: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename events 08/25/22 03:59:50.429
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:50.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:50.442
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/25/22 03:59:50.446
STEP: listing events in all namespaces 08/25/22 03:59:50.45
STEP: listing events in test namespace 08/25/22 03:59:50.458
STEP: listing events with field selection filtering on source 08/25/22 03:59:50.46
STEP: listing events with field selection filtering on reportingController 08/25/22 03:59:50.462
STEP: getting the test event 08/25/22 03:59:50.464
STEP: patching the test event 08/25/22 03:59:50.467
STEP: getting the test event 08/25/22 03:59:50.476
STEP: updating the test event 08/25/22 03:59:50.479
STEP: getting the test event 08/25/22 03:59:50.491
STEP: deleting the test event 08/25/22 03:59:50.493
STEP: listing events in all namespaces 08/25/22 03:59:50.498
STEP: listing events in test namespace 08/25/22 03:59:50.506
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Aug 25 03:59:50.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5652" for this suite. 08/25/22 03:59:50.512
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":232,"skipped":4400,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.088 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:50.428
    Aug 25 03:59:50.428: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename events 08/25/22 03:59:50.429
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:50.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:50.442
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/25/22 03:59:50.446
    STEP: listing events in all namespaces 08/25/22 03:59:50.45
    STEP: listing events in test namespace 08/25/22 03:59:50.458
    STEP: listing events with field selection filtering on source 08/25/22 03:59:50.46
    STEP: listing events with field selection filtering on reportingController 08/25/22 03:59:50.462
    STEP: getting the test event 08/25/22 03:59:50.464
    STEP: patching the test event 08/25/22 03:59:50.467
    STEP: getting the test event 08/25/22 03:59:50.476
    STEP: updating the test event 08/25/22 03:59:50.479
    STEP: getting the test event 08/25/22 03:59:50.491
    STEP: deleting the test event 08/25/22 03:59:50.493
    STEP: listing events in all namespaces 08/25/22 03:59:50.498
    STEP: listing events in test namespace 08/25/22 03:59:50.506
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Aug 25 03:59:50.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5652" for this suite. 08/25/22 03:59:50.512
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:50.518
Aug 25 03:59:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename proxy 08/25/22 03:59:50.519
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:50.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:50.533
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 25 03:59:50.536: INFO: Creating pod...
Aug 25 03:59:50.542: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2174" to be "running"
Aug 25 03:59:50.544: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.710786ms
Aug 25 03:59:52.549: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007665619s
Aug 25 03:59:52.549: INFO: Pod "agnhost" satisfied condition "running"
Aug 25 03:59:52.549: INFO: Creating service...
Aug 25 03:59:52.559: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=DELETE
Aug 25 03:59:52.563: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 25 03:59:52.563: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=OPTIONS
Aug 25 03:59:52.566: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 25 03:59:52.566: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=PATCH
Aug 25 03:59:52.569: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 25 03:59:52.569: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=POST
Aug 25 03:59:52.572: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 25 03:59:52.572: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=PUT
Aug 25 03:59:52.574: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 25 03:59:52.574: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 25 03:59:52.578: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 25 03:59:52.578: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 25 03:59:52.581: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 25 03:59:52.581: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 25 03:59:52.584: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 25 03:59:52.584: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=POST
Aug 25 03:59:52.588: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 25 03:59:52.588: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=PUT
Aug 25 03:59:52.591: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 25 03:59:52.591: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=GET
Aug 25 03:59:52.593: INFO: http.Client request:GET StatusCode:301
Aug 25 03:59:52.593: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=GET
Aug 25 03:59:52.596: INFO: http.Client request:GET StatusCode:301
Aug 25 03:59:52.596: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=HEAD
Aug 25 03:59:52.598: INFO: http.Client request:HEAD StatusCode:301
Aug 25 03:59:52.598: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 25 03:59:52.600: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Aug 25 03:59:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2174" for this suite. 08/25/22 03:59:52.604
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":233,"skipped":4426,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.091 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:50.518
    Aug 25 03:59:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename proxy 08/25/22 03:59:50.519
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:50.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:50.533
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 25 03:59:50.536: INFO: Creating pod...
    Aug 25 03:59:50.542: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2174" to be "running"
    Aug 25 03:59:50.544: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.710786ms
    Aug 25 03:59:52.549: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007665619s
    Aug 25 03:59:52.549: INFO: Pod "agnhost" satisfied condition "running"
    Aug 25 03:59:52.549: INFO: Creating service...
    Aug 25 03:59:52.559: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=DELETE
    Aug 25 03:59:52.563: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 25 03:59:52.563: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=OPTIONS
    Aug 25 03:59:52.566: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 25 03:59:52.566: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=PATCH
    Aug 25 03:59:52.569: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 25 03:59:52.569: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=POST
    Aug 25 03:59:52.572: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 25 03:59:52.572: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=PUT
    Aug 25 03:59:52.574: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 25 03:59:52.574: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 25 03:59:52.578: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 25 03:59:52.578: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 25 03:59:52.581: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 25 03:59:52.581: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 25 03:59:52.584: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 25 03:59:52.584: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=POST
    Aug 25 03:59:52.588: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 25 03:59:52.588: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 25 03:59:52.591: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 25 03:59:52.591: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=GET
    Aug 25 03:59:52.593: INFO: http.Client request:GET StatusCode:301
    Aug 25 03:59:52.593: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=GET
    Aug 25 03:59:52.596: INFO: http.Client request:GET StatusCode:301
    Aug 25 03:59:52.596: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/pods/agnhost/proxy?method=HEAD
    Aug 25 03:59:52.598: INFO: http.Client request:HEAD StatusCode:301
    Aug 25 03:59:52.598: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2174/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 25 03:59:52.600: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Aug 25 03:59:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2174" for this suite. 08/25/22 03:59:52.604
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:52.611
Aug 25 03:59:52.611: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replication-controller 08/25/22 03:59:52.612
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:52.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:52.628
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 08/25/22 03:59:52.632
Aug 25 03:59:52.638: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5513" to be "running and ready"
Aug 25 03:59:52.641: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980252ms
Aug 25 03:59:52.641: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 25 03:59:54.646: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007454642s
Aug 25 03:59:54.646: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 25 03:59:54.646: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/25/22 03:59:54.648
STEP: Then the orphan pod is adopted 08/25/22 03:59:54.653
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 25 03:59:55.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5513" for this suite. 08/25/22 03:59:55.663
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":234,"skipped":4443,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.055 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:52.611
    Aug 25 03:59:52.611: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replication-controller 08/25/22 03:59:52.612
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:52.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:52.628
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/25/22 03:59:52.632
    Aug 25 03:59:52.638: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5513" to be "running and ready"
    Aug 25 03:59:52.641: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980252ms
    Aug 25 03:59:52.641: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 03:59:54.646: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007454642s
    Aug 25 03:59:54.646: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 25 03:59:54.646: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/25/22 03:59:54.648
    STEP: Then the orphan pod is adopted 08/25/22 03:59:54.653
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 25 03:59:55.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5513" for this suite. 08/25/22 03:59:55.663
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:55.67
Aug 25 03:59:55.671: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 03:59:55.672
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:55.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:55.683
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 08/25/22 03:59:55.686
Aug 25 03:59:55.686: INFO: Creating e2e-svc-a-92slf
Aug 25 03:59:55.691: INFO: Creating e2e-svc-b-fxx7f
Aug 25 03:59:55.698: INFO: Creating e2e-svc-c-z9htg
STEP: deleting service collection 08/25/22 03:59:55.708
Aug 25 03:59:55.733: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 03:59:55.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3359" for this suite. 08/25/22 03:59:55.735
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":235,"skipped":4511,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:55.67
    Aug 25 03:59:55.671: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 03:59:55.672
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:55.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:55.683
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 08/25/22 03:59:55.686
    Aug 25 03:59:55.686: INFO: Creating e2e-svc-a-92slf
    Aug 25 03:59:55.691: INFO: Creating e2e-svc-b-fxx7f
    Aug 25 03:59:55.698: INFO: Creating e2e-svc-c-z9htg
    STEP: deleting service collection 08/25/22 03:59:55.708
    Aug 25 03:59:55.733: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 03:59:55.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3359" for this suite. 08/25/22 03:59:55.735
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:55.738
Aug 25 03:59:55.738: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 03:59:55.739
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:55.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:55.749
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-456f31f9-566b-4f3f-a71d-7e69892e333f 08/25/22 03:59:55.752
STEP: Creating a pod to test consume secrets 08/25/22 03:59:55.755
Aug 25 03:59:55.760: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8" in namespace "projected-3702" to be "Succeeded or Failed"
Aug 25 03:59:55.763: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.495982ms
Aug 25 03:59:57.767: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.006868003s
Aug 25 03:59:59.768: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007834944s
STEP: Saw pod success 08/25/22 03:59:59.768
Aug 25 03:59:59.768: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8" satisfied condition "Succeeded or Failed"
Aug 25 03:59:59.772: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/25/22 03:59:59.778
Aug 25 03:59:59.785: INFO: Waiting for pod pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8 to disappear
Aug 25 03:59:59.788: INFO: Pod pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 03:59:59.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3702" for this suite. 08/25/22 03:59:59.792
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":236,"skipped":4515,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.058 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:55.738
    Aug 25 03:59:55.738: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 03:59:55.739
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:55.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:55.749
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-456f31f9-566b-4f3f-a71d-7e69892e333f 08/25/22 03:59:55.752
    STEP: Creating a pod to test consume secrets 08/25/22 03:59:55.755
    Aug 25 03:59:55.760: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8" in namespace "projected-3702" to be "Succeeded or Failed"
    Aug 25 03:59:55.763: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.495982ms
    Aug 25 03:59:57.767: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.006868003s
    Aug 25 03:59:59.768: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007834944s
    STEP: Saw pod success 08/25/22 03:59:59.768
    Aug 25 03:59:59.768: INFO: Pod "pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8" satisfied condition "Succeeded or Failed"
    Aug 25 03:59:59.772: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 03:59:59.778
    Aug 25 03:59:59.785: INFO: Waiting for pod pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8 to disappear
    Aug 25 03:59:59.788: INFO: Pod pod-projected-secrets-85d6dbac-33be-413b-a9ce-96c0298321e8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 03:59:59.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3702" for this suite. 08/25/22 03:59:59.792
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 03:59:59.797
Aug 25 03:59:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 03:59:59.798
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:59.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:59.814
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-a7003a8e-96ca-4c88-8b42-c345e8b3bd09 08/25/22 03:59:59.818
STEP: Creating a pod to test consume secrets 08/25/22 03:59:59.822
Aug 25 03:59:59.828: INFO: Waiting up to 5m0s for pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059" in namespace "secrets-373" to be "Succeeded or Failed"
Aug 25 03:59:59.831: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781289ms
Aug 25 04:00:01.836: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059": Phase="Running", Reason="", readiness=false. Elapsed: 2.007971137s
Aug 25 04:00:03.836: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007307667s
STEP: Saw pod success 08/25/22 04:00:03.836
Aug 25 04:00:03.836: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059" satisfied condition "Succeeded or Failed"
Aug 25 04:00:03.840: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 04:00:03.845
Aug 25 04:00:03.851: INFO: Waiting for pod pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059 to disappear
Aug 25 04:00:03.854: INFO: Pod pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:00:03.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-373" for this suite. 08/25/22 04:00:03.859
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":237,"skipped":4517,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 03:59:59.797
    Aug 25 03:59:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 03:59:59.798
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 03:59:59.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 03:59:59.814
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-a7003a8e-96ca-4c88-8b42-c345e8b3bd09 08/25/22 03:59:59.818
    STEP: Creating a pod to test consume secrets 08/25/22 03:59:59.822
    Aug 25 03:59:59.828: INFO: Waiting up to 5m0s for pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059" in namespace "secrets-373" to be "Succeeded or Failed"
    Aug 25 03:59:59.831: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781289ms
    Aug 25 04:00:01.836: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059": Phase="Running", Reason="", readiness=false. Elapsed: 2.007971137s
    Aug 25 04:00:03.836: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007307667s
    STEP: Saw pod success 08/25/22 04:00:03.836
    Aug 25 04:00:03.836: INFO: Pod "pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059" satisfied condition "Succeeded or Failed"
    Aug 25 04:00:03.840: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:00:03.845
    Aug 25 04:00:03.851: INFO: Waiting for pod pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059 to disappear
    Aug 25 04:00:03.854: INFO: Pod pod-secrets-0317cdb4-50ae-4b6f-85a5-33fa3ede4059 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:00:03.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-373" for this suite. 08/25/22 04:00:03.859
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:00:03.864
Aug 25 04:00:03.864: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-webhook 08/25/22 04:00:03.866
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:00:03.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:00:03.882
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/25/22 04:00:03.886
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/25/22 04:00:04.104
STEP: Deploying the custom resource conversion webhook pod 08/25/22 04:00:04.112
STEP: Wait for the deployment to be ready 08/25/22 04:00:04.122
Aug 25 04:00:04.130: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 25 04:00:06.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 04:00:08.147
STEP: Verifying the service has paired with the endpoint 08/25/22 04:00:08.157
Aug 25 04:00:09.157: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 25 04:00:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Creating a v1 custom resource 08/25/22 04:00:11.77
STEP: Create a v2 custom resource 08/25/22 04:00:11.78
STEP: List CRs in v1 08/25/22 04:00:11.835
STEP: List CRs in v2 08/25/22 04:00:11.84
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:00:12.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1068" for this suite. 08/25/22 04:00:12.362
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":238,"skipped":4521,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.527 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:00:03.864
    Aug 25 04:00:03.864: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-webhook 08/25/22 04:00:03.866
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:00:03.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:00:03.882
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/25/22 04:00:03.886
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/25/22 04:00:04.104
    STEP: Deploying the custom resource conversion webhook pod 08/25/22 04:00:04.112
    STEP: Wait for the deployment to be ready 08/25/22 04:00:04.122
    Aug 25 04:00:04.130: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Aug 25 04:00:06.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 0, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 04:00:08.147
    STEP: Verifying the service has paired with the endpoint 08/25/22 04:00:08.157
    Aug 25 04:00:09.157: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 25 04:00:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Creating a v1 custom resource 08/25/22 04:00:11.77
    STEP: Create a v2 custom resource 08/25/22 04:00:11.78
    STEP: List CRs in v1 08/25/22 04:00:11.835
    STEP: List CRs in v2 08/25/22 04:00:11.84
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:00:12.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1068" for this suite. 08/25/22 04:00:12.362
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:00:12.391
Aug 25 04:00:12.392: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename security-context 08/25/22 04:00:12.392
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:00:12.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:00:12.404
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/25/22 04:00:12.408
Aug 25 04:00:12.414: INFO: Waiting up to 5m0s for pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09" in namespace "security-context-3131" to be "Succeeded or Failed"
Aug 25 04:00:12.417: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90577ms
Aug 25 04:00:14.422: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007828391s
Aug 25 04:00:16.422: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008620654s
STEP: Saw pod success 08/25/22 04:00:16.422
Aug 25 04:00:16.423: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09" satisfied condition "Succeeded or Failed"
Aug 25 04:00:16.427: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09 container test-container: <nil>
STEP: delete the pod 08/25/22 04:00:16.432
Aug 25 04:00:16.440: INFO: Waiting for pod security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09 to disappear
Aug 25 04:00:16.444: INFO: Pod security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 25 04:00:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3131" for this suite. 08/25/22 04:00:16.447
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":239,"skipped":4524,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.061 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:00:12.391
    Aug 25 04:00:12.392: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename security-context 08/25/22 04:00:12.392
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:00:12.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:00:12.404
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/25/22 04:00:12.408
    Aug 25 04:00:12.414: INFO: Waiting up to 5m0s for pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09" in namespace "security-context-3131" to be "Succeeded or Failed"
    Aug 25 04:00:12.417: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90577ms
    Aug 25 04:00:14.422: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007828391s
    Aug 25 04:00:16.422: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008620654s
    STEP: Saw pod success 08/25/22 04:00:16.422
    Aug 25 04:00:16.423: INFO: Pod "security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09" satisfied condition "Succeeded or Failed"
    Aug 25 04:00:16.427: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09 container test-container: <nil>
    STEP: delete the pod 08/25/22 04:00:16.432
    Aug 25 04:00:16.440: INFO: Waiting for pod security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09 to disappear
    Aug 25 04:00:16.444: INFO: Pod security-context-ed5af6d2-c13f-4346-b84e-236bdde4ec09 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 25 04:00:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3131" for this suite. 08/25/22 04:00:16.447
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:00:16.453
Aug 25 04:00:16.453: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename cronjob 08/25/22 04:00:16.454
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:00:16.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:00:16.471
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/25/22 04:00:16.476
STEP: Ensuring a job is scheduled 08/25/22 04:00:16.479
STEP: Ensuring exactly one is scheduled 08/25/22 04:01:00.484
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/25/22 04:01:00.488
STEP: Ensuring no more jobs are scheduled 08/25/22 04:01:00.492
STEP: Removing cronjob 08/25/22 04:06:00.501
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 25 04:06:00.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9091" for this suite. 08/25/22 04:06:00.509
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":240,"skipped":4528,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [344.061 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:00:16.453
    Aug 25 04:00:16.453: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename cronjob 08/25/22 04:00:16.454
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:00:16.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:00:16.471
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/25/22 04:00:16.476
    STEP: Ensuring a job is scheduled 08/25/22 04:00:16.479
    STEP: Ensuring exactly one is scheduled 08/25/22 04:01:00.484
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/25/22 04:01:00.488
    STEP: Ensuring no more jobs are scheduled 08/25/22 04:01:00.492
    STEP: Removing cronjob 08/25/22 04:06:00.501
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 25 04:06:00.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9091" for this suite. 08/25/22 04:06:00.509
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:00.515
Aug 25 04:06:00.515: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 04:06:00.516
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:00.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:00.548
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2575 08/25/22 04:06:00.552
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Aug 25 04:06:00.561: INFO: Found 0 stateful pods, waiting for 1
Aug 25 04:06:10.566: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/25/22 04:06:10.574
W0825 04:06:10.579551      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 25 04:06:10.586: INFO: Found 1 stateful pods, waiting for 2
Aug 25 04:06:20.592: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 04:06:20.592: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/25/22 04:06:20.601
STEP: Delete all of the StatefulSets 08/25/22 04:06:20.605
STEP: Verify that StatefulSets have been deleted 08/25/22 04:06:20.61
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 04:06:20.614: INFO: Deleting all statefulset in ns statefulset-2575
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 04:06:20.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2575" for this suite. 08/25/22 04:06:20.627
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":241,"skipped":4533,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [20.118 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:00.515
    Aug 25 04:06:00.515: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 04:06:00.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:00.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:00.548
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2575 08/25/22 04:06:00.552
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Aug 25 04:06:00.561: INFO: Found 0 stateful pods, waiting for 1
    Aug 25 04:06:10.566: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/25/22 04:06:10.574
    W0825 04:06:10.579551      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 25 04:06:10.586: INFO: Found 1 stateful pods, waiting for 2
    Aug 25 04:06:20.592: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 04:06:20.592: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/25/22 04:06:20.601
    STEP: Delete all of the StatefulSets 08/25/22 04:06:20.605
    STEP: Verify that StatefulSets have been deleted 08/25/22 04:06:20.61
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 04:06:20.614: INFO: Deleting all statefulset in ns statefulset-2575
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 04:06:20.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2575" for this suite. 08/25/22 04:06:20.627
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:20.633
Aug 25 04:06:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename job 08/25/22 04:06:20.635
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:20.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:20.646
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 08/25/22 04:06:20.649
STEP: Ensuring active pods == parallelism 08/25/22 04:06:20.653
STEP: Orphaning one of the Job's Pods 08/25/22 04:06:24.66
Aug 25 04:06:25.175: INFO: Successfully updated pod "adopt-release-bttl9"
STEP: Checking that the Job readopts the Pod 08/25/22 04:06:25.175
Aug 25 04:06:25.175: INFO: Waiting up to 15m0s for pod "adopt-release-bttl9" in namespace "job-4862" to be "adopted"
Aug 25 04:06:25.178: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.810613ms
Aug 25 04:06:27.183: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008493341s
Aug 25 04:06:27.183: INFO: Pod "adopt-release-bttl9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/25/22 04:06:27.183
Aug 25 04:06:27.698: INFO: Successfully updated pod "adopt-release-bttl9"
STEP: Checking that the Job releases the Pod 08/25/22 04:06:27.698
Aug 25 04:06:27.698: INFO: Waiting up to 15m0s for pod "adopt-release-bttl9" in namespace "job-4862" to be "released"
Aug 25 04:06:27.702: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 3.815573ms
Aug 25 04:06:29.707: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008288468s
Aug 25 04:06:29.707: INFO: Pod "adopt-release-bttl9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 25 04:06:29.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4862" for this suite. 08/25/22 04:06:29.711
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":242,"skipped":4535,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [9.083 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:20.633
    Aug 25 04:06:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename job 08/25/22 04:06:20.635
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:20.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:20.646
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 08/25/22 04:06:20.649
    STEP: Ensuring active pods == parallelism 08/25/22 04:06:20.653
    STEP: Orphaning one of the Job's Pods 08/25/22 04:06:24.66
    Aug 25 04:06:25.175: INFO: Successfully updated pod "adopt-release-bttl9"
    STEP: Checking that the Job readopts the Pod 08/25/22 04:06:25.175
    Aug 25 04:06:25.175: INFO: Waiting up to 15m0s for pod "adopt-release-bttl9" in namespace "job-4862" to be "adopted"
    Aug 25 04:06:25.178: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.810613ms
    Aug 25 04:06:27.183: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008493341s
    Aug 25 04:06:27.183: INFO: Pod "adopt-release-bttl9" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/25/22 04:06:27.183
    Aug 25 04:06:27.698: INFO: Successfully updated pod "adopt-release-bttl9"
    STEP: Checking that the Job releases the Pod 08/25/22 04:06:27.698
    Aug 25 04:06:27.698: INFO: Waiting up to 15m0s for pod "adopt-release-bttl9" in namespace "job-4862" to be "released"
    Aug 25 04:06:27.702: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 3.815573ms
    Aug 25 04:06:29.707: INFO: Pod "adopt-release-bttl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008288468s
    Aug 25 04:06:29.707: INFO: Pod "adopt-release-bttl9" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 25 04:06:29.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4862" for this suite. 08/25/22 04:06:29.711
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:29.721
Aug 25 04:06:29.721: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:06:29.722
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:29.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:29.737
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 08/25/22 04:06:29.741
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/25/22 04:06:29.745
STEP: patching the secret 08/25/22 04:06:29.751
STEP: deleting the secret using a LabelSelector 08/25/22 04:06:29.759
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/25/22 04:06:29.763
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:06:29.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7685" for this suite. 08/25/22 04:06:29.772
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":243,"skipped":4581,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.055 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:29.721
    Aug 25 04:06:29.721: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:06:29.722
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:29.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:29.737
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 08/25/22 04:06:29.741
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/25/22 04:06:29.745
    STEP: patching the secret 08/25/22 04:06:29.751
    STEP: deleting the secret using a LabelSelector 08/25/22 04:06:29.759
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/25/22 04:06:29.763
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:06:29.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7685" for this suite. 08/25/22 04:06:29.772
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:29.777
Aug 25 04:06:29.777: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename containers 08/25/22 04:06:29.778
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:29.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:29.794
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 08/25/22 04:06:29.797
Aug 25 04:06:29.803: INFO: Waiting up to 5m0s for pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e" in namespace "containers-8822" to be "Succeeded or Failed"
Aug 25 04:06:29.805: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546223ms
Aug 25 04:06:31.811: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007976316s
Aug 25 04:06:33.811: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00842758s
STEP: Saw pod success 08/25/22 04:06:33.811
Aug 25 04:06:33.812: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e" satisfied condition "Succeeded or Failed"
Aug 25 04:06:33.816: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e container agnhost-container: <nil>
STEP: delete the pod 08/25/22 04:06:33.836
Aug 25 04:06:33.845: INFO: Waiting for pod client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e to disappear
Aug 25 04:06:33.847: INFO: Pod client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 25 04:06:33.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8822" for this suite. 08/25/22 04:06:33.85
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":244,"skipped":4597,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.078 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:29.777
    Aug 25 04:06:29.777: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename containers 08/25/22 04:06:29.778
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:29.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:29.794
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 08/25/22 04:06:29.797
    Aug 25 04:06:29.803: INFO: Waiting up to 5m0s for pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e" in namespace "containers-8822" to be "Succeeded or Failed"
    Aug 25 04:06:29.805: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546223ms
    Aug 25 04:06:31.811: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007976316s
    Aug 25 04:06:33.811: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00842758s
    STEP: Saw pod success 08/25/22 04:06:33.811
    Aug 25 04:06:33.812: INFO: Pod "client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e" satisfied condition "Succeeded or Failed"
    Aug 25 04:06:33.816: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 04:06:33.836
    Aug 25 04:06:33.845: INFO: Waiting for pod client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e to disappear
    Aug 25 04:06:33.847: INFO: Pod client-containers-b45f9b1f-c5c7-4715-af61-e63694544b2e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 25 04:06:33.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8822" for this suite. 08/25/22 04:06:33.85
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:33.855
Aug 25 04:06:33.855: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 04:06:33.856
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:33.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:33.872
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 04:06:33.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3635" for this suite. 08/25/22 04:06:33.882
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":245,"skipped":4597,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:33.855
    Aug 25 04:06:33.855: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 04:06:33.856
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:33.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:33.872
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 04:06:33.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3635" for this suite. 08/25/22 04:06:33.882
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:33.889
Aug 25 04:06:33.889: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename disruption 08/25/22 04:06:33.89
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:33.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:33.903
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 08/25/22 04:06:33.906
STEP: Waiting for the pdb to be processed 08/25/22 04:06:33.91
STEP: First trying to evict a pod which shouldn't be evictable 08/25/22 04:06:35.922
STEP: Waiting for all pods to be running 08/25/22 04:06:35.922
Aug 25 04:06:35.926: INFO: pods: 0 < 3
STEP: locating a running pod 08/25/22 04:06:37.932
STEP: Updating the pdb to allow a pod to be evicted 08/25/22 04:06:37.948
STEP: Waiting for the pdb to be processed 08/25/22 04:06:37.955
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/25/22 04:06:37.959
STEP: Waiting for all pods to be running 08/25/22 04:06:37.959
STEP: Waiting for the pdb to observed all healthy pods 08/25/22 04:06:37.962
STEP: Patching the pdb to disallow a pod to be evicted 08/25/22 04:06:37.978
STEP: Waiting for the pdb to be processed 08/25/22 04:06:37.989
STEP: Waiting for all pods to be running 08/25/22 04:06:39.996
STEP: locating a running pod 08/25/22 04:06:40.002
STEP: Deleting the pdb to allow a pod to be evicted 08/25/22 04:06:40.012
STEP: Waiting for the pdb to be deleted 08/25/22 04:06:40.017
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/25/22 04:06:40.021
STEP: Waiting for all pods to be running 08/25/22 04:06:40.021
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 25 04:06:40.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8451" for this suite. 08/25/22 04:06:40.038
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":246,"skipped":4647,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.154 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:33.889
    Aug 25 04:06:33.889: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename disruption 08/25/22 04:06:33.89
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:33.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:33.903
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 08/25/22 04:06:33.906
    STEP: Waiting for the pdb to be processed 08/25/22 04:06:33.91
    STEP: First trying to evict a pod which shouldn't be evictable 08/25/22 04:06:35.922
    STEP: Waiting for all pods to be running 08/25/22 04:06:35.922
    Aug 25 04:06:35.926: INFO: pods: 0 < 3
    STEP: locating a running pod 08/25/22 04:06:37.932
    STEP: Updating the pdb to allow a pod to be evicted 08/25/22 04:06:37.948
    STEP: Waiting for the pdb to be processed 08/25/22 04:06:37.955
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/25/22 04:06:37.959
    STEP: Waiting for all pods to be running 08/25/22 04:06:37.959
    STEP: Waiting for the pdb to observed all healthy pods 08/25/22 04:06:37.962
    STEP: Patching the pdb to disallow a pod to be evicted 08/25/22 04:06:37.978
    STEP: Waiting for the pdb to be processed 08/25/22 04:06:37.989
    STEP: Waiting for all pods to be running 08/25/22 04:06:39.996
    STEP: locating a running pod 08/25/22 04:06:40.002
    STEP: Deleting the pdb to allow a pod to be evicted 08/25/22 04:06:40.012
    STEP: Waiting for the pdb to be deleted 08/25/22 04:06:40.017
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/25/22 04:06:40.021
    STEP: Waiting for all pods to be running 08/25/22 04:06:40.021
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 25 04:06:40.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8451" for this suite. 08/25/22 04:06:40.038
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:40.045
Aug 25 04:06:40.045: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 04:06:40.047
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:40.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:40.061
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8502 08/25/22 04:06:40.064
STEP: creating replication controller nodeport-test in namespace services-8502 08/25/22 04:06:40.074
I0825 04:06:40.078694      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8502, replica count: 2
I0825 04:06:43.130226      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 04:06:43.130: INFO: Creating new exec pod
Aug 25 04:06:43.135: INFO: Waiting up to 5m0s for pod "execpodpzdgg" in namespace "services-8502" to be "running"
Aug 25 04:06:43.139: INFO: Pod "execpodpzdgg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958889ms
Aug 25 04:06:45.143: INFO: Pod "execpodpzdgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007783892s
Aug 25 04:06:45.143: INFO: Pod "execpodpzdgg" satisfied condition "running"
Aug 25 04:06:46.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 25 04:06:46.325: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 25 04:06:46.325: INFO: stdout: ""
Aug 25 04:06:47.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 25 04:06:47.537: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 25 04:06:47.537: INFO: stdout: "nodeport-test-ssnfs"
Aug 25 04:06:47.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.95.169 80'
Aug 25 04:06:47.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.95.169 80\nConnection to 10.110.95.169 80 port [tcp/http] succeeded!\n"
Aug 25 04:06:47.723: INFO: stdout: "nodeport-test-ssnfs"
Aug 25 04:06:47.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 30111'
Aug 25 04:06:47.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 30111\nConnection to 86.109.11.217 30111 port [tcp/*] succeeded!\n"
Aug 25 04:06:47.913: INFO: stdout: "nodeport-test-ssnfs"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 04:06:47.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8502" for this suite. 08/25/22 04:06:47.918
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":247,"skipped":4667,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.878 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:40.045
    Aug 25 04:06:40.045: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 04:06:40.047
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:40.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:40.061
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8502 08/25/22 04:06:40.064
    STEP: creating replication controller nodeport-test in namespace services-8502 08/25/22 04:06:40.074
    I0825 04:06:40.078694      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8502, replica count: 2
    I0825 04:06:43.130226      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 04:06:43.130: INFO: Creating new exec pod
    Aug 25 04:06:43.135: INFO: Waiting up to 5m0s for pod "execpodpzdgg" in namespace "services-8502" to be "running"
    Aug 25 04:06:43.139: INFO: Pod "execpodpzdgg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958889ms
    Aug 25 04:06:45.143: INFO: Pod "execpodpzdgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007783892s
    Aug 25 04:06:45.143: INFO: Pod "execpodpzdgg" satisfied condition "running"
    Aug 25 04:06:46.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Aug 25 04:06:46.325: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 25 04:06:46.325: INFO: stdout: ""
    Aug 25 04:06:47.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Aug 25 04:06:47.537: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 25 04:06:47.537: INFO: stdout: "nodeport-test-ssnfs"
    Aug 25 04:06:47.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.95.169 80'
    Aug 25 04:06:47.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.95.169 80\nConnection to 10.110.95.169 80 port [tcp/http] succeeded!\n"
    Aug 25 04:06:47.723: INFO: stdout: "nodeport-test-ssnfs"
    Aug 25 04:06:47.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-8502 exec execpodpzdgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 30111'
    Aug 25 04:06:47.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 30111\nConnection to 86.109.11.217 30111 port [tcp/*] succeeded!\n"
    Aug 25 04:06:47.913: INFO: stdout: "nodeport-test-ssnfs"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 04:06:47.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8502" for this suite. 08/25/22 04:06:47.918
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:47.925
Aug 25 04:06:47.925: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename job 08/25/22 04:06:47.927
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:47.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:47.942
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 08/25/22 04:06:47.95
STEP: Patching the Job 08/25/22 04:06:47.953
STEP: Watching for Job to be patched 08/25/22 04:06:47.967
Aug 25 04:06:47.969: INFO: Event ADDED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 25 04:06:47.969: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 25 04:06:47.969: INFO: Event MODIFIED found for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/25/22 04:06:47.969
STEP: Watching for Job to be updated 08/25/22 04:06:47.979
Aug 25 04:06:47.981: INFO: Event MODIFIED found for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 25 04:06:47.981: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/25/22 04:06:47.981
Aug 25 04:06:47.984: INFO: Job: e2e-qd4xk as labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched]
STEP: Waiting for job to complete 08/25/22 04:06:47.984
STEP: Delete a job collection with a labelselector 08/25/22 04:06:57.99
STEP: Watching for Job to be deleted 08/25/22 04:06:57.996
Aug 25 04:06:58.000: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 25 04:06:58.000: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 25 04:06:58.000: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 25 04:06:58.001: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 25 04:06:58.001: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 25 04:06:58.001: INFO: Event DELETED found for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/25/22 04:06:58.001
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 25 04:06:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4976" for this suite. 08/25/22 04:06:58.009
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":248,"skipped":4689,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:47.925
    Aug 25 04:06:47.925: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename job 08/25/22 04:06:47.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:47.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:47.942
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 08/25/22 04:06:47.95
    STEP: Patching the Job 08/25/22 04:06:47.953
    STEP: Watching for Job to be patched 08/25/22 04:06:47.967
    Aug 25 04:06:47.969: INFO: Event ADDED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 25 04:06:47.969: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 25 04:06:47.969: INFO: Event MODIFIED found for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/25/22 04:06:47.969
    STEP: Watching for Job to be updated 08/25/22 04:06:47.979
    Aug 25 04:06:47.981: INFO: Event MODIFIED found for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 25 04:06:47.981: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/25/22 04:06:47.981
    Aug 25 04:06:47.984: INFO: Job: e2e-qd4xk as labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched]
    STEP: Waiting for job to complete 08/25/22 04:06:47.984
    STEP: Delete a job collection with a labelselector 08/25/22 04:06:57.99
    STEP: Watching for Job to be deleted 08/25/22 04:06:57.996
    Aug 25 04:06:58.000: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 25 04:06:58.000: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 25 04:06:58.000: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 25 04:06:58.001: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 25 04:06:58.001: INFO: Event MODIFIED observed for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 25 04:06:58.001: INFO: Event DELETED found for Job e2e-qd4xk in namespace job-4976 with labels: map[e2e-job-label:e2e-qd4xk e2e-qd4xk:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/25/22 04:06:58.001
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 25 04:06:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4976" for this suite. 08/25/22 04:06:58.009
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:06:58.014
Aug 25 04:06:58.014: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:06:58.015
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:58.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:58.028
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-c004bd43-1f76-4c1a-945b-2d5b96460951 08/25/22 04:06:58.032
STEP: Creating a pod to test consume configMaps 08/25/22 04:06:58.035
Aug 25 04:06:58.041: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7" in namespace "configmap-7436" to be "Succeeded or Failed"
Aug 25 04:06:58.044: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26045ms
Aug 25 04:07:00.050: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008848606s
Aug 25 04:07:02.049: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00780491s
STEP: Saw pod success 08/25/22 04:07:02.049
Aug 25 04:07:02.049: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7" satisfied condition "Succeeded or Failed"
Aug 25 04:07:02.053: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7 container configmap-volume-test: <nil>
STEP: delete the pod 08/25/22 04:07:02.058
Aug 25 04:07:02.066: INFO: Waiting for pod pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7 to disappear
Aug 25 04:07:02.070: INFO: Pod pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:07:02.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7436" for this suite. 08/25/22 04:07:02.077
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":249,"skipped":4702,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:06:58.014
    Aug 25 04:06:58.014: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:06:58.015
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:06:58.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:06:58.028
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-c004bd43-1f76-4c1a-945b-2d5b96460951 08/25/22 04:06:58.032
    STEP: Creating a pod to test consume configMaps 08/25/22 04:06:58.035
    Aug 25 04:06:58.041: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7" in namespace "configmap-7436" to be "Succeeded or Failed"
    Aug 25 04:06:58.044: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26045ms
    Aug 25 04:07:00.050: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008848606s
    Aug 25 04:07:02.049: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00780491s
    STEP: Saw pod success 08/25/22 04:07:02.049
    Aug 25 04:07:02.049: INFO: Pod "pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:02.053: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7 container configmap-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:07:02.058
    Aug 25 04:07:02.066: INFO: Waiting for pod pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7 to disappear
    Aug 25 04:07:02.070: INFO: Pod pod-configmaps-e0203bda-ca9f-47cf-9868-6557b65c79b7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:07:02.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7436" for this suite. 08/25/22 04:07:02.077
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:02.082
Aug 25 04:07:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:07:02.083
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:02.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:02.098
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-ade567f5-a7ed-415c-8d2e-0f9ff3018178 08/25/22 04:07:02.102
STEP: Creating a pod to test consume secrets 08/25/22 04:07:02.105
Aug 25 04:07:02.112: INFO: Waiting up to 5m0s for pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41" in namespace "secrets-4847" to be "Succeeded or Failed"
Aug 25 04:07:02.115: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.950066ms
Aug 25 04:07:04.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008666941s
Aug 25 04:07:06.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007804546s
Aug 25 04:07:08.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008388287s
STEP: Saw pod success 08/25/22 04:07:08.12
Aug 25 04:07:08.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41" satisfied condition "Succeeded or Failed"
Aug 25 04:07:08.125: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 04:07:08.13
Aug 25 04:07:08.137: INFO: Waiting for pod pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41 to disappear
Aug 25 04:07:08.139: INFO: Pod pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:07:08.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4847" for this suite. 08/25/22 04:07:08.143
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":250,"skipped":4712,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:02.082
    Aug 25 04:07:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:07:02.083
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:02.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:02.098
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-ade567f5-a7ed-415c-8d2e-0f9ff3018178 08/25/22 04:07:02.102
    STEP: Creating a pod to test consume secrets 08/25/22 04:07:02.105
    Aug 25 04:07:02.112: INFO: Waiting up to 5m0s for pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41" in namespace "secrets-4847" to be "Succeeded or Failed"
    Aug 25 04:07:02.115: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.950066ms
    Aug 25 04:07:04.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008666941s
    Aug 25 04:07:06.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007804546s
    Aug 25 04:07:08.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008388287s
    STEP: Saw pod success 08/25/22 04:07:08.12
    Aug 25 04:07:08.120: INFO: Pod "pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:08.125: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:07:08.13
    Aug 25 04:07:08.137: INFO: Waiting for pod pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41 to disappear
    Aug 25 04:07:08.139: INFO: Pod pod-secrets-f2827903-8e56-41cd-9ee3-a2539ab58b41 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:07:08.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4847" for this suite. 08/25/22 04:07:08.143
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:08.152
Aug 25 04:07:08.152: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 04:07:08.153
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:08.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:08.166
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Aug 25 04:07:08.180: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/25/22 04:07:08.184
Aug 25 04:07:08.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:08.187: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/25/22 04:07:08.187
Aug 25 04:07:08.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:08.205: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:09.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:09.210: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:10.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:07:10.211: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/25/22 04:07:10.215
Aug 25 04:07:10.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:07:10.238: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 25 04:07:11.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:11.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/25/22 04:07:11.242
Aug 25 04:07:11.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:11.262: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:12.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:12.267: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:13.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:13.266: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:14.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:07:14.268: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:07:14.276
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3160, will wait for the garbage collector to delete the pods 08/25/22 04:07:14.276
Aug 25 04:07:14.336: INFO: Deleting DaemonSet.extensions daemon-set took: 5.869309ms
Aug 25 04:07:14.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.191151ms
Aug 25 04:07:16.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:16.741: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 25 04:07:16.745: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46959"},"items":null}

Aug 25 04:07:16.748: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46959"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:07:16.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3160" for this suite. 08/25/22 04:07:16.773
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":251,"skipped":4735,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.626 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:08.152
    Aug 25 04:07:08.152: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 04:07:08.153
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:08.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:08.166
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Aug 25 04:07:08.180: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/25/22 04:07:08.184
    Aug 25 04:07:08.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:08.187: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/25/22 04:07:08.187
    Aug 25 04:07:08.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:08.205: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:09.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:09.210: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:10.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:07:10.211: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/25/22 04:07:10.215
    Aug 25 04:07:10.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:07:10.238: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 25 04:07:11.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:11.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/25/22 04:07:11.242
    Aug 25 04:07:11.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:11.262: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:12.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:12.267: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:13.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:13.266: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:14.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:07:14.268: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:07:14.276
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3160, will wait for the garbage collector to delete the pods 08/25/22 04:07:14.276
    Aug 25 04:07:14.336: INFO: Deleting DaemonSet.extensions daemon-set took: 5.869309ms
    Aug 25 04:07:14.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.191151ms
    Aug 25 04:07:16.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:16.741: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 25 04:07:16.745: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46959"},"items":null}

    Aug 25 04:07:16.748: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46959"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:07:16.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3160" for this suite. 08/25/22 04:07:16.773
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:16.78
Aug 25 04:07:16.780: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svcaccounts 08/25/22 04:07:16.781
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:16.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:16.796
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  08/25/22 04:07:16.799
Aug 25 04:07:16.804: INFO: Waiting up to 5m0s for pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd" in namespace "svcaccounts-2890" to be "Succeeded or Failed"
Aug 25 04:07:16.807: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7149ms
Aug 25 04:07:18.811: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.007113851s
Aug 25 04:07:20.812: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007741811s
STEP: Saw pod success 08/25/22 04:07:20.812
Aug 25 04:07:20.812: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd" satisfied condition "Succeeded or Failed"
Aug 25 04:07:20.817: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd container agnhost-container: <nil>
STEP: delete the pod 08/25/22 04:07:20.822
Aug 25 04:07:20.831: INFO: Waiting for pod test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd to disappear
Aug 25 04:07:20.834: INFO: Pod test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 25 04:07:20.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2890" for this suite. 08/25/22 04:07:20.839
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":252,"skipped":4763,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.063 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:16.78
    Aug 25 04:07:16.780: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svcaccounts 08/25/22 04:07:16.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:16.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:16.796
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  08/25/22 04:07:16.799
    Aug 25 04:07:16.804: INFO: Waiting up to 5m0s for pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd" in namespace "svcaccounts-2890" to be "Succeeded or Failed"
    Aug 25 04:07:16.807: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7149ms
    Aug 25 04:07:18.811: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.007113851s
    Aug 25 04:07:20.812: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007741811s
    STEP: Saw pod success 08/25/22 04:07:20.812
    Aug 25 04:07:20.812: INFO: Pod "test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:20.817: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 04:07:20.822
    Aug 25 04:07:20.831: INFO: Waiting for pod test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd to disappear
    Aug 25 04:07:20.834: INFO: Pod test-pod-d5a1a0bb-1c33-4fc5-8632-bf24118759cd no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 25 04:07:20.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2890" for this suite. 08/25/22 04:07:20.839
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:20.844
Aug 25 04:07:20.844: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:07:20.846
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:20.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:20.861
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 08/25/22 04:07:20.865
Aug 25 04:07:20.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592" in namespace "downward-api-9445" to be "Succeeded or Failed"
Aug 25 04:07:20.876: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109367ms
Aug 25 04:07:22.882: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592": Phase="Running", Reason="", readiness=false. Elapsed: 2.010582391s
Aug 25 04:07:24.882: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009884327s
STEP: Saw pod success 08/25/22 04:07:24.882
Aug 25 04:07:24.882: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592" satisfied condition "Succeeded or Failed"
Aug 25 04:07:24.886: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592 container client-container: <nil>
STEP: delete the pod 08/25/22 04:07:24.892
Aug 25 04:07:24.899: INFO: Waiting for pod downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592 to disappear
Aug 25 04:07:24.902: INFO: Pod downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 04:07:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9445" for this suite. 08/25/22 04:07:24.906
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":253,"skipped":4773,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:20.844
    Aug 25 04:07:20.844: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:07:20.846
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:20.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:20.861
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 08/25/22 04:07:20.865
    Aug 25 04:07:20.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592" in namespace "downward-api-9445" to be "Succeeded or Failed"
    Aug 25 04:07:20.876: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109367ms
    Aug 25 04:07:22.882: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592": Phase="Running", Reason="", readiness=false. Elapsed: 2.010582391s
    Aug 25 04:07:24.882: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009884327s
    STEP: Saw pod success 08/25/22 04:07:24.882
    Aug 25 04:07:24.882: INFO: Pod "downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:24.886: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592 container client-container: <nil>
    STEP: delete the pod 08/25/22 04:07:24.892
    Aug 25 04:07:24.899: INFO: Waiting for pod downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592 to disappear
    Aug 25 04:07:24.902: INFO: Pod downwardapi-volume-fbdda5b5-b697-4869-91c9-3a28dc5e0592 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 04:07:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9445" for this suite. 08/25/22 04:07:24.906
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:24.912
Aug 25 04:07:24.912: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 04:07:24.914
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:24.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:24.929
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/25/22 04:07:24.933
Aug 25 04:07:24.939: INFO: Waiting up to 5m0s for pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c" in namespace "emptydir-5443" to be "Succeeded or Failed"
Aug 25 04:07:24.942: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142491ms
Aug 25 04:07:26.947: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008109107s
Aug 25 04:07:28.948: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009223523s
STEP: Saw pod success 08/25/22 04:07:28.949
Aug 25 04:07:28.949: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c" satisfied condition "Succeeded or Failed"
Aug 25 04:07:28.953: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c container test-container: <nil>
STEP: delete the pod 08/25/22 04:07:28.957
Aug 25 04:07:28.963: INFO: Waiting for pod pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c to disappear
Aug 25 04:07:28.967: INFO: Pod pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 04:07:28.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5443" for this suite. 08/25/22 04:07:28.971
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4792,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:24.912
    Aug 25 04:07:24.912: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 04:07:24.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:24.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:24.929
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/25/22 04:07:24.933
    Aug 25 04:07:24.939: INFO: Waiting up to 5m0s for pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c" in namespace "emptydir-5443" to be "Succeeded or Failed"
    Aug 25 04:07:24.942: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142491ms
    Aug 25 04:07:26.947: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008109107s
    Aug 25 04:07:28.948: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009223523s
    STEP: Saw pod success 08/25/22 04:07:28.949
    Aug 25 04:07:28.949: INFO: Pod "pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:28.953: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c container test-container: <nil>
    STEP: delete the pod 08/25/22 04:07:28.957
    Aug 25 04:07:28.963: INFO: Waiting for pod pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c to disappear
    Aug 25 04:07:28.967: INFO: Pod pod-9be0f1bb-f1ba-46d8-b527-6f213ccb980c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 04:07:28.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5443" for this suite. 08/25/22 04:07:28.971
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:28.979
Aug 25 04:07:28.980: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:07:28.981
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:28.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:28.996
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 08/25/22 04:07:29
Aug 25 04:07:29.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111" in namespace "projected-5459" to be "Succeeded or Failed"
Aug 25 04:07:29.008: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732397ms
Aug 25 04:07:31.013: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007380077s
Aug 25 04:07:33.014: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008120347s
STEP: Saw pod success 08/25/22 04:07:33.014
Aug 25 04:07:33.014: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111" satisfied condition "Succeeded or Failed"
Aug 25 04:07:33.018: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111 container client-container: <nil>
STEP: delete the pod 08/25/22 04:07:33.023
Aug 25 04:07:33.030: INFO: Waiting for pod downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111 to disappear
Aug 25 04:07:33.033: INFO: Pod downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 04:07:33.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5459" for this suite. 08/25/22 04:07:33.038
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":255,"skipped":4831,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.063 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:28.979
    Aug 25 04:07:28.980: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:07:28.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:28.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:28.996
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 08/25/22 04:07:29
    Aug 25 04:07:29.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111" in namespace "projected-5459" to be "Succeeded or Failed"
    Aug 25 04:07:29.008: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732397ms
    Aug 25 04:07:31.013: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007380077s
    Aug 25 04:07:33.014: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008120347s
    STEP: Saw pod success 08/25/22 04:07:33.014
    Aug 25 04:07:33.014: INFO: Pod "downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:33.018: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111 container client-container: <nil>
    STEP: delete the pod 08/25/22 04:07:33.023
    Aug 25 04:07:33.030: INFO: Waiting for pod downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111 to disappear
    Aug 25 04:07:33.033: INFO: Pod downwardapi-volume-c11c6ad5-6516-496b-83eb-701e793ae111 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 04:07:33.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5459" for this suite. 08/25/22 04:07:33.038
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:33.043
Aug 25 04:07:33.043: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replicaset 08/25/22 04:07:33.044
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:33.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:33.061
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/25/22 04:07:33.065
STEP: Verify that the required pods have come up 08/25/22 04:07:33.069
Aug 25 04:07:33.073: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 25 04:07:38.079: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/25/22 04:07:38.079
Aug 25 04:07:38.082: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/25/22 04:07:38.082
STEP: DeleteCollection of the ReplicaSets 08/25/22 04:07:38.088
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/25/22 04:07:38.092
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 25 04:07:38.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5855" for this suite. 08/25/22 04:07:38.102
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":256,"skipped":4837,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.063 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:33.043
    Aug 25 04:07:33.043: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replicaset 08/25/22 04:07:33.044
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:33.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:33.061
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/25/22 04:07:33.065
    STEP: Verify that the required pods have come up 08/25/22 04:07:33.069
    Aug 25 04:07:33.073: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 25 04:07:38.079: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/25/22 04:07:38.079
    Aug 25 04:07:38.082: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/25/22 04:07:38.082
    STEP: DeleteCollection of the ReplicaSets 08/25/22 04:07:38.088
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/25/22 04:07:38.092
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 25 04:07:38.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5855" for this suite. 08/25/22 04:07:38.102
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:38.108
Aug 25 04:07:38.108: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 04:07:38.11
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:38.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:38.125
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 08/25/22 04:07:38.144
STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:07:38.147
Aug 25 04:07:38.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:38.152: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:39.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:39.160: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:40.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:07:40.161: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/25/22 04:07:40.165
Aug 25 04:07:40.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:40.183: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:41.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:41.193: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:07:42.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:07:42.192: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/25/22 04:07:42.192
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:07:42.199
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2394, will wait for the garbage collector to delete the pods 08/25/22 04:07:42.199
Aug 25 04:07:42.259: INFO: Deleting DaemonSet.extensions daemon-set took: 5.174921ms
Aug 25 04:07:42.359: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.50194ms
Aug 25 04:07:44.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:07:44.864: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 25 04:07:44.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47335"},"items":null}

Aug 25 04:07:44.871: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47335"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:07:44.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2394" for this suite. 08/25/22 04:07:44.885
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":257,"skipped":4852,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.783 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:38.108
    Aug 25 04:07:38.108: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 04:07:38.11
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:38.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:38.125
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 08/25/22 04:07:38.144
    STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:07:38.147
    Aug 25 04:07:38.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:38.152: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:39.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:39.160: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:40.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:07:40.161: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/25/22 04:07:40.165
    Aug 25 04:07:40.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:40.183: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:41.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:41.193: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:07:42.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:07:42.192: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/25/22 04:07:42.192
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:07:42.199
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2394, will wait for the garbage collector to delete the pods 08/25/22 04:07:42.199
    Aug 25 04:07:42.259: INFO: Deleting DaemonSet.extensions daemon-set took: 5.174921ms
    Aug 25 04:07:42.359: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.50194ms
    Aug 25 04:07:44.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:07:44.864: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 25 04:07:44.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47335"},"items":null}

    Aug 25 04:07:44.871: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47335"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:07:44.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2394" for this suite. 08/25/22 04:07:44.885
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:44.892
Aug 25 04:07:44.892: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename limitrange 08/25/22 04:07:44.893
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:44.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:44.909
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 08/25/22 04:07:44.913
STEP: Setting up watch 08/25/22 04:07:44.913
STEP: Submitting a LimitRange 08/25/22 04:07:45.016
STEP: Verifying LimitRange creation was observed 08/25/22 04:07:45.023
STEP: Fetching the LimitRange to ensure it has proper values 08/25/22 04:07:45.023
Aug 25 04:07:45.027: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 25 04:07:45.027: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/25/22 04:07:45.027
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/25/22 04:07:45.03
Aug 25 04:07:45.033: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 25 04:07:45.033: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/25/22 04:07:45.033
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/25/22 04:07:45.038
Aug 25 04:07:45.041: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 25 04:07:45.041: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/25/22 04:07:45.041
STEP: Failing to create a Pod with more than max resources 08/25/22 04:07:45.045
STEP: Updating a LimitRange 08/25/22 04:07:45.047
STEP: Verifying LimitRange updating is effective 08/25/22 04:07:45.051
STEP: Creating a Pod with less than former min resources 08/25/22 04:07:47.057
STEP: Failing to create a Pod with more than max resources 08/25/22 04:07:47.063
STEP: Deleting a LimitRange 08/25/22 04:07:47.067
STEP: Verifying the LimitRange was deleted 08/25/22 04:07:47.072
Aug 25 04:07:52.076: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/25/22 04:07:52.076
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Aug 25 04:07:52.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2631" for this suite. 08/25/22 04:07:52.088
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":258,"skipped":4879,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [7.201 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:44.892
    Aug 25 04:07:44.892: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename limitrange 08/25/22 04:07:44.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:44.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:44.909
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 08/25/22 04:07:44.913
    STEP: Setting up watch 08/25/22 04:07:44.913
    STEP: Submitting a LimitRange 08/25/22 04:07:45.016
    STEP: Verifying LimitRange creation was observed 08/25/22 04:07:45.023
    STEP: Fetching the LimitRange to ensure it has proper values 08/25/22 04:07:45.023
    Aug 25 04:07:45.027: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 25 04:07:45.027: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/25/22 04:07:45.027
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/25/22 04:07:45.03
    Aug 25 04:07:45.033: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 25 04:07:45.033: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/25/22 04:07:45.033
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/25/22 04:07:45.038
    Aug 25 04:07:45.041: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 25 04:07:45.041: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/25/22 04:07:45.041
    STEP: Failing to create a Pod with more than max resources 08/25/22 04:07:45.045
    STEP: Updating a LimitRange 08/25/22 04:07:45.047
    STEP: Verifying LimitRange updating is effective 08/25/22 04:07:45.051
    STEP: Creating a Pod with less than former min resources 08/25/22 04:07:47.057
    STEP: Failing to create a Pod with more than max resources 08/25/22 04:07:47.063
    STEP: Deleting a LimitRange 08/25/22 04:07:47.067
    STEP: Verifying the LimitRange was deleted 08/25/22 04:07:47.072
    Aug 25 04:07:52.076: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/25/22 04:07:52.076
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Aug 25 04:07:52.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-2631" for this suite. 08/25/22 04:07:52.088
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:52.094
Aug 25 04:07:52.094: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:07:52.095
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:52.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:52.111
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 08/25/22 04:07:52.115
Aug 25 04:07:52.122: INFO: Waiting up to 5m0s for pod "downward-api-7719a429-7819-4431-8293-a364f9780da3" in namespace "downward-api-684" to be "Succeeded or Failed"
Aug 25 04:07:52.125: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.312995ms
Aug 25 04:07:54.131: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009377755s
Aug 25 04:07:56.130: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008155716s
STEP: Saw pod success 08/25/22 04:07:56.13
Aug 25 04:07:56.130: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3" satisfied condition "Succeeded or Failed"
Aug 25 04:07:56.135: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-7719a429-7819-4431-8293-a364f9780da3 container dapi-container: <nil>
STEP: delete the pod 08/25/22 04:07:56.141
Aug 25 04:07:56.149: INFO: Waiting for pod downward-api-7719a429-7819-4431-8293-a364f9780da3 to disappear
Aug 25 04:07:56.153: INFO: Pod downward-api-7719a429-7819-4431-8293-a364f9780da3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 25 04:07:56.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-684" for this suite. 08/25/22 04:07:56.157
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":259,"skipped":4913,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:52.094
    Aug 25 04:07:52.094: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:07:52.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:52.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:52.111
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 08/25/22 04:07:52.115
    Aug 25 04:07:52.122: INFO: Waiting up to 5m0s for pod "downward-api-7719a429-7819-4431-8293-a364f9780da3" in namespace "downward-api-684" to be "Succeeded or Failed"
    Aug 25 04:07:52.125: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.312995ms
    Aug 25 04:07:54.131: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009377755s
    Aug 25 04:07:56.130: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008155716s
    STEP: Saw pod success 08/25/22 04:07:56.13
    Aug 25 04:07:56.130: INFO: Pod "downward-api-7719a429-7819-4431-8293-a364f9780da3" satisfied condition "Succeeded or Failed"
    Aug 25 04:07:56.135: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-7719a429-7819-4431-8293-a364f9780da3 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 04:07:56.141
    Aug 25 04:07:56.149: INFO: Waiting for pod downward-api-7719a429-7819-4431-8293-a364f9780da3 to disappear
    Aug 25 04:07:56.153: INFO: Pod downward-api-7719a429-7819-4431-8293-a364f9780da3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 25 04:07:56.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-684" for this suite. 08/25/22 04:07:56.157
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:56.164
Aug 25 04:07:56.164: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubelet-test 08/25/22 04:07:56.165
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:56.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:56.18
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 25 04:07:56.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8312" for this suite. 08/25/22 04:07:56.2
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":260,"skipped":4929,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.040 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:56.164
    Aug 25 04:07:56.164: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubelet-test 08/25/22 04:07:56.165
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:56.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:56.18
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 25 04:07:56.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8312" for this suite. 08/25/22 04:07:56.2
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:56.206
Aug 25 04:07:56.206: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:07:56.207
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:56.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:56.219
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:07:56.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4570" for this suite. 08/25/22 04:07:56.258
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":261,"skipped":4945,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.056 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:56.206
    Aug 25 04:07:56.206: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:07:56.207
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:56.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:56.219
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:07:56.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4570" for this suite. 08/25/22 04:07:56.258
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:07:56.263
Aug 25 04:07:56.263: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 04:07:56.264
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:56.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:56.279
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 08/25/22 04:07:56.284
Aug 25 04:07:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: rename a version 08/25/22 04:08:05.716
STEP: check the new version name is served 08/25/22 04:08:05.73
STEP: check the old version name is removed 08/25/22 04:08:09.297
STEP: check the other version is not changed 08/25/22 04:08:11.058
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:08:18.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6685" for this suite. 08/25/22 04:08:18.689
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":262,"skipped":4955,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [22.432 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:07:56.263
    Aug 25 04:07:56.263: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 04:07:56.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:07:56.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:07:56.279
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 08/25/22 04:07:56.284
    Aug 25 04:07:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: rename a version 08/25/22 04:08:05.716
    STEP: check the new version name is served 08/25/22 04:08:05.73
    STEP: check the old version name is removed 08/25/22 04:08:09.297
    STEP: check the other version is not changed 08/25/22 04:08:11.058
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:08:18.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6685" for this suite. 08/25/22 04:08:18.689
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:08:18.696
Aug 25 04:08:18.696: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replication-controller 08/25/22 04:08:18.697
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:08:18.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:08:18.713
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Aug 25 04:08:18.717: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/25/22 04:08:19.726
STEP: Checking rc "condition-test" has the desired failure condition set 08/25/22 04:08:19.732
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/25/22 04:08:20.741
Aug 25 04:08:20.752: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/25/22 04:08:20.752
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 25 04:08:21.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8235" for this suite. 08/25/22 04:08:21.766
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":263,"skipped":4959,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.075 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:08:18.696
    Aug 25 04:08:18.696: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replication-controller 08/25/22 04:08:18.697
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:08:18.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:08:18.713
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Aug 25 04:08:18.717: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/25/22 04:08:19.726
    STEP: Checking rc "condition-test" has the desired failure condition set 08/25/22 04:08:19.732
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/25/22 04:08:20.741
    Aug 25 04:08:20.752: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/25/22 04:08:20.752
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 25 04:08:21.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8235" for this suite. 08/25/22 04:08:21.766
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:08:21.772
Aug 25 04:08:21.773: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename statefulset 08/25/22 04:08:21.774
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:08:21.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:08:21.79
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1855 08/25/22 04:08:21.792
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-1855 08/25/22 04:08:21.796
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1855 08/25/22 04:08:21.801
Aug 25 04:08:21.804: INFO: Found 0 stateful pods, waiting for 1
Aug 25 04:08:31.810: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/25/22 04:08:31.81
Aug 25 04:08:31.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 04:08:32.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 04:08:32.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 04:08:32.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 04:08:32.022: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 25 04:08:42.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 04:08:42.029: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 04:08:42.044: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Aug 25 04:08:42.045: INFO: ss-0  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  }]
Aug 25 04:08:42.045: INFO: 
Aug 25 04:08:42.045: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 25 04:08:43.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995772967s
Aug 25 04:08:44.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990229767s
Aug 25 04:08:45.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984363803s
Aug 25 04:08:46.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97796641s
Aug 25 04:08:47.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972478628s
Aug 25 04:08:48.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967015109s
Aug 25 04:08:49.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961406787s
Aug 25 04:08:50.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95533361s
Aug 25 04:08:51.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.465432ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1855 08/25/22 04:08:52.096
Aug 25 04:08:52.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 04:08:52.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 25 04:08:52.311: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 04:08:52.311: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 04:08:52.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 04:08:52.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 25 04:08:52.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 04:08:52.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 04:08:52.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 25 04:08:52.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 25 04:08:52.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 25 04:08:52.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 25 04:08:52.649: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 25 04:09:02.657: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 04:09:02.657: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 25 04:09:02.657: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/25/22 04:09:02.657
Aug 25 04:09:02.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 04:09:02.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 04:09:02.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 04:09:02.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 04:09:02.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 04:09:03.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 04:09:03.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 04:09:03.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 04:09:03.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 25 04:09:03.213: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 25 04:09:03.213: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 25 04:09:03.213: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 25 04:09:03.213: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 04:09:03.217: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 25 04:09:13.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 04:09:13.230: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 04:09:13.230: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 25 04:09:13.244: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Aug 25 04:09:13.244: INFO: ss-0  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  }]
Aug 25 04:09:13.244: INFO: ss-1  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
Aug 25 04:09:13.244: INFO: ss-2  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
Aug 25 04:09:13.244: INFO: 
Aug 25 04:09:13.244: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 25 04:09:14.249: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Aug 25 04:09:14.249: INFO: ss-1  bobymcbobs-c849-control-plane-p55pp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
Aug 25 04:09:14.249: INFO: ss-2  bobymcbobs-c849-control-plane-p55pp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
Aug 25 04:09:14.249: INFO: 
Aug 25 04:09:14.249: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 25 04:09:15.253: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990602941s
Aug 25 04:09:16.258: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986461692s
Aug 25 04:09:17.261: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982306701s
Aug 25 04:09:18.266: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.978377633s
Aug 25 04:09:19.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974341608s
Aug 25 04:09:20.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.970128445s
Aug 25 04:09:21.279: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965578302s
Aug 25 04:09:22.282: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.301856ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1855 08/25/22 04:09:23.283
Aug 25 04:09:23.288: INFO: Scaling statefulset ss to 0
Aug 25 04:09:23.302: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 25 04:09:23.305: INFO: Deleting all statefulset in ns statefulset-1855
Aug 25 04:09:23.308: INFO: Scaling statefulset ss to 0
Aug 25 04:09:23.321: INFO: Waiting for statefulset status.replicas updated to 0
Aug 25 04:09:23.324: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 25 04:09:23.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1855" for this suite. 08/25/22 04:09:23.335
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":264,"skipped":4976,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [61.566 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:08:21.772
    Aug 25 04:08:21.773: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename statefulset 08/25/22 04:08:21.774
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:08:21.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:08:21.79
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1855 08/25/22 04:08:21.792
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-1855 08/25/22 04:08:21.796
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1855 08/25/22 04:08:21.801
    Aug 25 04:08:21.804: INFO: Found 0 stateful pods, waiting for 1
    Aug 25 04:08:31.810: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/25/22 04:08:31.81
    Aug 25 04:08:31.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 04:08:32.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 04:08:32.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 04:08:32.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 04:08:32.022: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 25 04:08:42.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 04:08:42.029: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 04:08:42.044: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
    Aug 25 04:08:42.045: INFO: ss-0  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  }]
    Aug 25 04:08:42.045: INFO: 
    Aug 25 04:08:42.045: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 25 04:08:43.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995772967s
    Aug 25 04:08:44.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990229767s
    Aug 25 04:08:45.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984363803s
    Aug 25 04:08:46.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97796641s
    Aug 25 04:08:47.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972478628s
    Aug 25 04:08:48.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967015109s
    Aug 25 04:08:49.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961406787s
    Aug 25 04:08:50.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95533361s
    Aug 25 04:08:51.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.465432ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1855 08/25/22 04:08:52.096
    Aug 25 04:08:52.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 04:08:52.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 25 04:08:52.311: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 04:08:52.311: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 04:08:52.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 04:08:52.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 25 04:08:52.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 04:08:52.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 04:08:52.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 25 04:08:52.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 25 04:08:52.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 25 04:08:52.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 25 04:08:52.649: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 25 04:09:02.657: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 04:09:02.657: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 25 04:09:02.657: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/25/22 04:09:02.657
    Aug 25 04:09:02.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 04:09:02.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 04:09:02.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 04:09:02.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 04:09:02.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 04:09:03.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 04:09:03.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 04:09:03.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 04:09:03.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=statefulset-1855 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 25 04:09:03.213: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 25 04:09:03.213: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 25 04:09:03.213: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 25 04:09:03.213: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 04:09:03.217: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 25 04:09:13.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 04:09:13.230: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 04:09:13.230: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 25 04:09:13.244: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
    Aug 25 04:09:13.244: INFO: ss-0  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:21 +0000 UTC  }]
    Aug 25 04:09:13.244: INFO: ss-1  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
    Aug 25 04:09:13.244: INFO: ss-2  bobymcbobs-c849-control-plane-p55pp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
    Aug 25 04:09:13.244: INFO: 
    Aug 25 04:09:13.244: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 25 04:09:14.249: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
    Aug 25 04:09:14.249: INFO: ss-1  bobymcbobs-c849-control-plane-p55pp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
    Aug 25 04:09:14.249: INFO: ss-2  bobymcbobs-c849-control-plane-p55pp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:08:42 +0000 UTC  }]
    Aug 25 04:09:14.249: INFO: 
    Aug 25 04:09:14.249: INFO: StatefulSet ss has not reached scale 0, at 2
    Aug 25 04:09:15.253: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990602941s
    Aug 25 04:09:16.258: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986461692s
    Aug 25 04:09:17.261: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982306701s
    Aug 25 04:09:18.266: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.978377633s
    Aug 25 04:09:19.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974341608s
    Aug 25 04:09:20.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.970128445s
    Aug 25 04:09:21.279: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965578302s
    Aug 25 04:09:22.282: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.301856ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1855 08/25/22 04:09:23.283
    Aug 25 04:09:23.288: INFO: Scaling statefulset ss to 0
    Aug 25 04:09:23.302: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 25 04:09:23.305: INFO: Deleting all statefulset in ns statefulset-1855
    Aug 25 04:09:23.308: INFO: Scaling statefulset ss to 0
    Aug 25 04:09:23.321: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 25 04:09:23.324: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 25 04:09:23.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1855" for this suite. 08/25/22 04:09:23.335
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:09:23.339
Aug 25 04:09:23.339: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename ingress 08/25/22 04:09:23.34
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:23.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:23.352
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/25/22 04:09:23.355
STEP: getting /apis/networking.k8s.io 08/25/22 04:09:23.358
STEP: getting /apis/networking.k8s.iov1 08/25/22 04:09:23.359
STEP: creating 08/25/22 04:09:23.36
STEP: getting 08/25/22 04:09:23.372
STEP: listing 08/25/22 04:09:23.375
STEP: watching 08/25/22 04:09:23.378
Aug 25 04:09:23.378: INFO: starting watch
STEP: cluster-wide listing 08/25/22 04:09:23.379
STEP: cluster-wide watching 08/25/22 04:09:23.383
Aug 25 04:09:23.383: INFO: starting watch
STEP: patching 08/25/22 04:09:23.384
STEP: updating 08/25/22 04:09:23.389
Aug 25 04:09:23.398: INFO: waiting for watch events with expected annotations
Aug 25 04:09:23.398: INFO: saw patched and updated annotations
STEP: patching /status 08/25/22 04:09:23.398
STEP: updating /status 08/25/22 04:09:23.404
STEP: get /status 08/25/22 04:09:23.412
STEP: deleting 08/25/22 04:09:23.416
STEP: deleting a collection 08/25/22 04:09:23.428
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Aug 25 04:09:23.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3108" for this suite. 08/25/22 04:09:23.444
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":265,"skipped":4977,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.109 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:09:23.339
    Aug 25 04:09:23.339: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename ingress 08/25/22 04:09:23.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:23.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:23.352
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/25/22 04:09:23.355
    STEP: getting /apis/networking.k8s.io 08/25/22 04:09:23.358
    STEP: getting /apis/networking.k8s.iov1 08/25/22 04:09:23.359
    STEP: creating 08/25/22 04:09:23.36
    STEP: getting 08/25/22 04:09:23.372
    STEP: listing 08/25/22 04:09:23.375
    STEP: watching 08/25/22 04:09:23.378
    Aug 25 04:09:23.378: INFO: starting watch
    STEP: cluster-wide listing 08/25/22 04:09:23.379
    STEP: cluster-wide watching 08/25/22 04:09:23.383
    Aug 25 04:09:23.383: INFO: starting watch
    STEP: patching 08/25/22 04:09:23.384
    STEP: updating 08/25/22 04:09:23.389
    Aug 25 04:09:23.398: INFO: waiting for watch events with expected annotations
    Aug 25 04:09:23.398: INFO: saw patched and updated annotations
    STEP: patching /status 08/25/22 04:09:23.398
    STEP: updating /status 08/25/22 04:09:23.404
    STEP: get /status 08/25/22 04:09:23.412
    STEP: deleting 08/25/22 04:09:23.416
    STEP: deleting a collection 08/25/22 04:09:23.428
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Aug 25 04:09:23.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-3108" for this suite. 08/25/22 04:09:23.444
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:09:23.449
Aug 25 04:09:23.450: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename podtemplate 08/25/22 04:09:23.45
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:23.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:23.466
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/25/22 04:09:23.47
STEP: Replace a pod template 08/25/22 04:09:23.474
Aug 25 04:09:23.481: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Aug 25 04:09:23.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1569" for this suite. 08/25/22 04:09:23.485
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":266,"skipped":5004,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.040 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:09:23.449
    Aug 25 04:09:23.450: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename podtemplate 08/25/22 04:09:23.45
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:23.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:23.466
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/25/22 04:09:23.47
    STEP: Replace a pod template 08/25/22 04:09:23.474
    Aug 25 04:09:23.481: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Aug 25 04:09:23.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1569" for this suite. 08/25/22 04:09:23.485
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:09:23.49
Aug 25 04:09:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 04:09:23.492
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:23.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:23.508
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 08/25/22 04:09:23.511
STEP: Creating a ResourceQuota 08/25/22 04:09:28.516
STEP: Ensuring resource quota status is calculated 08/25/22 04:09:28.521
STEP: Creating a Service 08/25/22 04:09:30.526
STEP: Creating a NodePort Service 08/25/22 04:09:30.543
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/25/22 04:09:30.56
STEP: Ensuring resource quota status captures service creation 08/25/22 04:09:30.582
STEP: Deleting Services 08/25/22 04:09:32.587
STEP: Ensuring resource quota status released usage 08/25/22 04:09:32.608
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 04:09:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9761" for this suite. 08/25/22 04:09:34.618
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":267,"skipped":5016,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [11.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:09:23.49
    Aug 25 04:09:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 04:09:23.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:23.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:23.508
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 08/25/22 04:09:23.511
    STEP: Creating a ResourceQuota 08/25/22 04:09:28.516
    STEP: Ensuring resource quota status is calculated 08/25/22 04:09:28.521
    STEP: Creating a Service 08/25/22 04:09:30.526
    STEP: Creating a NodePort Service 08/25/22 04:09:30.543
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/25/22 04:09:30.56
    STEP: Ensuring resource quota status captures service creation 08/25/22 04:09:30.582
    STEP: Deleting Services 08/25/22 04:09:32.587
    STEP: Ensuring resource quota status released usage 08/25/22 04:09:32.608
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 04:09:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9761" for this suite. 08/25/22 04:09:34.618
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:09:34.626
Aug 25 04:09:34.626: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 04:09:34.627
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:34.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:34.643
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-bf5141c3-af39-4580-ad93-3250c5f93325 in namespace container-probe-5273 08/25/22 04:09:34.647
Aug 25 04:09:34.652: INFO: Waiting up to 5m0s for pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325" in namespace "container-probe-5273" to be "not pending"
Aug 25 04:09:34.656: INFO: Pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320523ms
Aug 25 04:09:36.660: INFO: Pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325": Phase="Running", Reason="", readiness=true. Elapsed: 2.007890688s
Aug 25 04:09:36.660: INFO: Pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325" satisfied condition "not pending"
Aug 25 04:09:36.660: INFO: Started pod liveness-bf5141c3-af39-4580-ad93-3250c5f93325 in namespace container-probe-5273
STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 04:09:36.66
Aug 25 04:09:36.665: INFO: Initial restart count of pod liveness-bf5141c3-af39-4580-ad93-3250c5f93325 is 0
Aug 25 04:09:56.727: INFO: Restart count of pod container-probe-5273/liveness-bf5141c3-af39-4580-ad93-3250c5f93325 is now 1 (20.061853014s elapsed)
STEP: deleting the pod 08/25/22 04:09:56.727
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 04:09:56.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5273" for this suite. 08/25/22 04:09:56.74
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":268,"skipped":5076,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [22.119 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:09:34.626
    Aug 25 04:09:34.626: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 04:09:34.627
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:34.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:34.643
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-bf5141c3-af39-4580-ad93-3250c5f93325 in namespace container-probe-5273 08/25/22 04:09:34.647
    Aug 25 04:09:34.652: INFO: Waiting up to 5m0s for pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325" in namespace "container-probe-5273" to be "not pending"
    Aug 25 04:09:34.656: INFO: Pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320523ms
    Aug 25 04:09:36.660: INFO: Pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325": Phase="Running", Reason="", readiness=true. Elapsed: 2.007890688s
    Aug 25 04:09:36.660: INFO: Pod "liveness-bf5141c3-af39-4580-ad93-3250c5f93325" satisfied condition "not pending"
    Aug 25 04:09:36.660: INFO: Started pod liveness-bf5141c3-af39-4580-ad93-3250c5f93325 in namespace container-probe-5273
    STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 04:09:36.66
    Aug 25 04:09:36.665: INFO: Initial restart count of pod liveness-bf5141c3-af39-4580-ad93-3250c5f93325 is 0
    Aug 25 04:09:56.727: INFO: Restart count of pod container-probe-5273/liveness-bf5141c3-af39-4580-ad93-3250c5f93325 is now 1 (20.061853014s elapsed)
    STEP: deleting the pod 08/25/22 04:09:56.727
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 04:09:56.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5273" for this suite. 08/25/22 04:09:56.74
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:09:56.746
Aug 25 04:09:56.746: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 04:09:56.747
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:56.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:56.763
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 08/25/22 04:09:56.776
STEP: watching for Pod to be ready 08/25/22 04:09:56.783
Aug 25 04:09:56.785: INFO: observed Pod pod-test in namespace pods-4494 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 25 04:09:56.786: INFO: observed Pod pod-test in namespace pods-4494 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  }]
Aug 25 04:09:56.801: INFO: observed Pod pod-test in namespace pods-4494 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  }]
Aug 25 04:09:58.439: INFO: Found Pod pod-test in namespace pods-4494 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/25/22 04:09:58.444
STEP: getting the Pod and ensuring that it's patched 08/25/22 04:09:58.453
STEP: replacing the Pod's status Ready condition to False 08/25/22 04:09:58.46
STEP: check the Pod again to ensure its Ready conditions are False 08/25/22 04:09:58.469
STEP: deleting the Pod via a Collection with a LabelSelector 08/25/22 04:09:58.469
STEP: watching for the Pod to be deleted 08/25/22 04:09:58.473
Aug 25 04:09:58.474: INFO: observed event type MODIFIED
Aug 25 04:10:00.446: INFO: observed event type MODIFIED
Aug 25 04:10:01.453: INFO: observed event type MODIFIED
Aug 25 04:10:01.459: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 04:10:01.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4494" for this suite. 08/25/22 04:10:01.468
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":269,"skipped":5077,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.726 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:09:56.746
    Aug 25 04:09:56.746: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 04:09:56.747
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:09:56.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:09:56.763
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 08/25/22 04:09:56.776
    STEP: watching for Pod to be ready 08/25/22 04:09:56.783
    Aug 25 04:09:56.785: INFO: observed Pod pod-test in namespace pods-4494 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 25 04:09:56.786: INFO: observed Pod pod-test in namespace pods-4494 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  }]
    Aug 25 04:09:56.801: INFO: observed Pod pod-test in namespace pods-4494 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  }]
    Aug 25 04:09:58.439: INFO: Found Pod pod-test in namespace pods-4494 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-25 04:09:56 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/25/22 04:09:58.444
    STEP: getting the Pod and ensuring that it's patched 08/25/22 04:09:58.453
    STEP: replacing the Pod's status Ready condition to False 08/25/22 04:09:58.46
    STEP: check the Pod again to ensure its Ready conditions are False 08/25/22 04:09:58.469
    STEP: deleting the Pod via a Collection with a LabelSelector 08/25/22 04:09:58.469
    STEP: watching for the Pod to be deleted 08/25/22 04:09:58.473
    Aug 25 04:09:58.474: INFO: observed event type MODIFIED
    Aug 25 04:10:00.446: INFO: observed event type MODIFIED
    Aug 25 04:10:01.453: INFO: observed event type MODIFIED
    Aug 25 04:10:01.459: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 04:10:01.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4494" for this suite. 08/25/22 04:10:01.468
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:10:01.472
Aug 25 04:10:01.473: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 04:10:01.475
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:01.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:01.493
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 08/25/22 04:10:01.498
Aug 25 04:10:01.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 create -f -'
Aug 25 04:10:01.882: INFO: stderr: ""
Aug 25 04:10:01.882: INFO: stdout: "pod/pause created\n"
Aug 25 04:10:01.882: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 25 04:10:01.882: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9892" to be "running and ready"
Aug 25 04:10:01.887: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59142ms
Aug 25 04:10:01.887: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'bobymcbobs-c849-control-plane-p55pp' to be 'Running' but was 'Pending'
Aug 25 04:10:03.891: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009431976s
Aug 25 04:10:03.892: INFO: Pod "pause" satisfied condition "running and ready"
Aug 25 04:10:03.892: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 08/25/22 04:10:03.892
Aug 25 04:10:03.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 label pods pause testing-label=testing-label-value'
Aug 25 04:10:03.978: INFO: stderr: ""
Aug 25 04:10:03.978: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/25/22 04:10:03.978
Aug 25 04:10:03.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get pod pause -L testing-label'
Aug 25 04:10:04.090: INFO: stderr: ""
Aug 25 04:10:04.090: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/25/22 04:10:04.09
Aug 25 04:10:04.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 label pods pause testing-label-'
Aug 25 04:10:04.175: INFO: stderr: ""
Aug 25 04:10:04.175: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/25/22 04:10:04.175
Aug 25 04:10:04.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get pod pause -L testing-label'
Aug 25 04:10:04.253: INFO: stderr: ""
Aug 25 04:10:04.253: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 08/25/22 04:10:04.253
Aug 25 04:10:04.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 delete --grace-period=0 --force -f -'
Aug 25 04:10:04.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 04:10:04.340: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 25 04:10:04.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get rc,svc -l name=pause --no-headers'
Aug 25 04:10:04.419: INFO: stderr: "No resources found in kubectl-9892 namespace.\n"
Aug 25 04:10:04.419: INFO: stdout: ""
Aug 25 04:10:04.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 25 04:10:04.499: INFO: stderr: ""
Aug 25 04:10:04.499: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 04:10:04.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9892" for this suite. 08/25/22 04:10:04.504
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":270,"skipped":5077,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [3.036 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:10:01.472
    Aug 25 04:10:01.473: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 04:10:01.475
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:01.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:01.493
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 08/25/22 04:10:01.498
    Aug 25 04:10:01.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 create -f -'
    Aug 25 04:10:01.882: INFO: stderr: ""
    Aug 25 04:10:01.882: INFO: stdout: "pod/pause created\n"
    Aug 25 04:10:01.882: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 25 04:10:01.882: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9892" to be "running and ready"
    Aug 25 04:10:01.887: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59142ms
    Aug 25 04:10:01.887: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'bobymcbobs-c849-control-plane-p55pp' to be 'Running' but was 'Pending'
    Aug 25 04:10:03.891: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009431976s
    Aug 25 04:10:03.892: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 25 04:10:03.892: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 08/25/22 04:10:03.892
    Aug 25 04:10:03.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 label pods pause testing-label=testing-label-value'
    Aug 25 04:10:03.978: INFO: stderr: ""
    Aug 25 04:10:03.978: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/25/22 04:10:03.978
    Aug 25 04:10:03.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get pod pause -L testing-label'
    Aug 25 04:10:04.090: INFO: stderr: ""
    Aug 25 04:10:04.090: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/25/22 04:10:04.09
    Aug 25 04:10:04.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 label pods pause testing-label-'
    Aug 25 04:10:04.175: INFO: stderr: ""
    Aug 25 04:10:04.175: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/25/22 04:10:04.175
    Aug 25 04:10:04.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get pod pause -L testing-label'
    Aug 25 04:10:04.253: INFO: stderr: ""
    Aug 25 04:10:04.253: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 08/25/22 04:10:04.253
    Aug 25 04:10:04.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 delete --grace-period=0 --force -f -'
    Aug 25 04:10:04.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 04:10:04.340: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 25 04:10:04.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get rc,svc -l name=pause --no-headers'
    Aug 25 04:10:04.419: INFO: stderr: "No resources found in kubectl-9892 namespace.\n"
    Aug 25 04:10:04.419: INFO: stdout: ""
    Aug 25 04:10:04.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-9892 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 25 04:10:04.499: INFO: stderr: ""
    Aug 25 04:10:04.499: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 04:10:04.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9892" for this suite. 08/25/22 04:10:04.504
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:10:04.51
Aug 25 04:10:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 04:10:04.511
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:04.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:04.525
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 08/25/22 04:10:04.529
Aug 25 04:10:04.535: INFO: Waiting up to 5m0s for pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea" in namespace "emptydir-5791" to be "Succeeded or Failed"
Aug 25 04:10:04.538: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440758ms
Aug 25 04:10:06.553: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017411372s
Aug 25 04:10:08.543: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008002044s
STEP: Saw pod success 08/25/22 04:10:08.543
Aug 25 04:10:08.544: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea" satisfied condition "Succeeded or Failed"
Aug 25 04:10:08.547: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea container test-container: <nil>
STEP: delete the pod 08/25/22 04:10:08.562
Aug 25 04:10:08.570: INFO: Waiting for pod pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea to disappear
Aug 25 04:10:08.574: INFO: Pod pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 04:10:08.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5791" for this suite. 08/25/22 04:10:08.577
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":5097,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:10:04.51
    Aug 25 04:10:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 04:10:04.511
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:04.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:04.525
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/25/22 04:10:04.529
    Aug 25 04:10:04.535: INFO: Waiting up to 5m0s for pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea" in namespace "emptydir-5791" to be "Succeeded or Failed"
    Aug 25 04:10:04.538: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440758ms
    Aug 25 04:10:06.553: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017411372s
    Aug 25 04:10:08.543: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008002044s
    STEP: Saw pod success 08/25/22 04:10:08.543
    Aug 25 04:10:08.544: INFO: Pod "pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea" satisfied condition "Succeeded or Failed"
    Aug 25 04:10:08.547: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea container test-container: <nil>
    STEP: delete the pod 08/25/22 04:10:08.562
    Aug 25 04:10:08.570: INFO: Waiting for pod pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea to disappear
    Aug 25 04:10:08.574: INFO: Pod pod-a3d7e0b6-30b4-4fb3-a9a7-282d33dc76ea no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 04:10:08.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5791" for this suite. 08/25/22 04:10:08.577
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:10:08.582
Aug 25 04:10:08.583: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:10:08.584
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:08.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:08.599
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-582ec920-4964-444a-8f2b-f9874eb673d6 08/25/22 04:10:08.605
STEP: Creating a pod to test consume secrets 08/25/22 04:10:08.609
Aug 25 04:10:08.614: INFO: Waiting up to 5m0s for pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7" in namespace "secrets-6600" to be "Succeeded or Failed"
Aug 25 04:10:08.617: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189123ms
Aug 25 04:10:10.622: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.0075652s
Aug 25 04:10:12.621: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006608672s
STEP: Saw pod success 08/25/22 04:10:12.621
Aug 25 04:10:12.621: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7" satisfied condition "Succeeded or Failed"
Aug 25 04:10:12.625: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 04:10:12.63
Aug 25 04:10:12.638: INFO: Waiting for pod pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7 to disappear
Aug 25 04:10:12.641: INFO: Pod pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:10:12.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6600" for this suite. 08/25/22 04:10:12.645
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":272,"skipped":5102,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:10:08.582
    Aug 25 04:10:08.583: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:10:08.584
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:08.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:08.599
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-582ec920-4964-444a-8f2b-f9874eb673d6 08/25/22 04:10:08.605
    STEP: Creating a pod to test consume secrets 08/25/22 04:10:08.609
    Aug 25 04:10:08.614: INFO: Waiting up to 5m0s for pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7" in namespace "secrets-6600" to be "Succeeded or Failed"
    Aug 25 04:10:08.617: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189123ms
    Aug 25 04:10:10.622: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.0075652s
    Aug 25 04:10:12.621: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006608672s
    STEP: Saw pod success 08/25/22 04:10:12.621
    Aug 25 04:10:12.621: INFO: Pod "pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7" satisfied condition "Succeeded or Failed"
    Aug 25 04:10:12.625: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:10:12.63
    Aug 25 04:10:12.638: INFO: Waiting for pod pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7 to disappear
    Aug 25 04:10:12.641: INFO: Pod pod-secrets-dd1976c7-4a3a-45d8-9a92-87244e15f9b7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:10:12.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6600" for this suite. 08/25/22 04:10:12.645
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:10:12.651
Aug 25 04:10:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 04:10:12.652
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:12.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:12.669
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 08/25/22 04:10:12.673
Aug 25 04:10:12.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 create -f -'
Aug 25 04:10:12.914: INFO: stderr: ""
Aug 25 04:10:12.914: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 04:10:12.914
Aug 25 04:10:12.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 04:10:13.002: INFO: stderr: ""
Aug 25 04:10:13.002: INFO: stdout: "update-demo-nautilus-fgk9z update-demo-nautilus-tpbpx "
Aug 25 04:10:13.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-fgk9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:13.080: INFO: stderr: ""
Aug 25 04:10:13.080: INFO: stdout: ""
Aug 25 04:10:13.080: INFO: update-demo-nautilus-fgk9z is created but not running
Aug 25 04:10:18.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 04:10:18.158: INFO: stderr: ""
Aug 25 04:10:18.158: INFO: stdout: "update-demo-nautilus-fgk9z update-demo-nautilus-tpbpx "
Aug 25 04:10:18.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-fgk9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:18.239: INFO: stderr: ""
Aug 25 04:10:18.239: INFO: stdout: "true"
Aug 25 04:10:18.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-fgk9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 04:10:18.316: INFO: stderr: ""
Aug 25 04:10:18.316: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 04:10:18.316: INFO: validating pod update-demo-nautilus-fgk9z
Aug 25 04:10:18.321: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 04:10:18.321: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 04:10:18.321: INFO: update-demo-nautilus-fgk9z is verified up and running
Aug 25 04:10:18.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:18.402: INFO: stderr: ""
Aug 25 04:10:18.402: INFO: stdout: "true"
Aug 25 04:10:18.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 04:10:18.485: INFO: stderr: ""
Aug 25 04:10:18.485: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 04:10:18.485: INFO: validating pod update-demo-nautilus-tpbpx
Aug 25 04:10:18.490: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 04:10:18.490: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 04:10:18.490: INFO: update-demo-nautilus-tpbpx is verified up and running
STEP: scaling down the replication controller 08/25/22 04:10:18.49
Aug 25 04:10:18.494: INFO: scanned /root for discovery docs: <nil>
Aug 25 04:10:18.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 25 04:10:19.586: INFO: stderr: ""
Aug 25 04:10:19.586: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 04:10:19.586
Aug 25 04:10:19.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 04:10:19.667: INFO: stderr: ""
Aug 25 04:10:19.667: INFO: stdout: "update-demo-nautilus-tpbpx "
Aug 25 04:10:19.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:19.743: INFO: stderr: ""
Aug 25 04:10:19.743: INFO: stdout: "true"
Aug 25 04:10:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 04:10:19.819: INFO: stderr: ""
Aug 25 04:10:19.819: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 04:10:19.819: INFO: validating pod update-demo-nautilus-tpbpx
Aug 25 04:10:19.822: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 04:10:19.822: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 04:10:19.823: INFO: update-demo-nautilus-tpbpx is verified up and running
STEP: scaling up the replication controller 08/25/22 04:10:19.823
Aug 25 04:10:19.825: INFO: scanned /root for discovery docs: <nil>
Aug 25 04:10:19.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 25 04:10:20.916: INFO: stderr: ""
Aug 25 04:10:20.916: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 04:10:20.916
Aug 25 04:10:20.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 04:10:20.992: INFO: stderr: ""
Aug 25 04:10:20.992: INFO: stdout: "update-demo-nautilus-9zvfz update-demo-nautilus-tpbpx "
Aug 25 04:10:20.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-9zvfz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:21.068: INFO: stderr: ""
Aug 25 04:10:21.069: INFO: stdout: ""
Aug 25 04:10:21.069: INFO: update-demo-nautilus-9zvfz is created but not running
Aug 25 04:10:26.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 25 04:10:26.161: INFO: stderr: ""
Aug 25 04:10:26.161: INFO: stdout: "update-demo-nautilus-9zvfz update-demo-nautilus-tpbpx "
Aug 25 04:10:26.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-9zvfz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:26.228: INFO: stderr: ""
Aug 25 04:10:26.228: INFO: stdout: "true"
Aug 25 04:10:26.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-9zvfz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 04:10:26.296: INFO: stderr: ""
Aug 25 04:10:26.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 04:10:26.296: INFO: validating pod update-demo-nautilus-9zvfz
Aug 25 04:10:26.301: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 04:10:26.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 04:10:26.301: INFO: update-demo-nautilus-9zvfz is verified up and running
Aug 25 04:10:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 25 04:10:26.389: INFO: stderr: ""
Aug 25 04:10:26.389: INFO: stdout: "true"
Aug 25 04:10:26.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 25 04:10:26.465: INFO: stderr: ""
Aug 25 04:10:26.465: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 25 04:10:26.465: INFO: validating pod update-demo-nautilus-tpbpx
Aug 25 04:10:26.469: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 25 04:10:26.469: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 25 04:10:26.469: INFO: update-demo-nautilus-tpbpx is verified up and running
STEP: using delete to clean up resources 08/25/22 04:10:26.469
Aug 25 04:10:26.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 delete --grace-period=0 --force -f -'
Aug 25 04:10:26.545: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 25 04:10:26.546: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 25 04:10:26.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get rc,svc -l name=update-demo --no-headers'
Aug 25 04:10:26.629: INFO: stderr: "No resources found in kubectl-6433 namespace.\n"
Aug 25 04:10:26.629: INFO: stdout: ""
Aug 25 04:10:26.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 25 04:10:26.720: INFO: stderr: ""
Aug 25 04:10:26.720: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 04:10:26.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6433" for this suite. 08/25/22 04:10:26.724
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":273,"skipped":5114,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [14.078 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:10:12.651
    Aug 25 04:10:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 04:10:12.652
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:12.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:12.669
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 08/25/22 04:10:12.673
    Aug 25 04:10:12.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 create -f -'
    Aug 25 04:10:12.914: INFO: stderr: ""
    Aug 25 04:10:12.914: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 04:10:12.914
    Aug 25 04:10:12.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 04:10:13.002: INFO: stderr: ""
    Aug 25 04:10:13.002: INFO: stdout: "update-demo-nautilus-fgk9z update-demo-nautilus-tpbpx "
    Aug 25 04:10:13.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-fgk9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:13.080: INFO: stderr: ""
    Aug 25 04:10:13.080: INFO: stdout: ""
    Aug 25 04:10:13.080: INFO: update-demo-nautilus-fgk9z is created but not running
    Aug 25 04:10:18.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 04:10:18.158: INFO: stderr: ""
    Aug 25 04:10:18.158: INFO: stdout: "update-demo-nautilus-fgk9z update-demo-nautilus-tpbpx "
    Aug 25 04:10:18.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-fgk9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:18.239: INFO: stderr: ""
    Aug 25 04:10:18.239: INFO: stdout: "true"
    Aug 25 04:10:18.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-fgk9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 04:10:18.316: INFO: stderr: ""
    Aug 25 04:10:18.316: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 04:10:18.316: INFO: validating pod update-demo-nautilus-fgk9z
    Aug 25 04:10:18.321: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 04:10:18.321: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 04:10:18.321: INFO: update-demo-nautilus-fgk9z is verified up and running
    Aug 25 04:10:18.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:18.402: INFO: stderr: ""
    Aug 25 04:10:18.402: INFO: stdout: "true"
    Aug 25 04:10:18.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 04:10:18.485: INFO: stderr: ""
    Aug 25 04:10:18.485: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 04:10:18.485: INFO: validating pod update-demo-nautilus-tpbpx
    Aug 25 04:10:18.490: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 04:10:18.490: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 04:10:18.490: INFO: update-demo-nautilus-tpbpx is verified up and running
    STEP: scaling down the replication controller 08/25/22 04:10:18.49
    Aug 25 04:10:18.494: INFO: scanned /root for discovery docs: <nil>
    Aug 25 04:10:18.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 25 04:10:19.586: INFO: stderr: ""
    Aug 25 04:10:19.586: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 04:10:19.586
    Aug 25 04:10:19.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 04:10:19.667: INFO: stderr: ""
    Aug 25 04:10:19.667: INFO: stdout: "update-demo-nautilus-tpbpx "
    Aug 25 04:10:19.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:19.743: INFO: stderr: ""
    Aug 25 04:10:19.743: INFO: stdout: "true"
    Aug 25 04:10:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 04:10:19.819: INFO: stderr: ""
    Aug 25 04:10:19.819: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 04:10:19.819: INFO: validating pod update-demo-nautilus-tpbpx
    Aug 25 04:10:19.822: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 04:10:19.822: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 04:10:19.823: INFO: update-demo-nautilus-tpbpx is verified up and running
    STEP: scaling up the replication controller 08/25/22 04:10:19.823
    Aug 25 04:10:19.825: INFO: scanned /root for discovery docs: <nil>
    Aug 25 04:10:19.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 25 04:10:20.916: INFO: stderr: ""
    Aug 25 04:10:20.916: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/25/22 04:10:20.916
    Aug 25 04:10:20.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 04:10:20.992: INFO: stderr: ""
    Aug 25 04:10:20.992: INFO: stdout: "update-demo-nautilus-9zvfz update-demo-nautilus-tpbpx "
    Aug 25 04:10:20.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-9zvfz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:21.068: INFO: stderr: ""
    Aug 25 04:10:21.069: INFO: stdout: ""
    Aug 25 04:10:21.069: INFO: update-demo-nautilus-9zvfz is created but not running
    Aug 25 04:10:26.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 25 04:10:26.161: INFO: stderr: ""
    Aug 25 04:10:26.161: INFO: stdout: "update-demo-nautilus-9zvfz update-demo-nautilus-tpbpx "
    Aug 25 04:10:26.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-9zvfz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:26.228: INFO: stderr: ""
    Aug 25 04:10:26.228: INFO: stdout: "true"
    Aug 25 04:10:26.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-9zvfz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 04:10:26.296: INFO: stderr: ""
    Aug 25 04:10:26.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 04:10:26.296: INFO: validating pod update-demo-nautilus-9zvfz
    Aug 25 04:10:26.301: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 04:10:26.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 04:10:26.301: INFO: update-demo-nautilus-9zvfz is verified up and running
    Aug 25 04:10:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 25 04:10:26.389: INFO: stderr: ""
    Aug 25 04:10:26.389: INFO: stdout: "true"
    Aug 25 04:10:26.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods update-demo-nautilus-tpbpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 25 04:10:26.465: INFO: stderr: ""
    Aug 25 04:10:26.465: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 25 04:10:26.465: INFO: validating pod update-demo-nautilus-tpbpx
    Aug 25 04:10:26.469: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 25 04:10:26.469: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 25 04:10:26.469: INFO: update-demo-nautilus-tpbpx is verified up and running
    STEP: using delete to clean up resources 08/25/22 04:10:26.469
    Aug 25 04:10:26.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 delete --grace-period=0 --force -f -'
    Aug 25 04:10:26.545: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 25 04:10:26.546: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 25 04:10:26.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get rc,svc -l name=update-demo --no-headers'
    Aug 25 04:10:26.629: INFO: stderr: "No resources found in kubectl-6433 namespace.\n"
    Aug 25 04:10:26.629: INFO: stdout: ""
    Aug 25 04:10:26.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-6433 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 25 04:10:26.720: INFO: stderr: ""
    Aug 25 04:10:26.720: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 04:10:26.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6433" for this suite. 08/25/22 04:10:26.724
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:10:26.73
Aug 25 04:10:26.730: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename containers 08/25/22 04:10:26.731
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:26.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:26.746
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Aug 25 04:10:26.756: INFO: Waiting up to 5m0s for pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09" in namespace "containers-4141" to be "running"
Aug 25 04:10:26.759: INFO: Pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081119ms
Aug 25 04:10:28.764: INFO: Pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09": Phase="Running", Reason="", readiness=true. Elapsed: 2.008253503s
Aug 25 04:10:28.764: INFO: Pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 25 04:10:28.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4141" for this suite. 08/25/22 04:10:28.774
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":274,"skipped":5127,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.049 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:10:26.73
    Aug 25 04:10:26.730: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename containers 08/25/22 04:10:26.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:26.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:26.746
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Aug 25 04:10:26.756: INFO: Waiting up to 5m0s for pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09" in namespace "containers-4141" to be "running"
    Aug 25 04:10:26.759: INFO: Pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081119ms
    Aug 25 04:10:28.764: INFO: Pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09": Phase="Running", Reason="", readiness=true. Elapsed: 2.008253503s
    Aug 25 04:10:28.764: INFO: Pod "client-containers-a13872e5-5a73-4288-94ea-74f641e6bd09" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 25 04:10:28.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4141" for this suite. 08/25/22 04:10:28.774
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:10:28.784
Aug 25 04:10:28.784: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 04:10:28.785
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:28.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:28.799
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/25/22 04:10:28.804
Aug 25 04:10:28.805: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/25/22 04:10:44.273
Aug 25 04:10:44.274: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 04:10:48.382: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:11:02.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4426" for this suite. 08/25/22 04:11:02.638
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":275,"skipped":5204,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [33.858 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:10:28.784
    Aug 25 04:10:28.784: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 04:10:28.785
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:10:28.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:10:28.799
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/25/22 04:10:28.804
    Aug 25 04:10:28.805: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/25/22 04:10:44.273
    Aug 25 04:10:44.274: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 04:10:48.382: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:11:02.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4426" for this suite. 08/25/22 04:11:02.638
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:02.644
Aug 25 04:11:02.644: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-webhook 08/25/22 04:11:02.645
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:02.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:02.661
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/25/22 04:11:02.667
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/25/22 04:11:03.382
STEP: Deploying the custom resource conversion webhook pod 08/25/22 04:11:03.39
STEP: Wait for the deployment to be ready 08/25/22 04:11:03.4
Aug 25 04:11:03.407: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 04:11:05.42
STEP: Verifying the service has paired with the endpoint 08/25/22 04:11:05.429
Aug 25 04:11:06.430: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 25 04:11:06.434: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Creating a v1 custom resource 08/25/22 04:11:09.038
STEP: v2 custom resource should be converted 08/25/22 04:11:09.044
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:11:09.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1731" for this suite. 08/25/22 04:11:09.567
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":276,"skipped":5220,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.955 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:02.644
    Aug 25 04:11:02.644: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-webhook 08/25/22 04:11:02.645
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:02.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:02.661
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/25/22 04:11:02.667
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/25/22 04:11:03.382
    STEP: Deploying the custom resource conversion webhook pod 08/25/22 04:11:03.39
    STEP: Wait for the deployment to be ready 08/25/22 04:11:03.4
    Aug 25 04:11:03.407: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 04:11:05.42
    STEP: Verifying the service has paired with the endpoint 08/25/22 04:11:05.429
    Aug 25 04:11:06.430: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 25 04:11:06.434: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Creating a v1 custom resource 08/25/22 04:11:09.038
    STEP: v2 custom resource should be converted 08/25/22 04:11:09.044
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:11:09.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1731" for this suite. 08/25/22 04:11:09.567
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:09.6
Aug 25 04:11:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 04:11:09.602
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:09.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:09.614
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 08/25/22 04:11:09.617
Aug 25 04:11:09.622: INFO: Waiting up to 5m0s for pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e" in namespace "emptydir-5454" to be "Succeeded or Failed"
Aug 25 04:11:09.625: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19592ms
Aug 25 04:11:11.631: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484055s
Aug 25 04:11:13.630: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008190796s
STEP: Saw pod success 08/25/22 04:11:13.63
Aug 25 04:11:13.630: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e" satisfied condition "Succeeded or Failed"
Aug 25 04:11:13.635: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-6f54a725-b34a-4f25-9bb6-7910db41373e container test-container: <nil>
STEP: delete the pod 08/25/22 04:11:13.64
Aug 25 04:11:13.648: INFO: Waiting for pod pod-6f54a725-b34a-4f25-9bb6-7910db41373e to disappear
Aug 25 04:11:13.653: INFO: Pod pod-6f54a725-b34a-4f25-9bb6-7910db41373e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 04:11:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5454" for this suite. 08/25/22 04:11:13.657
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":277,"skipped":5238,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.060 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:09.6
    Aug 25 04:11:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 04:11:09.602
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:09.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:09.614
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/25/22 04:11:09.617
    Aug 25 04:11:09.622: INFO: Waiting up to 5m0s for pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e" in namespace "emptydir-5454" to be "Succeeded or Failed"
    Aug 25 04:11:09.625: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19592ms
    Aug 25 04:11:11.631: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484055s
    Aug 25 04:11:13.630: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008190796s
    STEP: Saw pod success 08/25/22 04:11:13.63
    Aug 25 04:11:13.630: INFO: Pod "pod-6f54a725-b34a-4f25-9bb6-7910db41373e" satisfied condition "Succeeded or Failed"
    Aug 25 04:11:13.635: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-6f54a725-b34a-4f25-9bb6-7910db41373e container test-container: <nil>
    STEP: delete the pod 08/25/22 04:11:13.64
    Aug 25 04:11:13.648: INFO: Waiting for pod pod-6f54a725-b34a-4f25-9bb6-7910db41373e to disappear
    Aug 25 04:11:13.653: INFO: Pod pod-6f54a725-b34a-4f25-9bb6-7910db41373e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 04:11:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5454" for this suite. 08/25/22 04:11:13.657
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:13.661
Aug 25 04:11:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replication-controller 08/25/22 04:11:13.662
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:13.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:13.673
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 08/25/22 04:11:13.677
STEP: When the matched label of one of its pods change 08/25/22 04:11:13.681
Aug 25 04:11:13.683: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 25 04:11:18.690: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/25/22 04:11:18.701
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 25 04:11:19.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6498" for this suite. 08/25/22 04:11:19.716
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":278,"skipped":5249,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.060 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:13.661
    Aug 25 04:11:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replication-controller 08/25/22 04:11:13.662
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:13.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:13.673
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 08/25/22 04:11:13.677
    STEP: When the matched label of one of its pods change 08/25/22 04:11:13.681
    Aug 25 04:11:13.683: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 25 04:11:18.690: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/25/22 04:11:18.701
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 25 04:11:19.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6498" for this suite. 08/25/22 04:11:19.716
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:19.723
Aug 25 04:11:19.723: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replication-controller 08/25/22 04:11:19.724
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:19.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:19.741
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 08/25/22 04:11:19.749
STEP: waiting for RC to be added 08/25/22 04:11:19.753
STEP: waiting for available Replicas 08/25/22 04:11:19.754
STEP: patching ReplicationController 08/25/22 04:11:20.854
STEP: waiting for RC to be modified 08/25/22 04:11:20.867
STEP: patching ReplicationController status 08/25/22 04:11:20.868
STEP: waiting for RC to be modified 08/25/22 04:11:20.874
STEP: waiting for available Replicas 08/25/22 04:11:20.874
STEP: fetching ReplicationController status 08/25/22 04:11:20.878
STEP: patching ReplicationController scale 08/25/22 04:11:20.881
STEP: waiting for RC to be modified 08/25/22 04:11:20.886
STEP: waiting for ReplicationController's scale to be the max amount 08/25/22 04:11:20.887
STEP: fetching ReplicationController; ensuring that it's patched 08/25/22 04:11:21.862
STEP: updating ReplicationController status 08/25/22 04:11:21.866
STEP: waiting for RC to be modified 08/25/22 04:11:21.871
STEP: listing all ReplicationControllers 08/25/22 04:11:21.871
STEP: checking that ReplicationController has expected values 08/25/22 04:11:21.875
STEP: deleting ReplicationControllers by collection 08/25/22 04:11:21.875
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/25/22 04:11:21.881
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 25 04:11:21.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8726" for this suite. 08/25/22 04:11:21.924
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":279,"skipped":5264,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.206 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:19.723
    Aug 25 04:11:19.723: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replication-controller 08/25/22 04:11:19.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:19.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:19.741
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 08/25/22 04:11:19.749
    STEP: waiting for RC to be added 08/25/22 04:11:19.753
    STEP: waiting for available Replicas 08/25/22 04:11:19.754
    STEP: patching ReplicationController 08/25/22 04:11:20.854
    STEP: waiting for RC to be modified 08/25/22 04:11:20.867
    STEP: patching ReplicationController status 08/25/22 04:11:20.868
    STEP: waiting for RC to be modified 08/25/22 04:11:20.874
    STEP: waiting for available Replicas 08/25/22 04:11:20.874
    STEP: fetching ReplicationController status 08/25/22 04:11:20.878
    STEP: patching ReplicationController scale 08/25/22 04:11:20.881
    STEP: waiting for RC to be modified 08/25/22 04:11:20.886
    STEP: waiting for ReplicationController's scale to be the max amount 08/25/22 04:11:20.887
    STEP: fetching ReplicationController; ensuring that it's patched 08/25/22 04:11:21.862
    STEP: updating ReplicationController status 08/25/22 04:11:21.866
    STEP: waiting for RC to be modified 08/25/22 04:11:21.871
    STEP: listing all ReplicationControllers 08/25/22 04:11:21.871
    STEP: checking that ReplicationController has expected values 08/25/22 04:11:21.875
    STEP: deleting ReplicationControllers by collection 08/25/22 04:11:21.875
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/25/22 04:11:21.881
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 25 04:11:21.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8726" for this suite. 08/25/22 04:11:21.924
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:21.93
Aug 25 04:11:21.930: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename namespaces 08/25/22 04:11:21.932
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:21.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:21.948
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 08/25/22 04:11:21.951
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:21.96
STEP: Creating a pod in the namespace 08/25/22 04:11:21.965
STEP: Waiting for the pod to have running status 08/25/22 04:11:21.971
Aug 25 04:11:21.971: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3943" to be "running"
Aug 25 04:11:21.974: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578953ms
Aug 25 04:11:23.978: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006419783s
Aug 25 04:11:23.978: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/25/22 04:11:23.978
STEP: Waiting for the namespace to be removed. 08/25/22 04:11:23.982
STEP: Recreating the namespace 08/25/22 04:11:34.987
STEP: Verifying there are no pods in the namespace 08/25/22 04:11:35
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:11:35.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5157" for this suite. 08/25/22 04:11:35.007
STEP: Destroying namespace "nsdeletetest-3943" for this suite. 08/25/22 04:11:35.011
Aug 25 04:11:35.014: INFO: Namespace nsdeletetest-3943 was already deleted
STEP: Destroying namespace "nsdeletetest-9551" for this suite. 08/25/22 04:11:35.014
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":280,"skipped":5277,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [13.087 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:21.93
    Aug 25 04:11:21.930: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename namespaces 08/25/22 04:11:21.932
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:21.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:21.948
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 08/25/22 04:11:21.951
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:21.96
    STEP: Creating a pod in the namespace 08/25/22 04:11:21.965
    STEP: Waiting for the pod to have running status 08/25/22 04:11:21.971
    Aug 25 04:11:21.971: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3943" to be "running"
    Aug 25 04:11:21.974: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578953ms
    Aug 25 04:11:23.978: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006419783s
    Aug 25 04:11:23.978: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/25/22 04:11:23.978
    STEP: Waiting for the namespace to be removed. 08/25/22 04:11:23.982
    STEP: Recreating the namespace 08/25/22 04:11:34.987
    STEP: Verifying there are no pods in the namespace 08/25/22 04:11:35
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:11:35.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5157" for this suite. 08/25/22 04:11:35.007
    STEP: Destroying namespace "nsdeletetest-3943" for this suite. 08/25/22 04:11:35.011
    Aug 25 04:11:35.014: INFO: Namespace nsdeletetest-3943 was already deleted
    STEP: Destroying namespace "nsdeletetest-9551" for this suite. 08/25/22 04:11:35.014
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:35.018
Aug 25 04:11:35.018: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 04:11:35.02
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:35.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:35.032
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 08/25/22 04:11:35.045
STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:11:35.049
Aug 25 04:11:35.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:11:35.054: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:11:36.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:11:36.063: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/25/22 04:11:36.067
Aug 25 04:11:36.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:11:36.084: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:11:37.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:11:37.094: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:11:38.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:11:38.094: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:11:39.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:11:39.095: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:11:40.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:11:40.093: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:11:40.095
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7194, will wait for the garbage collector to delete the pods 08/25/22 04:11:40.095
Aug 25 04:11:40.153: INFO: Deleting DaemonSet.extensions daemon-set took: 3.87216ms
Aug 25 04:11:40.253: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.477739ms
Aug 25 04:11:43.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:11:43.058: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 25 04:11:43.062: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49708"},"items":null}

Aug 25 04:11:43.065: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49708"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:11:43.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7194" for this suite. 08/25/22 04:11:43.078
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":281,"skipped":5277,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [8.064 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:35.018
    Aug 25 04:11:35.018: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 04:11:35.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:35.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:35.032
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 08/25/22 04:11:35.045
    STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:11:35.049
    Aug 25 04:11:35.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:11:35.054: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:11:36.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:11:36.063: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/25/22 04:11:36.067
    Aug 25 04:11:36.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:11:36.084: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:11:37.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:11:37.094: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:11:38.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:11:38.094: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:11:39.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:11:39.095: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:11:40.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:11:40.093: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:11:40.095
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7194, will wait for the garbage collector to delete the pods 08/25/22 04:11:40.095
    Aug 25 04:11:40.153: INFO: Deleting DaemonSet.extensions daemon-set took: 3.87216ms
    Aug 25 04:11:40.253: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.477739ms
    Aug 25 04:11:43.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:11:43.058: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 25 04:11:43.062: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49708"},"items":null}

    Aug 25 04:11:43.065: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49708"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:11:43.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7194" for this suite. 08/25/22 04:11:43.078
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:11:43.083
Aug 25 04:11:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename cronjob 08/25/22 04:11:43.084
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:43.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:43.1
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/25/22 04:11:43.104
STEP: Ensuring more than one job is running at a time 08/25/22 04:11:43.11
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/25/22 04:13:01.115
STEP: Removing cronjob 08/25/22 04:13:01.12
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 25 04:13:01.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8713" for this suite. 08/25/22 04:13:01.13
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":282,"skipped":5277,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [78.051 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:11:43.083
    Aug 25 04:11:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename cronjob 08/25/22 04:11:43.084
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:11:43.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:11:43.1
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/25/22 04:11:43.104
    STEP: Ensuring more than one job is running at a time 08/25/22 04:11:43.11
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/25/22 04:13:01.115
    STEP: Removing cronjob 08/25/22 04:13:01.12
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 25 04:13:01.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8713" for this suite. 08/25/22 04:13:01.13
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:01.135
Aug 25 04:13:01.135: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename tables 08/25/22 04:13:01.137
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:01.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:01.152
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Aug 25 04:13:01.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6721" for this suite. 08/25/22 04:13:01.161
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":283,"skipped":5294,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.029 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:01.135
    Aug 25 04:13:01.135: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename tables 08/25/22 04:13:01.137
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:01.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:01.152
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Aug 25 04:13:01.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-6721" for this suite. 08/25/22 04:13:01.161
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:01.168
Aug 25 04:13:01.168: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename gc 08/25/22 04:13:01.169
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:01.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:01.182
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/25/22 04:13:01.185
STEP: delete the rc 08/25/22 04:13:06.198
STEP: wait for all pods to be garbage collected 08/25/22 04:13:06.203
STEP: Gathering metrics 08/25/22 04:13:11.211
Aug 25 04:13:11.240: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
Aug 25 04:13:11.244: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 3.982025ms
Aug 25 04:13:11.244: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
Aug 25 04:13:11.244: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
Aug 25 04:13:11.329: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 25 04:13:11.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6600" for this suite. 08/25/22 04:13:11.334
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":284,"skipped":5308,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.171 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:01.168
    Aug 25 04:13:01.168: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename gc 08/25/22 04:13:01.169
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:01.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:01.182
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/25/22 04:13:01.185
    STEP: delete the rc 08/25/22 04:13:06.198
    STEP: wait for all pods to be garbage collected 08/25/22 04:13:06.203
    STEP: Gathering metrics 08/25/22 04:13:11.211
    Aug 25 04:13:11.240: INFO: Waiting up to 5m0s for pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" in namespace "kube-system" to be "running and ready"
    Aug 25 04:13:11.244: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp": Phase="Running", Reason="", readiness=true. Elapsed: 3.982025ms
    Aug 25 04:13:11.244: INFO: The phase of Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp is Running (Ready = true)
    Aug 25 04:13:11.244: INFO: Pod "kube-controller-manager-bobymcbobs-c849-control-plane-p55pp" satisfied condition "running and ready"
    Aug 25 04:13:11.329: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 25 04:13:11.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6600" for this suite. 08/25/22 04:13:11.334
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:11.342
Aug 25 04:13:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:13:11.344
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:11.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:11.357
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 08/25/22 04:13:11.362
Aug 25 04:13:11.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60" in namespace "projected-1153" to be "Succeeded or Failed"
Aug 25 04:13:11.371: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758679ms
Aug 25 04:13:13.375: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00715393s
Aug 25 04:13:15.377: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008651158s
STEP: Saw pod success 08/25/22 04:13:15.377
Aug 25 04:13:15.377: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60" satisfied condition "Succeeded or Failed"
Aug 25 04:13:15.381: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60 container client-container: <nil>
STEP: delete the pod 08/25/22 04:13:15.386
Aug 25 04:13:15.392: INFO: Waiting for pod downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60 to disappear
Aug 25 04:13:15.395: INFO: Pod downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 04:13:15.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1153" for this suite. 08/25/22 04:13:15.398
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":285,"skipped":5348,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:11.342
    Aug 25 04:13:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:13:11.344
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:11.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:11.357
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 08/25/22 04:13:11.362
    Aug 25 04:13:11.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60" in namespace "projected-1153" to be "Succeeded or Failed"
    Aug 25 04:13:11.371: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758679ms
    Aug 25 04:13:13.375: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00715393s
    Aug 25 04:13:15.377: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008651158s
    STEP: Saw pod success 08/25/22 04:13:15.377
    Aug 25 04:13:15.377: INFO: Pod "downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60" satisfied condition "Succeeded or Failed"
    Aug 25 04:13:15.381: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60 container client-container: <nil>
    STEP: delete the pod 08/25/22 04:13:15.386
    Aug 25 04:13:15.392: INFO: Waiting for pod downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60 to disappear
    Aug 25 04:13:15.395: INFO: Pod downwardapi-volume-5d76c0c2-206a-4cc6-9f36-eb69a9c77c60 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 04:13:15.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1153" for this suite. 08/25/22 04:13:15.398
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:15.404
Aug 25 04:13:15.404: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename runtimeclass 08/25/22 04:13:15.405
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:15.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:15.419
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 25 04:13:15.432: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9241 to be scheduled
Aug 25 04:13:15.435: INFO: 1 pods are not scheduled: [runtimeclass-9241/test-runtimeclass-runtimeclass-9241-preconfigured-handler-zxps9(1c055cdc-7e14-4036-a96b-5027e40bb689)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 25 04:13:17.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9241" for this suite. 08/25/22 04:13:17.451
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":286,"skipped":5365,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [2.052 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:15.404
    Aug 25 04:13:15.404: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename runtimeclass 08/25/22 04:13:15.405
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:15.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:15.419
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 25 04:13:15.432: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9241 to be scheduled
    Aug 25 04:13:15.435: INFO: 1 pods are not scheduled: [runtimeclass-9241/test-runtimeclass-runtimeclass-9241-preconfigured-handler-zxps9(1c055cdc-7e14-4036-a96b-5027e40bb689)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 25 04:13:17.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9241" for this suite. 08/25/22 04:13:17.451
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:17.457
Aug 25 04:13:17.457: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 04:13:17.459
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:17.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:17.474
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 04:13:17.49
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 04:13:17.763
STEP: Deploying the webhook pod 08/25/22 04:13:17.772
STEP: Wait for the deployment to be ready 08/25/22 04:13:17.782
Aug 25 04:13:17.790: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 04:13:19.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 04:13:21.81
STEP: Verifying the service has paired with the endpoint 08/25/22 04:13:21.82
Aug 25 04:13:22.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/25/22 04:13:22.825
STEP: create a pod that should be updated by the webhook 08/25/22 04:13:22.843
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:13:22.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7684" for this suite. 08/25/22 04:13:22.869
STEP: Destroying namespace "webhook-7684-markers" for this suite. 08/25/22 04:13:22.873
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":287,"skipped":5381,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [5.439 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:17.457
    Aug 25 04:13:17.457: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 04:13:17.459
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:17.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:17.474
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 04:13:17.49
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 04:13:17.763
    STEP: Deploying the webhook pod 08/25/22 04:13:17.772
    STEP: Wait for the deployment to be ready 08/25/22 04:13:17.782
    Aug 25 04:13:17.790: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 04:13:19.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 13, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 04:13:21.81
    STEP: Verifying the service has paired with the endpoint 08/25/22 04:13:21.82
    Aug 25 04:13:22.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/25/22 04:13:22.825
    STEP: create a pod that should be updated by the webhook 08/25/22 04:13:22.843
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:13:22.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7684" for this suite. 08/25/22 04:13:22.869
    STEP: Destroying namespace "webhook-7684-markers" for this suite. 08/25/22 04:13:22.873
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:22.897
Aug 25 04:13:22.897: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 04:13:22.898
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:22.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:22.919
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-2958 08/25/22 04:13:22.922
STEP: creating service affinity-clusterip-transition in namespace services-2958 08/25/22 04:13:22.922
STEP: creating replication controller affinity-clusterip-transition in namespace services-2958 08/25/22 04:13:22.931
I0825 04:13:22.935368      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2958, replica count: 3
I0825 04:13:25.986983      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0825 04:13:28.987902      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 04:13:28.994: INFO: Creating new exec pod
Aug 25 04:13:28.999: INFO: Waiting up to 5m0s for pod "execpod-affinityzq9dc" in namespace "services-2958" to be "running"
Aug 25 04:13:29.002: INFO: Pod "execpod-affinityzq9dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.348399ms
Aug 25 04:13:31.007: INFO: Pod "execpod-affinityzq9dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008382899s
Aug 25 04:13:31.007: INFO: Pod "execpod-affinityzq9dc" satisfied condition "running"
Aug 25 04:13:32.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 25 04:13:32.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 25 04:13:32.167: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:13:32.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.214.208 80'
Aug 25 04:13:32.348: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.214.208 80\nConnection to 10.107.214.208 80 port [tcp/http] succeeded!\n"
Aug 25 04:13:32.348: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:13:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.214.208:80/ ; done'
Aug 25 04:13:32.594: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n"
Aug 25 04:13:32.594: INFO: stdout: "\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5"
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
Aug 25 04:13:32.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.214.208:80/ ; done'
Aug 25 04:13:32.885: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n"
Aug 25 04:13:32.885: INFO: stdout: "\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt"
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
Aug 25 04:13:32.885: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2958, will wait for the garbage collector to delete the pods 08/25/22 04:13:32.894
Aug 25 04:13:32.955: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.570393ms
Aug 25 04:13:33.055: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.59872ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 04:13:35.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2958" for this suite. 08/25/22 04:13:35.772
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":288,"skipped":5395,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [12.879 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:22.897
    Aug 25 04:13:22.897: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 04:13:22.898
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:22.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:22.919
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-2958 08/25/22 04:13:22.922
    STEP: creating service affinity-clusterip-transition in namespace services-2958 08/25/22 04:13:22.922
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2958 08/25/22 04:13:22.931
    I0825 04:13:22.935368      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2958, replica count: 3
    I0825 04:13:25.986983      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0825 04:13:28.987902      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 04:13:28.994: INFO: Creating new exec pod
    Aug 25 04:13:28.999: INFO: Waiting up to 5m0s for pod "execpod-affinityzq9dc" in namespace "services-2958" to be "running"
    Aug 25 04:13:29.002: INFO: Pod "execpod-affinityzq9dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.348399ms
    Aug 25 04:13:31.007: INFO: Pod "execpod-affinityzq9dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008382899s
    Aug 25 04:13:31.007: INFO: Pod "execpod-affinityzq9dc" satisfied condition "running"
    Aug 25 04:13:32.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Aug 25 04:13:32.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 25 04:13:32.167: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:13:32.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.214.208 80'
    Aug 25 04:13:32.348: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.214.208 80\nConnection to 10.107.214.208 80 port [tcp/http] succeeded!\n"
    Aug 25 04:13:32.348: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:13:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.214.208:80/ ; done'
    Aug 25 04:13:32.594: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n"
    Aug 25 04:13:32.594: INFO: stdout: "\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-xs6tj\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5\naffinity-clusterip-transition-h7gz5"
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-xs6tj
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.594: INFO: Received response from host: affinity-clusterip-transition-h7gz5
    Aug 25 04:13:32.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-2958 exec execpod-affinityzq9dc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.214.208:80/ ; done'
    Aug 25 04:13:32.885: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.214.208:80/\n"
    Aug 25 04:13:32.885: INFO: stdout: "\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt\naffinity-clusterip-transition-sz5tt"
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Received response from host: affinity-clusterip-transition-sz5tt
    Aug 25 04:13:32.885: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2958, will wait for the garbage collector to delete the pods 08/25/22 04:13:32.894
    Aug 25 04:13:32.955: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.570393ms
    Aug 25 04:13:33.055: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.59872ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 04:13:35.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2958" for this suite. 08/25/22 04:13:35.772
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:35.776
Aug 25 04:13:35.776: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:13:35.777
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:35.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:35.791
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-eead993c-d2e0-493b-bc26-8656aa329283 08/25/22 04:13:35.795
STEP: Creating a pod to test consume configMaps 08/25/22 04:13:35.798
Aug 25 04:13:35.804: INFO: Waiting up to 5m0s for pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076" in namespace "configmap-2485" to be "Succeeded or Failed"
Aug 25 04:13:35.807: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263382ms
Aug 25 04:13:37.813: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009084719s
Aug 25 04:13:39.813: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008898152s
STEP: Saw pod success 08/25/22 04:13:39.813
Aug 25 04:13:39.813: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076" satisfied condition "Succeeded or Failed"
Aug 25 04:13:39.818: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 04:13:39.823
Aug 25 04:13:39.830: INFO: Waiting for pod pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076 to disappear
Aug 25 04:13:39.833: INFO: Pod pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:13:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2485" for this suite. 08/25/22 04:13:39.838
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":289,"skipped":5395,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:35.776
    Aug 25 04:13:35.776: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:13:35.777
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:35.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:35.791
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-eead993c-d2e0-493b-bc26-8656aa329283 08/25/22 04:13:35.795
    STEP: Creating a pod to test consume configMaps 08/25/22 04:13:35.798
    Aug 25 04:13:35.804: INFO: Waiting up to 5m0s for pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076" in namespace "configmap-2485" to be "Succeeded or Failed"
    Aug 25 04:13:35.807: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263382ms
    Aug 25 04:13:37.813: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009084719s
    Aug 25 04:13:39.813: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008898152s
    STEP: Saw pod success 08/25/22 04:13:39.813
    Aug 25 04:13:39.813: INFO: Pod "pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076" satisfied condition "Succeeded or Failed"
    Aug 25 04:13:39.818: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 04:13:39.823
    Aug 25 04:13:39.830: INFO: Waiting for pod pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076 to disappear
    Aug 25 04:13:39.833: INFO: Pod pod-configmaps-54f71176-f391-4251-a9cd-50ebf6591076 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:13:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2485" for this suite. 08/25/22 04:13:39.838
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:13:39.845
Aug 25 04:13:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-probe 08/25/22 04:13:39.846
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:39.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:39.861
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 in namespace container-probe-5448 08/25/22 04:13:39.864
Aug 25 04:13:39.870: INFO: Waiting up to 5m0s for pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6" in namespace "container-probe-5448" to be "not pending"
Aug 25 04:13:39.873: INFO: Pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140534ms
Aug 25 04:13:41.878: INFO: Pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007803574s
Aug 25 04:13:41.878: INFO: Pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6" satisfied condition "not pending"
Aug 25 04:13:41.878: INFO: Started pod busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 in namespace container-probe-5448
STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 04:13:41.878
Aug 25 04:13:41.882: INFO: Initial restart count of pod busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 is 0
Aug 25 04:14:32.022: INFO: Restart count of pod container-probe-5448/busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 is now 1 (50.139512634s elapsed)
STEP: deleting the pod 08/25/22 04:14:32.022
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 25 04:14:32.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5448" for this suite. 08/25/22 04:14:32.034
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":290,"skipped":5411,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [52.195 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:13:39.845
    Aug 25 04:13:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-probe 08/25/22 04:13:39.846
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:13:39.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:13:39.861
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 in namespace container-probe-5448 08/25/22 04:13:39.864
    Aug 25 04:13:39.870: INFO: Waiting up to 5m0s for pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6" in namespace "container-probe-5448" to be "not pending"
    Aug 25 04:13:39.873: INFO: Pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140534ms
    Aug 25 04:13:41.878: INFO: Pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007803574s
    Aug 25 04:13:41.878: INFO: Pod "busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6" satisfied condition "not pending"
    Aug 25 04:13:41.878: INFO: Started pod busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 in namespace container-probe-5448
    STEP: checking the pod's current state and verifying that restartCount is present 08/25/22 04:13:41.878
    Aug 25 04:13:41.882: INFO: Initial restart count of pod busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 is 0
    Aug 25 04:14:32.022: INFO: Restart count of pod container-probe-5448/busybox-6572662b-c8d5-46b6-b6c1-9b1969875ae6 is now 1 (50.139512634s elapsed)
    STEP: deleting the pod 08/25/22 04:14:32.022
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 25 04:14:32.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5448" for this suite. 08/25/22 04:14:32.034
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:14:32.04
Aug 25 04:14:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename job 08/25/22 04:14:32.041
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:32.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:32.058
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 08/25/22 04:14:32.063
STEP: Ensuring job reaches completions 08/25/22 04:14:32.069
STEP: Ensuring pods with index for job exist 08/25/22 04:14:42.073
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 25 04:14:42.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3743" for this suite. 08/25/22 04:14:42.083
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":291,"skipped":5422,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.048 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:14:32.04
    Aug 25 04:14:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename job 08/25/22 04:14:32.041
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:32.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:32.058
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 08/25/22 04:14:32.063
    STEP: Ensuring job reaches completions 08/25/22 04:14:32.069
    STEP: Ensuring pods with index for job exist 08/25/22 04:14:42.073
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 25 04:14:42.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3743" for this suite. 08/25/22 04:14:42.083
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:14:42.09
Aug 25 04:14:42.090: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename events 08/25/22 04:14:42.091
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:42.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:42.107
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/25/22 04:14:42.111
STEP: listing all events in all namespaces 08/25/22 04:14:42.114
STEP: patching the test event 08/25/22 04:14:42.124
STEP: fetching the test event 08/25/22 04:14:42.128
STEP: updating the test event 08/25/22 04:14:42.131
STEP: getting the test event 08/25/22 04:14:42.138
STEP: deleting the test event 08/25/22 04:14:42.141
STEP: listing all events in all namespaces 08/25/22 04:14:42.145
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Aug 25 04:14:42.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3628" for this suite. 08/25/22 04:14:42.157
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":292,"skipped":5432,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.072 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:14:42.09
    Aug 25 04:14:42.090: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename events 08/25/22 04:14:42.091
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:42.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:42.107
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/25/22 04:14:42.111
    STEP: listing all events in all namespaces 08/25/22 04:14:42.114
    STEP: patching the test event 08/25/22 04:14:42.124
    STEP: fetching the test event 08/25/22 04:14:42.128
    STEP: updating the test event 08/25/22 04:14:42.131
    STEP: getting the test event 08/25/22 04:14:42.138
    STEP: deleting the test event 08/25/22 04:14:42.141
    STEP: listing all events in all namespaces 08/25/22 04:14:42.145
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Aug 25 04:14:42.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3628" for this suite. 08/25/22 04:14:42.157
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:14:42.162
Aug 25 04:14:42.162: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svc-latency 08/25/22 04:14:42.164
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:42.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:42.178
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 25 04:14:42.181: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4396 08/25/22 04:14:42.182
I0825 04:14:42.186369      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4396, replica count: 1
I0825 04:14:43.236788      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0825 04:14:44.237603      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 04:14:44.350: INFO: Created: latency-svc-8gqmz
Aug 25 04:14:44.355: INFO: Got endpoints: latency-svc-8gqmz [17.482484ms]
Aug 25 04:14:44.366: INFO: Created: latency-svc-t42nv
Aug 25 04:14:44.370: INFO: Got endpoints: latency-svc-t42nv [14.406832ms]
Aug 25 04:14:44.429: INFO: Created: latency-svc-86m8q
Aug 25 04:14:44.430: INFO: Created: latency-svc-j75vx
Aug 25 04:14:44.430: INFO: Created: latency-svc-wcxpz
Aug 25 04:14:44.430: INFO: Created: latency-svc-6g7bq
Aug 25 04:14:44.430: INFO: Created: latency-svc-bwvgw
Aug 25 04:14:44.430: INFO: Created: latency-svc-d4d4c
Aug 25 04:14:44.430: INFO: Created: latency-svc-xgzdd
Aug 25 04:14:44.430: INFO: Created: latency-svc-lmq2t
Aug 25 04:14:44.437: INFO: Created: latency-svc-w594c
Aug 25 04:14:44.437: INFO: Created: latency-svc-95vtl
Aug 25 04:14:44.437: INFO: Created: latency-svc-fgxjk
Aug 25 04:14:44.437: INFO: Created: latency-svc-hlk5k
Aug 25 04:14:44.437: INFO: Created: latency-svc-km82s
Aug 25 04:14:44.437: INFO: Got endpoints: latency-svc-6g7bq [81.420117ms]
Aug 25 04:14:44.437: INFO: Created: latency-svc-vdq4v
Aug 25 04:14:44.437: INFO: Created: latency-svc-2lx9p
Aug 25 04:14:44.439: INFO: Got endpoints: latency-svc-j75vx [83.821065ms]
Aug 25 04:14:44.440: INFO: Got endpoints: latency-svc-wcxpz [83.781003ms]
Aug 25 04:14:44.440: INFO: Got endpoints: latency-svc-86m8q [83.957272ms]
Aug 25 04:14:44.440: INFO: Got endpoints: latency-svc-95vtl [84.110525ms]
Aug 25 04:14:44.441: INFO: Got endpoints: latency-svc-lmq2t [85.017258ms]
Aug 25 04:14:44.442: INFO: Got endpoints: latency-svc-km82s [86.056625ms]
Aug 25 04:14:44.444: INFO: Got endpoints: latency-svc-bwvgw [74.522225ms]
Aug 25 04:14:44.445: INFO: Created: latency-svc-xdbn5
Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-vdq4v [89.349354ms]
Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-xgzdd [89.49851ms]
Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-d4d4c [89.7417ms]
Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-hlk5k [90.005435ms]
Aug 25 04:14:44.448: INFO: Got endpoints: latency-svc-2lx9p [92.637353ms]
Aug 25 04:14:44.448: INFO: Created: latency-svc-qhjl5
Aug 25 04:14:44.449: INFO: Got endpoints: latency-svc-fgxjk [92.823655ms]
Aug 25 04:14:44.461: INFO: Got endpoints: latency-svc-xdbn5 [24.504951ms]
Aug 25 04:14:44.461: INFO: Got endpoints: latency-svc-w594c [105.830128ms]
Aug 25 04:14:44.461: INFO: Got endpoints: latency-svc-qhjl5 [21.970254ms]
Aug 25 04:14:44.463: INFO: Created: latency-svc-6rngb
Aug 25 04:14:44.466: INFO: Got endpoints: latency-svc-6rngb [26.610668ms]
Aug 25 04:14:44.467: INFO: Created: latency-svc-kw28n
Aug 25 04:14:44.470: INFO: Got endpoints: latency-svc-kw28n [30.406216ms]
Aug 25 04:14:44.471: INFO: Created: latency-svc-qhr5r
Aug 25 04:14:44.473: INFO: Got endpoints: latency-svc-qhr5r [32.574205ms]
Aug 25 04:14:44.480: INFO: Created: latency-svc-rz7jn
Aug 25 04:14:44.482: INFO: Got endpoints: latency-svc-rz7jn [41.924572ms]
Aug 25 04:14:44.483: INFO: Created: latency-svc-g2w7c
Aug 25 04:14:44.485: INFO: Got endpoints: latency-svc-g2w7c [43.857911ms]
Aug 25 04:14:44.486: INFO: Created: latency-svc-57tvt
Aug 25 04:14:44.489: INFO: Got endpoints: latency-svc-57tvt [44.663344ms]
Aug 25 04:14:44.490: INFO: Created: latency-svc-kx27j
Aug 25 04:14:44.492: INFO: Got endpoints: latency-svc-kx27j [47.470073ms]
Aug 25 04:14:44.493: INFO: Created: latency-svc-gxdlh
Aug 25 04:14:44.495: INFO: Got endpoints: latency-svc-gxdlh [50.115923ms]
Aug 25 04:14:44.497: INFO: Created: latency-svc-qbhkc
Aug 25 04:14:44.499: INFO: Got endpoints: latency-svc-qbhkc [53.609226ms]
Aug 25 04:14:44.501: INFO: Created: latency-svc-6sh4w
Aug 25 04:14:44.504: INFO: Got endpoints: latency-svc-6sh4w [58.670315ms]
Aug 25 04:14:44.504: INFO: Created: latency-svc-s2sk9
Aug 25 04:14:44.507: INFO: Got endpoints: latency-svc-s2sk9 [59.20421ms]
Aug 25 04:14:44.509: INFO: Created: latency-svc-wz6ph
Aug 25 04:14:44.514: INFO: Created: latency-svc-9sccz
Aug 25 04:14:44.514: INFO: Got endpoints: latency-svc-wz6ph [65.142202ms]
Aug 25 04:14:44.516: INFO: Created: latency-svc-z2l4j
Aug 25 04:14:44.516: INFO: Got endpoints: latency-svc-9sccz [54.81337ms]
Aug 25 04:14:44.519: INFO: Got endpoints: latency-svc-z2l4j [57.458075ms]
Aug 25 04:14:44.519: INFO: Created: latency-svc-r64r7
Aug 25 04:14:44.523: INFO: Created: latency-svc-zmz4d
Aug 25 04:14:44.526: INFO: Created: latency-svc-4b56q
Aug 25 04:14:44.530: INFO: Created: latency-svc-5lkpf
Aug 25 04:14:44.534: INFO: Created: latency-svc-ztmkq
Aug 25 04:14:44.538: INFO: Created: latency-svc-xgrbh
Aug 25 04:14:44.543: INFO: Created: latency-svc-qpdk8
Aug 25 04:14:44.548: INFO: Created: latency-svc-qngdp
Aug 25 04:14:44.554: INFO: Created: latency-svc-pm9dd
Aug 25 04:14:44.554: INFO: Got endpoints: latency-svc-r64r7 [92.696332ms]
Aug 25 04:14:44.559: INFO: Created: latency-svc-scmt7
Aug 25 04:14:44.605: INFO: Got endpoints: latency-svc-zmz4d [138.528663ms]
Aug 25 04:14:44.605: INFO: Created: latency-svc-t8q4w
Aug 25 04:14:44.605: INFO: Created: latency-svc-tfmlc
Aug 25 04:14:44.605: INFO: Created: latency-svc-mnmwj
Aug 25 04:14:44.606: INFO: Created: latency-svc-vc5sl
Aug 25 04:14:44.606: INFO: Created: latency-svc-mkq45
Aug 25 04:14:44.606: INFO: Created: latency-svc-8sc9d
Aug 25 04:14:44.613: INFO: Created: latency-svc-szh9g
Aug 25 04:14:44.655: INFO: Got endpoints: latency-svc-4b56q [184.490861ms]
Aug 25 04:14:44.665: INFO: Created: latency-svc-qfc4g
Aug 25 04:14:44.705: INFO: Got endpoints: latency-svc-5lkpf [232.078028ms]
Aug 25 04:14:44.716: INFO: Created: latency-svc-ggl82
Aug 25 04:14:44.754: INFO: Got endpoints: latency-svc-ztmkq [272.436437ms]
Aug 25 04:14:44.765: INFO: Created: latency-svc-4lckq
Aug 25 04:14:44.805: INFO: Got endpoints: latency-svc-xgrbh [319.492815ms]
Aug 25 04:14:44.815: INFO: Created: latency-svc-rrlrd
Aug 25 04:14:44.854: INFO: Got endpoints: latency-svc-qpdk8 [365.106723ms]
Aug 25 04:14:44.865: INFO: Created: latency-svc-b2ffj
Aug 25 04:14:44.906: INFO: Got endpoints: latency-svc-qngdp [413.17546ms]
Aug 25 04:14:44.916: INFO: Created: latency-svc-sc47n
Aug 25 04:14:44.954: INFO: Got endpoints: latency-svc-pm9dd [458.431262ms]
Aug 25 04:14:44.964: INFO: Created: latency-svc-xk6rw
Aug 25 04:14:45.005: INFO: Got endpoints: latency-svc-scmt7 [505.485462ms]
Aug 25 04:14:45.029: INFO: Created: latency-svc-f9mdb
Aug 25 04:14:45.054: INFO: Got endpoints: latency-svc-t8q4w [547.114062ms]
Aug 25 04:14:45.062: INFO: Created: latency-svc-zxncm
Aug 25 04:14:45.106: INFO: Got endpoints: latency-svc-tfmlc [589.921259ms]
Aug 25 04:14:45.116: INFO: Created: latency-svc-9wjxn
Aug 25 04:14:45.154: INFO: Got endpoints: latency-svc-mnmwj [650.263485ms]
Aug 25 04:14:45.165: INFO: Created: latency-svc-dgr5t
Aug 25 04:14:45.205: INFO: Got endpoints: latency-svc-mkq45 [686.439105ms]
Aug 25 04:14:45.216: INFO: Created: latency-svc-8dwwg
Aug 25 04:14:45.255: INFO: Got endpoints: latency-svc-8sc9d [741.000379ms]
Aug 25 04:14:45.265: INFO: Created: latency-svc-sw9pt
Aug 25 04:14:45.304: INFO: Got endpoints: latency-svc-vc5sl [750.065196ms]
Aug 25 04:14:45.315: INFO: Created: latency-svc-7wdbc
Aug 25 04:14:45.355: INFO: Got endpoints: latency-svc-szh9g [750.188651ms]
Aug 25 04:14:45.366: INFO: Created: latency-svc-sd7x5
Aug 25 04:14:45.405: INFO: Got endpoints: latency-svc-qfc4g [750.154371ms]
Aug 25 04:14:45.419: INFO: Created: latency-svc-5fdnj
Aug 25 04:14:45.455: INFO: Got endpoints: latency-svc-ggl82 [749.076479ms]
Aug 25 04:14:45.463: INFO: Created: latency-svc-mf2f2
Aug 25 04:14:45.505: INFO: Got endpoints: latency-svc-4lckq [751.398218ms]
Aug 25 04:14:45.515: INFO: Created: latency-svc-t9zft
Aug 25 04:14:45.556: INFO: Got endpoints: latency-svc-rrlrd [751.045837ms]
Aug 25 04:14:45.566: INFO: Created: latency-svc-lq9p2
Aug 25 04:14:45.605: INFO: Got endpoints: latency-svc-b2ffj [751.041477ms]
Aug 25 04:14:45.615: INFO: Created: latency-svc-kf2m2
Aug 25 04:14:45.656: INFO: Got endpoints: latency-svc-sc47n [750.293433ms]
Aug 25 04:14:45.666: INFO: Created: latency-svc-xb7br
Aug 25 04:14:45.705: INFO: Got endpoints: latency-svc-xk6rw [750.786751ms]
Aug 25 04:14:45.714: INFO: Created: latency-svc-cr6cn
Aug 25 04:14:45.755: INFO: Got endpoints: latency-svc-f9mdb [750.433278ms]
Aug 25 04:14:45.765: INFO: Created: latency-svc-hm2qr
Aug 25 04:14:45.805: INFO: Got endpoints: latency-svc-zxncm [750.22497ms]
Aug 25 04:14:45.814: INFO: Created: latency-svc-wh44c
Aug 25 04:14:45.854: INFO: Got endpoints: latency-svc-9wjxn [748.226567ms]
Aug 25 04:14:45.863: INFO: Created: latency-svc-ncjkm
Aug 25 04:14:45.905: INFO: Got endpoints: latency-svc-dgr5t [750.994536ms]
Aug 25 04:14:45.915: INFO: Created: latency-svc-7sztm
Aug 25 04:14:45.954: INFO: Got endpoints: latency-svc-8dwwg [748.322667ms]
Aug 25 04:14:45.964: INFO: Created: latency-svc-52v27
Aug 25 04:14:46.005: INFO: Got endpoints: latency-svc-sw9pt [749.878973ms]
Aug 25 04:14:46.014: INFO: Created: latency-svc-jpbqw
Aug 25 04:14:46.055: INFO: Got endpoints: latency-svc-7wdbc [751.026286ms]
Aug 25 04:14:46.065: INFO: Created: latency-svc-j856s
Aug 25 04:14:46.105: INFO: Got endpoints: latency-svc-sd7x5 [750.102634ms]
Aug 25 04:14:46.114: INFO: Created: latency-svc-q9zcd
Aug 25 04:14:46.155: INFO: Got endpoints: latency-svc-5fdnj [750.181138ms]
Aug 25 04:14:46.165: INFO: Created: latency-svc-hdj9c
Aug 25 04:14:46.203: INFO: Got endpoints: latency-svc-mf2f2 [748.909491ms]
Aug 25 04:14:46.213: INFO: Created: latency-svc-tnhg5
Aug 25 04:14:46.255: INFO: Got endpoints: latency-svc-t9zft [749.738965ms]
Aug 25 04:14:46.265: INFO: Created: latency-svc-5pf5m
Aug 25 04:14:46.305: INFO: Got endpoints: latency-svc-lq9p2 [749.219529ms]
Aug 25 04:14:46.315: INFO: Created: latency-svc-scdt9
Aug 25 04:14:46.355: INFO: Got endpoints: latency-svc-kf2m2 [750.029768ms]
Aug 25 04:14:46.365: INFO: Created: latency-svc-7rfwx
Aug 25 04:14:46.405: INFO: Got endpoints: latency-svc-xb7br [749.155711ms]
Aug 25 04:14:46.416: INFO: Created: latency-svc-mqczt
Aug 25 04:14:46.455: INFO: Got endpoints: latency-svc-cr6cn [750.842163ms]
Aug 25 04:14:46.466: INFO: Created: latency-svc-qlf95
Aug 25 04:14:46.505: INFO: Got endpoints: latency-svc-hm2qr [749.40983ms]
Aug 25 04:14:46.519: INFO: Created: latency-svc-jcdhn
Aug 25 04:14:46.555: INFO: Got endpoints: latency-svc-wh44c [749.85553ms]
Aug 25 04:14:46.564: INFO: Created: latency-svc-6txfk
Aug 25 04:14:46.605: INFO: Got endpoints: latency-svc-ncjkm [750.349057ms]
Aug 25 04:14:46.614: INFO: Created: latency-svc-zhtqm
Aug 25 04:14:46.657: INFO: Got endpoints: latency-svc-7sztm [751.190035ms]
Aug 25 04:14:46.669: INFO: Created: latency-svc-2l44q
Aug 25 04:14:46.705: INFO: Got endpoints: latency-svc-52v27 [751.196938ms]
Aug 25 04:14:46.714: INFO: Created: latency-svc-zsq5k
Aug 25 04:14:46.755: INFO: Got endpoints: latency-svc-jpbqw [750.156695ms]
Aug 25 04:14:46.765: INFO: Created: latency-svc-p2zsv
Aug 25 04:14:46.806: INFO: Got endpoints: latency-svc-j856s [750.192549ms]
Aug 25 04:14:46.816: INFO: Created: latency-svc-dtxvx
Aug 25 04:14:46.855: INFO: Got endpoints: latency-svc-q9zcd [750.132806ms]
Aug 25 04:14:46.868: INFO: Created: latency-svc-8glnh
Aug 25 04:14:46.905: INFO: Got endpoints: latency-svc-hdj9c [749.96544ms]
Aug 25 04:14:46.915: INFO: Created: latency-svc-gvmlf
Aug 25 04:14:46.955: INFO: Got endpoints: latency-svc-tnhg5 [751.100161ms]
Aug 25 04:14:46.967: INFO: Created: latency-svc-969zs
Aug 25 04:14:47.005: INFO: Got endpoints: latency-svc-5pf5m [749.562923ms]
Aug 25 04:14:47.014: INFO: Created: latency-svc-fzq48
Aug 25 04:14:47.056: INFO: Got endpoints: latency-svc-scdt9 [750.16338ms]
Aug 25 04:14:47.065: INFO: Created: latency-svc-cfsrd
Aug 25 04:14:47.120: INFO: Got endpoints: latency-svc-7rfwx [765.008666ms]
Aug 25 04:14:47.131: INFO: Created: latency-svc-rhx6z
Aug 25 04:14:47.154: INFO: Got endpoints: latency-svc-mqczt [748.784633ms]
Aug 25 04:14:47.162: INFO: Created: latency-svc-4fd9l
Aug 25 04:14:47.202: INFO: Got endpoints: latency-svc-qlf95 [746.916914ms]
Aug 25 04:14:47.210: INFO: Created: latency-svc-7r89w
Aug 25 04:14:47.254: INFO: Got endpoints: latency-svc-jcdhn [749.00686ms]
Aug 25 04:14:47.261: INFO: Created: latency-svc-scv4d
Aug 25 04:14:47.304: INFO: Got endpoints: latency-svc-6txfk [749.271583ms]
Aug 25 04:14:47.312: INFO: Created: latency-svc-z4zdt
Aug 25 04:14:47.353: INFO: Got endpoints: latency-svc-zhtqm [747.667022ms]
Aug 25 04:14:47.360: INFO: Created: latency-svc-5xsxr
Aug 25 04:14:47.403: INFO: Got endpoints: latency-svc-2l44q [746.623219ms]
Aug 25 04:14:47.411: INFO: Created: latency-svc-2v57r
Aug 25 04:14:47.453: INFO: Got endpoints: latency-svc-zsq5k [748.391326ms]
Aug 25 04:14:47.461: INFO: Created: latency-svc-9tx4h
Aug 25 04:14:47.503: INFO: Got endpoints: latency-svc-p2zsv [747.7143ms]
Aug 25 04:14:47.512: INFO: Created: latency-svc-9strl
Aug 25 04:14:47.554: INFO: Got endpoints: latency-svc-dtxvx [747.991603ms]
Aug 25 04:14:47.563: INFO: Created: latency-svc-ccrnh
Aug 25 04:14:47.604: INFO: Got endpoints: latency-svc-8glnh [748.765769ms]
Aug 25 04:14:47.614: INFO: Created: latency-svc-76zzc
Aug 25 04:14:47.655: INFO: Got endpoints: latency-svc-gvmlf [749.88464ms]
Aug 25 04:14:47.664: INFO: Created: latency-svc-nkkzj
Aug 25 04:14:47.705: INFO: Got endpoints: latency-svc-969zs [750.099189ms]
Aug 25 04:14:47.714: INFO: Created: latency-svc-tckkn
Aug 25 04:14:47.755: INFO: Got endpoints: latency-svc-fzq48 [750.389065ms]
Aug 25 04:14:47.765: INFO: Created: latency-svc-5zskq
Aug 25 04:14:47.805: INFO: Got endpoints: latency-svc-cfsrd [749.647177ms]
Aug 25 04:14:47.815: INFO: Created: latency-svc-mv7kx
Aug 25 04:14:47.854: INFO: Got endpoints: latency-svc-rhx6z [733.637624ms]
Aug 25 04:14:47.864: INFO: Created: latency-svc-mq4hr
Aug 25 04:14:47.905: INFO: Got endpoints: latency-svc-4fd9l [751.004661ms]
Aug 25 04:14:47.915: INFO: Created: latency-svc-c7ptt
Aug 25 04:14:47.955: INFO: Got endpoints: latency-svc-7r89w [752.821562ms]
Aug 25 04:14:47.965: INFO: Created: latency-svc-4l58l
Aug 25 04:14:48.005: INFO: Got endpoints: latency-svc-scv4d [751.339618ms]
Aug 25 04:14:48.014: INFO: Created: latency-svc-zx2bs
Aug 25 04:14:48.055: INFO: Got endpoints: latency-svc-z4zdt [750.876949ms]
Aug 25 04:14:48.065: INFO: Created: latency-svc-fcjwx
Aug 25 04:14:48.106: INFO: Got endpoints: latency-svc-5xsxr [753.141634ms]
Aug 25 04:14:48.116: INFO: Created: latency-svc-87wtg
Aug 25 04:14:48.154: INFO: Got endpoints: latency-svc-2v57r [751.184455ms]
Aug 25 04:14:48.164: INFO: Created: latency-svc-g2g62
Aug 25 04:14:48.205: INFO: Got endpoints: latency-svc-9tx4h [751.186359ms]
Aug 25 04:14:48.215: INFO: Created: latency-svc-hkxwr
Aug 25 04:14:48.255: INFO: Got endpoints: latency-svc-9strl [751.972388ms]
Aug 25 04:14:48.265: INFO: Created: latency-svc-gghkz
Aug 25 04:14:48.304: INFO: Got endpoints: latency-svc-ccrnh [750.684116ms]
Aug 25 04:14:48.314: INFO: Created: latency-svc-6b5kj
Aug 25 04:14:48.354: INFO: Got endpoints: latency-svc-76zzc [750.14691ms]
Aug 25 04:14:48.365: INFO: Created: latency-svc-7kw28
Aug 25 04:14:48.405: INFO: Got endpoints: latency-svc-nkkzj [749.839821ms]
Aug 25 04:14:48.412: INFO: Created: latency-svc-bzzjc
Aug 25 04:14:48.454: INFO: Got endpoints: latency-svc-tckkn [748.768393ms]
Aug 25 04:14:48.462: INFO: Created: latency-svc-5g7w4
Aug 25 04:14:48.507: INFO: Got endpoints: latency-svc-5zskq [751.476633ms]
Aug 25 04:14:48.519: INFO: Created: latency-svc-bthxn
Aug 25 04:14:48.554: INFO: Got endpoints: latency-svc-mv7kx [748.519861ms]
Aug 25 04:14:48.562: INFO: Created: latency-svc-8t86n
Aug 25 04:14:48.605: INFO: Got endpoints: latency-svc-mq4hr [750.987214ms]
Aug 25 04:14:48.617: INFO: Created: latency-svc-9r4wm
Aug 25 04:14:48.655: INFO: Got endpoints: latency-svc-c7ptt [749.689329ms]
Aug 25 04:14:48.666: INFO: Created: latency-svc-g9r6g
Aug 25 04:14:48.705: INFO: Got endpoints: latency-svc-4l58l [749.856602ms]
Aug 25 04:14:48.717: INFO: Created: latency-svc-tvlp2
Aug 25 04:14:48.755: INFO: Got endpoints: latency-svc-zx2bs [749.938706ms]
Aug 25 04:14:48.766: INFO: Created: latency-svc-d2twl
Aug 25 04:14:48.806: INFO: Got endpoints: latency-svc-fcjwx [750.646091ms]
Aug 25 04:14:48.816: INFO: Created: latency-svc-66dsd
Aug 25 04:14:48.855: INFO: Got endpoints: latency-svc-87wtg [748.789199ms]
Aug 25 04:14:48.864: INFO: Created: latency-svc-ttpcq
Aug 25 04:14:48.905: INFO: Got endpoints: latency-svc-g2g62 [750.436492ms]
Aug 25 04:14:48.916: INFO: Created: latency-svc-gv5rs
Aug 25 04:14:48.954: INFO: Got endpoints: latency-svc-hkxwr [749.647466ms]
Aug 25 04:14:48.965: INFO: Created: latency-svc-r2kq9
Aug 25 04:14:49.005: INFO: Got endpoints: latency-svc-gghkz [750.011352ms]
Aug 25 04:14:49.015: INFO: Created: latency-svc-648m4
Aug 25 04:14:49.055: INFO: Got endpoints: latency-svc-6b5kj [750.67089ms]
Aug 25 04:14:49.066: INFO: Created: latency-svc-w55f5
Aug 25 04:14:49.105: INFO: Got endpoints: latency-svc-7kw28 [750.679909ms]
Aug 25 04:14:49.116: INFO: Created: latency-svc-q9zhw
Aug 25 04:14:49.155: INFO: Got endpoints: latency-svc-bzzjc [750.561222ms]
Aug 25 04:14:49.165: INFO: Created: latency-svc-j68qb
Aug 25 04:14:49.205: INFO: Got endpoints: latency-svc-5g7w4 [751.012307ms]
Aug 25 04:14:49.229: INFO: Created: latency-svc-rvpft
Aug 25 04:14:49.254: INFO: Got endpoints: latency-svc-bthxn [747.412349ms]
Aug 25 04:14:49.261: INFO: Created: latency-svc-fpzrf
Aug 25 04:14:49.305: INFO: Got endpoints: latency-svc-8t86n [751.025838ms]
Aug 25 04:14:49.314: INFO: Created: latency-svc-n5x6w
Aug 25 04:14:49.354: INFO: Got endpoints: latency-svc-9r4wm [748.447468ms]
Aug 25 04:14:49.363: INFO: Created: latency-svc-ts2jl
Aug 25 04:14:49.404: INFO: Got endpoints: latency-svc-g9r6g [749.557987ms]
Aug 25 04:14:49.415: INFO: Created: latency-svc-7rsfp
Aug 25 04:14:49.456: INFO: Got endpoints: latency-svc-tvlp2 [750.549583ms]
Aug 25 04:14:49.466: INFO: Created: latency-svc-qsmvd
Aug 25 04:14:49.505: INFO: Got endpoints: latency-svc-d2twl [749.827589ms]
Aug 25 04:14:49.514: INFO: Created: latency-svc-nvrvv
Aug 25 04:14:49.556: INFO: Got endpoints: latency-svc-66dsd [750.581748ms]
Aug 25 04:14:49.567: INFO: Created: latency-svc-m27ck
Aug 25 04:14:49.605: INFO: Got endpoints: latency-svc-ttpcq [750.834156ms]
Aug 25 04:14:49.616: INFO: Created: latency-svc-trhj7
Aug 25 04:14:49.655: INFO: Got endpoints: latency-svc-gv5rs [749.867941ms]
Aug 25 04:14:49.665: INFO: Created: latency-svc-gwvbp
Aug 25 04:14:49.704: INFO: Got endpoints: latency-svc-r2kq9 [749.973691ms]
Aug 25 04:14:49.715: INFO: Created: latency-svc-xj8fj
Aug 25 04:14:49.755: INFO: Got endpoints: latency-svc-648m4 [750.594673ms]
Aug 25 04:14:49.769: INFO: Created: latency-svc-5trzm
Aug 25 04:14:49.804: INFO: Got endpoints: latency-svc-w55f5 [748.993916ms]
Aug 25 04:14:49.817: INFO: Created: latency-svc-p2gr2
Aug 25 04:14:49.855: INFO: Got endpoints: latency-svc-q9zhw [749.901966ms]
Aug 25 04:14:49.863: INFO: Created: latency-svc-qnkm6
Aug 25 04:14:49.905: INFO: Got endpoints: latency-svc-j68qb [749.507653ms]
Aug 25 04:14:49.914: INFO: Created: latency-svc-vx47d
Aug 25 04:14:49.955: INFO: Got endpoints: latency-svc-rvpft [749.980168ms]
Aug 25 04:14:49.967: INFO: Created: latency-svc-5gp95
Aug 25 04:14:50.004: INFO: Got endpoints: latency-svc-fpzrf [749.351895ms]
Aug 25 04:14:50.013: INFO: Created: latency-svc-cbdfq
Aug 25 04:14:50.055: INFO: Got endpoints: latency-svc-n5x6w [750.018488ms]
Aug 25 04:14:50.067: INFO: Created: latency-svc-c8snr
Aug 25 04:14:50.106: INFO: Got endpoints: latency-svc-ts2jl [752.461072ms]
Aug 25 04:14:50.115: INFO: Created: latency-svc-xgst7
Aug 25 04:14:50.156: INFO: Got endpoints: latency-svc-7rsfp [751.093488ms]
Aug 25 04:14:50.165: INFO: Created: latency-svc-56qfq
Aug 25 04:14:50.205: INFO: Got endpoints: latency-svc-qsmvd [748.99501ms]
Aug 25 04:14:50.215: INFO: Created: latency-svc-v88ms
Aug 25 04:14:50.255: INFO: Got endpoints: latency-svc-nvrvv [749.7141ms]
Aug 25 04:14:50.264: INFO: Created: latency-svc-6p7dv
Aug 25 04:14:50.305: INFO: Got endpoints: latency-svc-m27ck [748.44099ms]
Aug 25 04:14:50.314: INFO: Created: latency-svc-snd8v
Aug 25 04:14:50.356: INFO: Got endpoints: latency-svc-trhj7 [750.731099ms]
Aug 25 04:14:50.366: INFO: Created: latency-svc-qkndv
Aug 25 04:14:50.405: INFO: Got endpoints: latency-svc-gwvbp [750.134504ms]
Aug 25 04:14:50.415: INFO: Created: latency-svc-xs46d
Aug 25 04:14:50.455: INFO: Got endpoints: latency-svc-xj8fj [750.571954ms]
Aug 25 04:14:50.465: INFO: Created: latency-svc-gjwbm
Aug 25 04:14:50.505: INFO: Got endpoints: latency-svc-5trzm [749.459855ms]
Aug 25 04:14:50.515: INFO: Created: latency-svc-qncbz
Aug 25 04:14:50.555: INFO: Got endpoints: latency-svc-p2gr2 [751.038219ms]
Aug 25 04:14:50.565: INFO: Created: latency-svc-tbwcw
Aug 25 04:14:50.605: INFO: Got endpoints: latency-svc-qnkm6 [749.767797ms]
Aug 25 04:14:50.615: INFO: Created: latency-svc-b6bdv
Aug 25 04:14:50.656: INFO: Got endpoints: latency-svc-vx47d [751.323673ms]
Aug 25 04:14:50.666: INFO: Created: latency-svc-78fqp
Aug 25 04:14:50.704: INFO: Got endpoints: latency-svc-5gp95 [749.172687ms]
Aug 25 04:14:50.714: INFO: Created: latency-svc-c2pj7
Aug 25 04:14:50.754: INFO: Got endpoints: latency-svc-cbdfq [750.452098ms]
Aug 25 04:14:50.764: INFO: Created: latency-svc-c9m7d
Aug 25 04:14:50.805: INFO: Got endpoints: latency-svc-c8snr [749.900252ms]
Aug 25 04:14:50.815: INFO: Created: latency-svc-69496
Aug 25 04:14:50.856: INFO: Got endpoints: latency-svc-xgst7 [749.637661ms]
Aug 25 04:14:50.866: INFO: Created: latency-svc-v865f
Aug 25 04:14:50.904: INFO: Got endpoints: latency-svc-56qfq [748.829562ms]
Aug 25 04:14:50.915: INFO: Created: latency-svc-qb8w6
Aug 25 04:14:50.953: INFO: Got endpoints: latency-svc-v88ms [748.306176ms]
Aug 25 04:14:50.963: INFO: Created: latency-svc-s6zlt
Aug 25 04:14:51.004: INFO: Got endpoints: latency-svc-6p7dv [749.405588ms]
Aug 25 04:14:51.014: INFO: Created: latency-svc-9xth5
Aug 25 04:14:51.055: INFO: Got endpoints: latency-svc-snd8v [749.82416ms]
Aug 25 04:14:51.070: INFO: Created: latency-svc-s884r
Aug 25 04:14:51.105: INFO: Got endpoints: latency-svc-qkndv [748.554029ms]
Aug 25 04:14:51.115: INFO: Created: latency-svc-s5cjr
Aug 25 04:14:51.155: INFO: Got endpoints: latency-svc-xs46d [749.587576ms]
Aug 25 04:14:51.164: INFO: Created: latency-svc-8r8kq
Aug 25 04:14:51.205: INFO: Got endpoints: latency-svc-gjwbm [749.562438ms]
Aug 25 04:14:51.214: INFO: Created: latency-svc-dmf5b
Aug 25 04:14:51.254: INFO: Got endpoints: latency-svc-qncbz [749.413779ms]
Aug 25 04:14:51.263: INFO: Created: latency-svc-gcxzk
Aug 25 04:14:51.305: INFO: Got endpoints: latency-svc-tbwcw [749.480951ms]
Aug 25 04:14:51.315: INFO: Created: latency-svc-rxcpg
Aug 25 04:14:51.354: INFO: Got endpoints: latency-svc-b6bdv [749.144466ms]
Aug 25 04:14:51.363: INFO: Created: latency-svc-hk5g5
Aug 25 04:14:51.406: INFO: Got endpoints: latency-svc-78fqp [749.420029ms]
Aug 25 04:14:51.416: INFO: Created: latency-svc-j54hw
Aug 25 04:14:51.455: INFO: Got endpoints: latency-svc-c2pj7 [750.725031ms]
Aug 25 04:14:51.464: INFO: Created: latency-svc-q9tnh
Aug 25 04:14:51.509: INFO: Got endpoints: latency-svc-c9m7d [754.527777ms]
Aug 25 04:14:51.519: INFO: Created: latency-svc-jv8mp
Aug 25 04:14:51.555: INFO: Got endpoints: latency-svc-69496 [749.668103ms]
Aug 25 04:14:51.565: INFO: Created: latency-svc-tcgfd
Aug 25 04:14:51.605: INFO: Got endpoints: latency-svc-v865f [749.418671ms]
Aug 25 04:14:51.618: INFO: Created: latency-svc-6dt6k
Aug 25 04:14:51.655: INFO: Got endpoints: latency-svc-qb8w6 [750.494044ms]
Aug 25 04:14:51.665: INFO: Created: latency-svc-vprw9
Aug 25 04:14:51.704: INFO: Got endpoints: latency-svc-s6zlt [751.080024ms]
Aug 25 04:14:51.716: INFO: Created: latency-svc-98hsn
Aug 25 04:14:51.755: INFO: Got endpoints: latency-svc-9xth5 [751.153167ms]
Aug 25 04:14:51.765: INFO: Created: latency-svc-ggswq
Aug 25 04:14:51.805: INFO: Got endpoints: latency-svc-s884r [750.736672ms]
Aug 25 04:14:51.815: INFO: Created: latency-svc-mrmws
Aug 25 04:14:51.855: INFO: Got endpoints: latency-svc-s5cjr [750.254123ms]
Aug 25 04:14:51.865: INFO: Created: latency-svc-qgl5x
Aug 25 04:14:51.904: INFO: Got endpoints: latency-svc-8r8kq [749.302537ms]
Aug 25 04:14:51.914: INFO: Created: latency-svc-8xmvn
Aug 25 04:14:51.953: INFO: Got endpoints: latency-svc-dmf5b [748.322764ms]
Aug 25 04:14:51.963: INFO: Created: latency-svc-c5tqj
Aug 25 04:14:52.005: INFO: Got endpoints: latency-svc-gcxzk [750.439006ms]
Aug 25 04:14:52.015: INFO: Created: latency-svc-szp5v
Aug 25 04:14:52.056: INFO: Got endpoints: latency-svc-rxcpg [751.08345ms]
Aug 25 04:14:52.067: INFO: Created: latency-svc-mhklt
Aug 25 04:14:52.104: INFO: Got endpoints: latency-svc-hk5g5 [749.690902ms]
Aug 25 04:14:52.114: INFO: Created: latency-svc-n95pr
Aug 25 04:14:52.155: INFO: Got endpoints: latency-svc-j54hw [748.996121ms]
Aug 25 04:14:52.164: INFO: Created: latency-svc-2ktj2
Aug 25 04:14:52.204: INFO: Got endpoints: latency-svc-q9tnh [749.621847ms]
Aug 25 04:14:52.254: INFO: Got endpoints: latency-svc-jv8mp [745.075274ms]
Aug 25 04:14:52.304: INFO: Got endpoints: latency-svc-tcgfd [749.051418ms]
Aug 25 04:14:52.354: INFO: Got endpoints: latency-svc-6dt6k [748.898722ms]
Aug 25 04:14:52.404: INFO: Got endpoints: latency-svc-vprw9 [748.505484ms]
Aug 25 04:14:52.454: INFO: Got endpoints: latency-svc-98hsn [750.008172ms]
Aug 25 04:14:52.504: INFO: Got endpoints: latency-svc-ggswq [749.128129ms]
Aug 25 04:14:52.556: INFO: Got endpoints: latency-svc-mrmws [750.182721ms]
Aug 25 04:14:52.605: INFO: Got endpoints: latency-svc-qgl5x [749.958614ms]
Aug 25 04:14:52.655: INFO: Got endpoints: latency-svc-8xmvn [750.984023ms]
Aug 25 04:14:52.705: INFO: Got endpoints: latency-svc-c5tqj [752.388716ms]
Aug 25 04:14:52.755: INFO: Got endpoints: latency-svc-szp5v [750.096975ms]
Aug 25 04:14:52.805: INFO: Got endpoints: latency-svc-mhklt [748.907028ms]
Aug 25 04:14:52.855: INFO: Got endpoints: latency-svc-n95pr [751.392937ms]
Aug 25 04:14:52.905: INFO: Got endpoints: latency-svc-2ktj2 [749.749844ms]
Aug 25 04:14:52.905: INFO: Latencies: [14.406832ms 21.970254ms 24.504951ms 26.610668ms 30.406216ms 32.574205ms 41.924572ms 43.857911ms 44.663344ms 47.470073ms 50.115923ms 53.609226ms 54.81337ms 57.458075ms 58.670315ms 59.20421ms 65.142202ms 74.522225ms 81.420117ms 83.781003ms 83.821065ms 83.957272ms 84.110525ms 85.017258ms 86.056625ms 89.349354ms 89.49851ms 89.7417ms 90.005435ms 92.637353ms 92.696332ms 92.823655ms 105.830128ms 138.528663ms 184.490861ms 232.078028ms 272.436437ms 319.492815ms 365.106723ms 413.17546ms 458.431262ms 505.485462ms 547.114062ms 589.921259ms 650.263485ms 686.439105ms 733.637624ms 741.000379ms 745.075274ms 746.623219ms 746.916914ms 747.412349ms 747.667022ms 747.7143ms 747.991603ms 748.226567ms 748.306176ms 748.322667ms 748.322764ms 748.391326ms 748.44099ms 748.447468ms 748.505484ms 748.519861ms 748.554029ms 748.765769ms 748.768393ms 748.784633ms 748.789199ms 748.829562ms 748.898722ms 748.907028ms 748.909491ms 748.993916ms 748.99501ms 748.996121ms 749.00686ms 749.051418ms 749.076479ms 749.128129ms 749.144466ms 749.155711ms 749.172687ms 749.219529ms 749.271583ms 749.302537ms 749.351895ms 749.405588ms 749.40983ms 749.413779ms 749.418671ms 749.420029ms 749.459855ms 749.480951ms 749.507653ms 749.557987ms 749.562438ms 749.562923ms 749.587576ms 749.621847ms 749.637661ms 749.647177ms 749.647466ms 749.668103ms 749.689329ms 749.690902ms 749.7141ms 749.738965ms 749.749844ms 749.767797ms 749.82416ms 749.827589ms 749.839821ms 749.85553ms 749.856602ms 749.867941ms 749.878973ms 749.88464ms 749.900252ms 749.901966ms 749.938706ms 749.958614ms 749.96544ms 749.973691ms 749.980168ms 750.008172ms 750.011352ms 750.018488ms 750.029768ms 750.065196ms 750.096975ms 750.099189ms 750.102634ms 750.132806ms 750.134504ms 750.14691ms 750.154371ms 750.156695ms 750.16338ms 750.181138ms 750.182721ms 750.188651ms 750.192549ms 750.22497ms 750.254123ms 750.293433ms 750.349057ms 750.389065ms 750.433278ms 750.436492ms 750.439006ms 750.452098ms 750.494044ms 750.549583ms 750.561222ms 750.571954ms 750.581748ms 750.594673ms 750.646091ms 750.67089ms 750.679909ms 750.684116ms 750.725031ms 750.731099ms 750.736672ms 750.786751ms 750.834156ms 750.842163ms 750.876949ms 750.984023ms 750.987214ms 750.994536ms 751.004661ms 751.012307ms 751.025838ms 751.026286ms 751.038219ms 751.041477ms 751.045837ms 751.080024ms 751.08345ms 751.093488ms 751.100161ms 751.153167ms 751.184455ms 751.186359ms 751.190035ms 751.196938ms 751.323673ms 751.339618ms 751.392937ms 751.398218ms 751.476633ms 751.972388ms 752.388716ms 752.461072ms 752.821562ms 753.141634ms 754.527777ms 765.008666ms]
Aug 25 04:14:52.905: INFO: 50 %ile: 749.637661ms
Aug 25 04:14:52.905: INFO: 90 %ile: 751.08345ms
Aug 25 04:14:52.905: INFO: 99 %ile: 754.527777ms
Aug 25 04:14:52.905: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Aug 25 04:14:52.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4396" for this suite. 08/25/22 04:14:52.911
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":293,"skipped":5432,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [10.755 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:14:42.162
    Aug 25 04:14:42.162: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svc-latency 08/25/22 04:14:42.164
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:42.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:42.178
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 25 04:14:42.181: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4396 08/25/22 04:14:42.182
    I0825 04:14:42.186369      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4396, replica count: 1
    I0825 04:14:43.236788      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0825 04:14:44.237603      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 04:14:44.350: INFO: Created: latency-svc-8gqmz
    Aug 25 04:14:44.355: INFO: Got endpoints: latency-svc-8gqmz [17.482484ms]
    Aug 25 04:14:44.366: INFO: Created: latency-svc-t42nv
    Aug 25 04:14:44.370: INFO: Got endpoints: latency-svc-t42nv [14.406832ms]
    Aug 25 04:14:44.429: INFO: Created: latency-svc-86m8q
    Aug 25 04:14:44.430: INFO: Created: latency-svc-j75vx
    Aug 25 04:14:44.430: INFO: Created: latency-svc-wcxpz
    Aug 25 04:14:44.430: INFO: Created: latency-svc-6g7bq
    Aug 25 04:14:44.430: INFO: Created: latency-svc-bwvgw
    Aug 25 04:14:44.430: INFO: Created: latency-svc-d4d4c
    Aug 25 04:14:44.430: INFO: Created: latency-svc-xgzdd
    Aug 25 04:14:44.430: INFO: Created: latency-svc-lmq2t
    Aug 25 04:14:44.437: INFO: Created: latency-svc-w594c
    Aug 25 04:14:44.437: INFO: Created: latency-svc-95vtl
    Aug 25 04:14:44.437: INFO: Created: latency-svc-fgxjk
    Aug 25 04:14:44.437: INFO: Created: latency-svc-hlk5k
    Aug 25 04:14:44.437: INFO: Created: latency-svc-km82s
    Aug 25 04:14:44.437: INFO: Got endpoints: latency-svc-6g7bq [81.420117ms]
    Aug 25 04:14:44.437: INFO: Created: latency-svc-vdq4v
    Aug 25 04:14:44.437: INFO: Created: latency-svc-2lx9p
    Aug 25 04:14:44.439: INFO: Got endpoints: latency-svc-j75vx [83.821065ms]
    Aug 25 04:14:44.440: INFO: Got endpoints: latency-svc-wcxpz [83.781003ms]
    Aug 25 04:14:44.440: INFO: Got endpoints: latency-svc-86m8q [83.957272ms]
    Aug 25 04:14:44.440: INFO: Got endpoints: latency-svc-95vtl [84.110525ms]
    Aug 25 04:14:44.441: INFO: Got endpoints: latency-svc-lmq2t [85.017258ms]
    Aug 25 04:14:44.442: INFO: Got endpoints: latency-svc-km82s [86.056625ms]
    Aug 25 04:14:44.444: INFO: Got endpoints: latency-svc-bwvgw [74.522225ms]
    Aug 25 04:14:44.445: INFO: Created: latency-svc-xdbn5
    Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-vdq4v [89.349354ms]
    Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-xgzdd [89.49851ms]
    Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-d4d4c [89.7417ms]
    Aug 25 04:14:44.445: INFO: Got endpoints: latency-svc-hlk5k [90.005435ms]
    Aug 25 04:14:44.448: INFO: Got endpoints: latency-svc-2lx9p [92.637353ms]
    Aug 25 04:14:44.448: INFO: Created: latency-svc-qhjl5
    Aug 25 04:14:44.449: INFO: Got endpoints: latency-svc-fgxjk [92.823655ms]
    Aug 25 04:14:44.461: INFO: Got endpoints: latency-svc-xdbn5 [24.504951ms]
    Aug 25 04:14:44.461: INFO: Got endpoints: latency-svc-w594c [105.830128ms]
    Aug 25 04:14:44.461: INFO: Got endpoints: latency-svc-qhjl5 [21.970254ms]
    Aug 25 04:14:44.463: INFO: Created: latency-svc-6rngb
    Aug 25 04:14:44.466: INFO: Got endpoints: latency-svc-6rngb [26.610668ms]
    Aug 25 04:14:44.467: INFO: Created: latency-svc-kw28n
    Aug 25 04:14:44.470: INFO: Got endpoints: latency-svc-kw28n [30.406216ms]
    Aug 25 04:14:44.471: INFO: Created: latency-svc-qhr5r
    Aug 25 04:14:44.473: INFO: Got endpoints: latency-svc-qhr5r [32.574205ms]
    Aug 25 04:14:44.480: INFO: Created: latency-svc-rz7jn
    Aug 25 04:14:44.482: INFO: Got endpoints: latency-svc-rz7jn [41.924572ms]
    Aug 25 04:14:44.483: INFO: Created: latency-svc-g2w7c
    Aug 25 04:14:44.485: INFO: Got endpoints: latency-svc-g2w7c [43.857911ms]
    Aug 25 04:14:44.486: INFO: Created: latency-svc-57tvt
    Aug 25 04:14:44.489: INFO: Got endpoints: latency-svc-57tvt [44.663344ms]
    Aug 25 04:14:44.490: INFO: Created: latency-svc-kx27j
    Aug 25 04:14:44.492: INFO: Got endpoints: latency-svc-kx27j [47.470073ms]
    Aug 25 04:14:44.493: INFO: Created: latency-svc-gxdlh
    Aug 25 04:14:44.495: INFO: Got endpoints: latency-svc-gxdlh [50.115923ms]
    Aug 25 04:14:44.497: INFO: Created: latency-svc-qbhkc
    Aug 25 04:14:44.499: INFO: Got endpoints: latency-svc-qbhkc [53.609226ms]
    Aug 25 04:14:44.501: INFO: Created: latency-svc-6sh4w
    Aug 25 04:14:44.504: INFO: Got endpoints: latency-svc-6sh4w [58.670315ms]
    Aug 25 04:14:44.504: INFO: Created: latency-svc-s2sk9
    Aug 25 04:14:44.507: INFO: Got endpoints: latency-svc-s2sk9 [59.20421ms]
    Aug 25 04:14:44.509: INFO: Created: latency-svc-wz6ph
    Aug 25 04:14:44.514: INFO: Created: latency-svc-9sccz
    Aug 25 04:14:44.514: INFO: Got endpoints: latency-svc-wz6ph [65.142202ms]
    Aug 25 04:14:44.516: INFO: Created: latency-svc-z2l4j
    Aug 25 04:14:44.516: INFO: Got endpoints: latency-svc-9sccz [54.81337ms]
    Aug 25 04:14:44.519: INFO: Got endpoints: latency-svc-z2l4j [57.458075ms]
    Aug 25 04:14:44.519: INFO: Created: latency-svc-r64r7
    Aug 25 04:14:44.523: INFO: Created: latency-svc-zmz4d
    Aug 25 04:14:44.526: INFO: Created: latency-svc-4b56q
    Aug 25 04:14:44.530: INFO: Created: latency-svc-5lkpf
    Aug 25 04:14:44.534: INFO: Created: latency-svc-ztmkq
    Aug 25 04:14:44.538: INFO: Created: latency-svc-xgrbh
    Aug 25 04:14:44.543: INFO: Created: latency-svc-qpdk8
    Aug 25 04:14:44.548: INFO: Created: latency-svc-qngdp
    Aug 25 04:14:44.554: INFO: Created: latency-svc-pm9dd
    Aug 25 04:14:44.554: INFO: Got endpoints: latency-svc-r64r7 [92.696332ms]
    Aug 25 04:14:44.559: INFO: Created: latency-svc-scmt7
    Aug 25 04:14:44.605: INFO: Got endpoints: latency-svc-zmz4d [138.528663ms]
    Aug 25 04:14:44.605: INFO: Created: latency-svc-t8q4w
    Aug 25 04:14:44.605: INFO: Created: latency-svc-tfmlc
    Aug 25 04:14:44.605: INFO: Created: latency-svc-mnmwj
    Aug 25 04:14:44.606: INFO: Created: latency-svc-vc5sl
    Aug 25 04:14:44.606: INFO: Created: latency-svc-mkq45
    Aug 25 04:14:44.606: INFO: Created: latency-svc-8sc9d
    Aug 25 04:14:44.613: INFO: Created: latency-svc-szh9g
    Aug 25 04:14:44.655: INFO: Got endpoints: latency-svc-4b56q [184.490861ms]
    Aug 25 04:14:44.665: INFO: Created: latency-svc-qfc4g
    Aug 25 04:14:44.705: INFO: Got endpoints: latency-svc-5lkpf [232.078028ms]
    Aug 25 04:14:44.716: INFO: Created: latency-svc-ggl82
    Aug 25 04:14:44.754: INFO: Got endpoints: latency-svc-ztmkq [272.436437ms]
    Aug 25 04:14:44.765: INFO: Created: latency-svc-4lckq
    Aug 25 04:14:44.805: INFO: Got endpoints: latency-svc-xgrbh [319.492815ms]
    Aug 25 04:14:44.815: INFO: Created: latency-svc-rrlrd
    Aug 25 04:14:44.854: INFO: Got endpoints: latency-svc-qpdk8 [365.106723ms]
    Aug 25 04:14:44.865: INFO: Created: latency-svc-b2ffj
    Aug 25 04:14:44.906: INFO: Got endpoints: latency-svc-qngdp [413.17546ms]
    Aug 25 04:14:44.916: INFO: Created: latency-svc-sc47n
    Aug 25 04:14:44.954: INFO: Got endpoints: latency-svc-pm9dd [458.431262ms]
    Aug 25 04:14:44.964: INFO: Created: latency-svc-xk6rw
    Aug 25 04:14:45.005: INFO: Got endpoints: latency-svc-scmt7 [505.485462ms]
    Aug 25 04:14:45.029: INFO: Created: latency-svc-f9mdb
    Aug 25 04:14:45.054: INFO: Got endpoints: latency-svc-t8q4w [547.114062ms]
    Aug 25 04:14:45.062: INFO: Created: latency-svc-zxncm
    Aug 25 04:14:45.106: INFO: Got endpoints: latency-svc-tfmlc [589.921259ms]
    Aug 25 04:14:45.116: INFO: Created: latency-svc-9wjxn
    Aug 25 04:14:45.154: INFO: Got endpoints: latency-svc-mnmwj [650.263485ms]
    Aug 25 04:14:45.165: INFO: Created: latency-svc-dgr5t
    Aug 25 04:14:45.205: INFO: Got endpoints: latency-svc-mkq45 [686.439105ms]
    Aug 25 04:14:45.216: INFO: Created: latency-svc-8dwwg
    Aug 25 04:14:45.255: INFO: Got endpoints: latency-svc-8sc9d [741.000379ms]
    Aug 25 04:14:45.265: INFO: Created: latency-svc-sw9pt
    Aug 25 04:14:45.304: INFO: Got endpoints: latency-svc-vc5sl [750.065196ms]
    Aug 25 04:14:45.315: INFO: Created: latency-svc-7wdbc
    Aug 25 04:14:45.355: INFO: Got endpoints: latency-svc-szh9g [750.188651ms]
    Aug 25 04:14:45.366: INFO: Created: latency-svc-sd7x5
    Aug 25 04:14:45.405: INFO: Got endpoints: latency-svc-qfc4g [750.154371ms]
    Aug 25 04:14:45.419: INFO: Created: latency-svc-5fdnj
    Aug 25 04:14:45.455: INFO: Got endpoints: latency-svc-ggl82 [749.076479ms]
    Aug 25 04:14:45.463: INFO: Created: latency-svc-mf2f2
    Aug 25 04:14:45.505: INFO: Got endpoints: latency-svc-4lckq [751.398218ms]
    Aug 25 04:14:45.515: INFO: Created: latency-svc-t9zft
    Aug 25 04:14:45.556: INFO: Got endpoints: latency-svc-rrlrd [751.045837ms]
    Aug 25 04:14:45.566: INFO: Created: latency-svc-lq9p2
    Aug 25 04:14:45.605: INFO: Got endpoints: latency-svc-b2ffj [751.041477ms]
    Aug 25 04:14:45.615: INFO: Created: latency-svc-kf2m2
    Aug 25 04:14:45.656: INFO: Got endpoints: latency-svc-sc47n [750.293433ms]
    Aug 25 04:14:45.666: INFO: Created: latency-svc-xb7br
    Aug 25 04:14:45.705: INFO: Got endpoints: latency-svc-xk6rw [750.786751ms]
    Aug 25 04:14:45.714: INFO: Created: latency-svc-cr6cn
    Aug 25 04:14:45.755: INFO: Got endpoints: latency-svc-f9mdb [750.433278ms]
    Aug 25 04:14:45.765: INFO: Created: latency-svc-hm2qr
    Aug 25 04:14:45.805: INFO: Got endpoints: latency-svc-zxncm [750.22497ms]
    Aug 25 04:14:45.814: INFO: Created: latency-svc-wh44c
    Aug 25 04:14:45.854: INFO: Got endpoints: latency-svc-9wjxn [748.226567ms]
    Aug 25 04:14:45.863: INFO: Created: latency-svc-ncjkm
    Aug 25 04:14:45.905: INFO: Got endpoints: latency-svc-dgr5t [750.994536ms]
    Aug 25 04:14:45.915: INFO: Created: latency-svc-7sztm
    Aug 25 04:14:45.954: INFO: Got endpoints: latency-svc-8dwwg [748.322667ms]
    Aug 25 04:14:45.964: INFO: Created: latency-svc-52v27
    Aug 25 04:14:46.005: INFO: Got endpoints: latency-svc-sw9pt [749.878973ms]
    Aug 25 04:14:46.014: INFO: Created: latency-svc-jpbqw
    Aug 25 04:14:46.055: INFO: Got endpoints: latency-svc-7wdbc [751.026286ms]
    Aug 25 04:14:46.065: INFO: Created: latency-svc-j856s
    Aug 25 04:14:46.105: INFO: Got endpoints: latency-svc-sd7x5 [750.102634ms]
    Aug 25 04:14:46.114: INFO: Created: latency-svc-q9zcd
    Aug 25 04:14:46.155: INFO: Got endpoints: latency-svc-5fdnj [750.181138ms]
    Aug 25 04:14:46.165: INFO: Created: latency-svc-hdj9c
    Aug 25 04:14:46.203: INFO: Got endpoints: latency-svc-mf2f2 [748.909491ms]
    Aug 25 04:14:46.213: INFO: Created: latency-svc-tnhg5
    Aug 25 04:14:46.255: INFO: Got endpoints: latency-svc-t9zft [749.738965ms]
    Aug 25 04:14:46.265: INFO: Created: latency-svc-5pf5m
    Aug 25 04:14:46.305: INFO: Got endpoints: latency-svc-lq9p2 [749.219529ms]
    Aug 25 04:14:46.315: INFO: Created: latency-svc-scdt9
    Aug 25 04:14:46.355: INFO: Got endpoints: latency-svc-kf2m2 [750.029768ms]
    Aug 25 04:14:46.365: INFO: Created: latency-svc-7rfwx
    Aug 25 04:14:46.405: INFO: Got endpoints: latency-svc-xb7br [749.155711ms]
    Aug 25 04:14:46.416: INFO: Created: latency-svc-mqczt
    Aug 25 04:14:46.455: INFO: Got endpoints: latency-svc-cr6cn [750.842163ms]
    Aug 25 04:14:46.466: INFO: Created: latency-svc-qlf95
    Aug 25 04:14:46.505: INFO: Got endpoints: latency-svc-hm2qr [749.40983ms]
    Aug 25 04:14:46.519: INFO: Created: latency-svc-jcdhn
    Aug 25 04:14:46.555: INFO: Got endpoints: latency-svc-wh44c [749.85553ms]
    Aug 25 04:14:46.564: INFO: Created: latency-svc-6txfk
    Aug 25 04:14:46.605: INFO: Got endpoints: latency-svc-ncjkm [750.349057ms]
    Aug 25 04:14:46.614: INFO: Created: latency-svc-zhtqm
    Aug 25 04:14:46.657: INFO: Got endpoints: latency-svc-7sztm [751.190035ms]
    Aug 25 04:14:46.669: INFO: Created: latency-svc-2l44q
    Aug 25 04:14:46.705: INFO: Got endpoints: latency-svc-52v27 [751.196938ms]
    Aug 25 04:14:46.714: INFO: Created: latency-svc-zsq5k
    Aug 25 04:14:46.755: INFO: Got endpoints: latency-svc-jpbqw [750.156695ms]
    Aug 25 04:14:46.765: INFO: Created: latency-svc-p2zsv
    Aug 25 04:14:46.806: INFO: Got endpoints: latency-svc-j856s [750.192549ms]
    Aug 25 04:14:46.816: INFO: Created: latency-svc-dtxvx
    Aug 25 04:14:46.855: INFO: Got endpoints: latency-svc-q9zcd [750.132806ms]
    Aug 25 04:14:46.868: INFO: Created: latency-svc-8glnh
    Aug 25 04:14:46.905: INFO: Got endpoints: latency-svc-hdj9c [749.96544ms]
    Aug 25 04:14:46.915: INFO: Created: latency-svc-gvmlf
    Aug 25 04:14:46.955: INFO: Got endpoints: latency-svc-tnhg5 [751.100161ms]
    Aug 25 04:14:46.967: INFO: Created: latency-svc-969zs
    Aug 25 04:14:47.005: INFO: Got endpoints: latency-svc-5pf5m [749.562923ms]
    Aug 25 04:14:47.014: INFO: Created: latency-svc-fzq48
    Aug 25 04:14:47.056: INFO: Got endpoints: latency-svc-scdt9 [750.16338ms]
    Aug 25 04:14:47.065: INFO: Created: latency-svc-cfsrd
    Aug 25 04:14:47.120: INFO: Got endpoints: latency-svc-7rfwx [765.008666ms]
    Aug 25 04:14:47.131: INFO: Created: latency-svc-rhx6z
    Aug 25 04:14:47.154: INFO: Got endpoints: latency-svc-mqczt [748.784633ms]
    Aug 25 04:14:47.162: INFO: Created: latency-svc-4fd9l
    Aug 25 04:14:47.202: INFO: Got endpoints: latency-svc-qlf95 [746.916914ms]
    Aug 25 04:14:47.210: INFO: Created: latency-svc-7r89w
    Aug 25 04:14:47.254: INFO: Got endpoints: latency-svc-jcdhn [749.00686ms]
    Aug 25 04:14:47.261: INFO: Created: latency-svc-scv4d
    Aug 25 04:14:47.304: INFO: Got endpoints: latency-svc-6txfk [749.271583ms]
    Aug 25 04:14:47.312: INFO: Created: latency-svc-z4zdt
    Aug 25 04:14:47.353: INFO: Got endpoints: latency-svc-zhtqm [747.667022ms]
    Aug 25 04:14:47.360: INFO: Created: latency-svc-5xsxr
    Aug 25 04:14:47.403: INFO: Got endpoints: latency-svc-2l44q [746.623219ms]
    Aug 25 04:14:47.411: INFO: Created: latency-svc-2v57r
    Aug 25 04:14:47.453: INFO: Got endpoints: latency-svc-zsq5k [748.391326ms]
    Aug 25 04:14:47.461: INFO: Created: latency-svc-9tx4h
    Aug 25 04:14:47.503: INFO: Got endpoints: latency-svc-p2zsv [747.7143ms]
    Aug 25 04:14:47.512: INFO: Created: latency-svc-9strl
    Aug 25 04:14:47.554: INFO: Got endpoints: latency-svc-dtxvx [747.991603ms]
    Aug 25 04:14:47.563: INFO: Created: latency-svc-ccrnh
    Aug 25 04:14:47.604: INFO: Got endpoints: latency-svc-8glnh [748.765769ms]
    Aug 25 04:14:47.614: INFO: Created: latency-svc-76zzc
    Aug 25 04:14:47.655: INFO: Got endpoints: latency-svc-gvmlf [749.88464ms]
    Aug 25 04:14:47.664: INFO: Created: latency-svc-nkkzj
    Aug 25 04:14:47.705: INFO: Got endpoints: latency-svc-969zs [750.099189ms]
    Aug 25 04:14:47.714: INFO: Created: latency-svc-tckkn
    Aug 25 04:14:47.755: INFO: Got endpoints: latency-svc-fzq48 [750.389065ms]
    Aug 25 04:14:47.765: INFO: Created: latency-svc-5zskq
    Aug 25 04:14:47.805: INFO: Got endpoints: latency-svc-cfsrd [749.647177ms]
    Aug 25 04:14:47.815: INFO: Created: latency-svc-mv7kx
    Aug 25 04:14:47.854: INFO: Got endpoints: latency-svc-rhx6z [733.637624ms]
    Aug 25 04:14:47.864: INFO: Created: latency-svc-mq4hr
    Aug 25 04:14:47.905: INFO: Got endpoints: latency-svc-4fd9l [751.004661ms]
    Aug 25 04:14:47.915: INFO: Created: latency-svc-c7ptt
    Aug 25 04:14:47.955: INFO: Got endpoints: latency-svc-7r89w [752.821562ms]
    Aug 25 04:14:47.965: INFO: Created: latency-svc-4l58l
    Aug 25 04:14:48.005: INFO: Got endpoints: latency-svc-scv4d [751.339618ms]
    Aug 25 04:14:48.014: INFO: Created: latency-svc-zx2bs
    Aug 25 04:14:48.055: INFO: Got endpoints: latency-svc-z4zdt [750.876949ms]
    Aug 25 04:14:48.065: INFO: Created: latency-svc-fcjwx
    Aug 25 04:14:48.106: INFO: Got endpoints: latency-svc-5xsxr [753.141634ms]
    Aug 25 04:14:48.116: INFO: Created: latency-svc-87wtg
    Aug 25 04:14:48.154: INFO: Got endpoints: latency-svc-2v57r [751.184455ms]
    Aug 25 04:14:48.164: INFO: Created: latency-svc-g2g62
    Aug 25 04:14:48.205: INFO: Got endpoints: latency-svc-9tx4h [751.186359ms]
    Aug 25 04:14:48.215: INFO: Created: latency-svc-hkxwr
    Aug 25 04:14:48.255: INFO: Got endpoints: latency-svc-9strl [751.972388ms]
    Aug 25 04:14:48.265: INFO: Created: latency-svc-gghkz
    Aug 25 04:14:48.304: INFO: Got endpoints: latency-svc-ccrnh [750.684116ms]
    Aug 25 04:14:48.314: INFO: Created: latency-svc-6b5kj
    Aug 25 04:14:48.354: INFO: Got endpoints: latency-svc-76zzc [750.14691ms]
    Aug 25 04:14:48.365: INFO: Created: latency-svc-7kw28
    Aug 25 04:14:48.405: INFO: Got endpoints: latency-svc-nkkzj [749.839821ms]
    Aug 25 04:14:48.412: INFO: Created: latency-svc-bzzjc
    Aug 25 04:14:48.454: INFO: Got endpoints: latency-svc-tckkn [748.768393ms]
    Aug 25 04:14:48.462: INFO: Created: latency-svc-5g7w4
    Aug 25 04:14:48.507: INFO: Got endpoints: latency-svc-5zskq [751.476633ms]
    Aug 25 04:14:48.519: INFO: Created: latency-svc-bthxn
    Aug 25 04:14:48.554: INFO: Got endpoints: latency-svc-mv7kx [748.519861ms]
    Aug 25 04:14:48.562: INFO: Created: latency-svc-8t86n
    Aug 25 04:14:48.605: INFO: Got endpoints: latency-svc-mq4hr [750.987214ms]
    Aug 25 04:14:48.617: INFO: Created: latency-svc-9r4wm
    Aug 25 04:14:48.655: INFO: Got endpoints: latency-svc-c7ptt [749.689329ms]
    Aug 25 04:14:48.666: INFO: Created: latency-svc-g9r6g
    Aug 25 04:14:48.705: INFO: Got endpoints: latency-svc-4l58l [749.856602ms]
    Aug 25 04:14:48.717: INFO: Created: latency-svc-tvlp2
    Aug 25 04:14:48.755: INFO: Got endpoints: latency-svc-zx2bs [749.938706ms]
    Aug 25 04:14:48.766: INFO: Created: latency-svc-d2twl
    Aug 25 04:14:48.806: INFO: Got endpoints: latency-svc-fcjwx [750.646091ms]
    Aug 25 04:14:48.816: INFO: Created: latency-svc-66dsd
    Aug 25 04:14:48.855: INFO: Got endpoints: latency-svc-87wtg [748.789199ms]
    Aug 25 04:14:48.864: INFO: Created: latency-svc-ttpcq
    Aug 25 04:14:48.905: INFO: Got endpoints: latency-svc-g2g62 [750.436492ms]
    Aug 25 04:14:48.916: INFO: Created: latency-svc-gv5rs
    Aug 25 04:14:48.954: INFO: Got endpoints: latency-svc-hkxwr [749.647466ms]
    Aug 25 04:14:48.965: INFO: Created: latency-svc-r2kq9
    Aug 25 04:14:49.005: INFO: Got endpoints: latency-svc-gghkz [750.011352ms]
    Aug 25 04:14:49.015: INFO: Created: latency-svc-648m4
    Aug 25 04:14:49.055: INFO: Got endpoints: latency-svc-6b5kj [750.67089ms]
    Aug 25 04:14:49.066: INFO: Created: latency-svc-w55f5
    Aug 25 04:14:49.105: INFO: Got endpoints: latency-svc-7kw28 [750.679909ms]
    Aug 25 04:14:49.116: INFO: Created: latency-svc-q9zhw
    Aug 25 04:14:49.155: INFO: Got endpoints: latency-svc-bzzjc [750.561222ms]
    Aug 25 04:14:49.165: INFO: Created: latency-svc-j68qb
    Aug 25 04:14:49.205: INFO: Got endpoints: latency-svc-5g7w4 [751.012307ms]
    Aug 25 04:14:49.229: INFO: Created: latency-svc-rvpft
    Aug 25 04:14:49.254: INFO: Got endpoints: latency-svc-bthxn [747.412349ms]
    Aug 25 04:14:49.261: INFO: Created: latency-svc-fpzrf
    Aug 25 04:14:49.305: INFO: Got endpoints: latency-svc-8t86n [751.025838ms]
    Aug 25 04:14:49.314: INFO: Created: latency-svc-n5x6w
    Aug 25 04:14:49.354: INFO: Got endpoints: latency-svc-9r4wm [748.447468ms]
    Aug 25 04:14:49.363: INFO: Created: latency-svc-ts2jl
    Aug 25 04:14:49.404: INFO: Got endpoints: latency-svc-g9r6g [749.557987ms]
    Aug 25 04:14:49.415: INFO: Created: latency-svc-7rsfp
    Aug 25 04:14:49.456: INFO: Got endpoints: latency-svc-tvlp2 [750.549583ms]
    Aug 25 04:14:49.466: INFO: Created: latency-svc-qsmvd
    Aug 25 04:14:49.505: INFO: Got endpoints: latency-svc-d2twl [749.827589ms]
    Aug 25 04:14:49.514: INFO: Created: latency-svc-nvrvv
    Aug 25 04:14:49.556: INFO: Got endpoints: latency-svc-66dsd [750.581748ms]
    Aug 25 04:14:49.567: INFO: Created: latency-svc-m27ck
    Aug 25 04:14:49.605: INFO: Got endpoints: latency-svc-ttpcq [750.834156ms]
    Aug 25 04:14:49.616: INFO: Created: latency-svc-trhj7
    Aug 25 04:14:49.655: INFO: Got endpoints: latency-svc-gv5rs [749.867941ms]
    Aug 25 04:14:49.665: INFO: Created: latency-svc-gwvbp
    Aug 25 04:14:49.704: INFO: Got endpoints: latency-svc-r2kq9 [749.973691ms]
    Aug 25 04:14:49.715: INFO: Created: latency-svc-xj8fj
    Aug 25 04:14:49.755: INFO: Got endpoints: latency-svc-648m4 [750.594673ms]
    Aug 25 04:14:49.769: INFO: Created: latency-svc-5trzm
    Aug 25 04:14:49.804: INFO: Got endpoints: latency-svc-w55f5 [748.993916ms]
    Aug 25 04:14:49.817: INFO: Created: latency-svc-p2gr2
    Aug 25 04:14:49.855: INFO: Got endpoints: latency-svc-q9zhw [749.901966ms]
    Aug 25 04:14:49.863: INFO: Created: latency-svc-qnkm6
    Aug 25 04:14:49.905: INFO: Got endpoints: latency-svc-j68qb [749.507653ms]
    Aug 25 04:14:49.914: INFO: Created: latency-svc-vx47d
    Aug 25 04:14:49.955: INFO: Got endpoints: latency-svc-rvpft [749.980168ms]
    Aug 25 04:14:49.967: INFO: Created: latency-svc-5gp95
    Aug 25 04:14:50.004: INFO: Got endpoints: latency-svc-fpzrf [749.351895ms]
    Aug 25 04:14:50.013: INFO: Created: latency-svc-cbdfq
    Aug 25 04:14:50.055: INFO: Got endpoints: latency-svc-n5x6w [750.018488ms]
    Aug 25 04:14:50.067: INFO: Created: latency-svc-c8snr
    Aug 25 04:14:50.106: INFO: Got endpoints: latency-svc-ts2jl [752.461072ms]
    Aug 25 04:14:50.115: INFO: Created: latency-svc-xgst7
    Aug 25 04:14:50.156: INFO: Got endpoints: latency-svc-7rsfp [751.093488ms]
    Aug 25 04:14:50.165: INFO: Created: latency-svc-56qfq
    Aug 25 04:14:50.205: INFO: Got endpoints: latency-svc-qsmvd [748.99501ms]
    Aug 25 04:14:50.215: INFO: Created: latency-svc-v88ms
    Aug 25 04:14:50.255: INFO: Got endpoints: latency-svc-nvrvv [749.7141ms]
    Aug 25 04:14:50.264: INFO: Created: latency-svc-6p7dv
    Aug 25 04:14:50.305: INFO: Got endpoints: latency-svc-m27ck [748.44099ms]
    Aug 25 04:14:50.314: INFO: Created: latency-svc-snd8v
    Aug 25 04:14:50.356: INFO: Got endpoints: latency-svc-trhj7 [750.731099ms]
    Aug 25 04:14:50.366: INFO: Created: latency-svc-qkndv
    Aug 25 04:14:50.405: INFO: Got endpoints: latency-svc-gwvbp [750.134504ms]
    Aug 25 04:14:50.415: INFO: Created: latency-svc-xs46d
    Aug 25 04:14:50.455: INFO: Got endpoints: latency-svc-xj8fj [750.571954ms]
    Aug 25 04:14:50.465: INFO: Created: latency-svc-gjwbm
    Aug 25 04:14:50.505: INFO: Got endpoints: latency-svc-5trzm [749.459855ms]
    Aug 25 04:14:50.515: INFO: Created: latency-svc-qncbz
    Aug 25 04:14:50.555: INFO: Got endpoints: latency-svc-p2gr2 [751.038219ms]
    Aug 25 04:14:50.565: INFO: Created: latency-svc-tbwcw
    Aug 25 04:14:50.605: INFO: Got endpoints: latency-svc-qnkm6 [749.767797ms]
    Aug 25 04:14:50.615: INFO: Created: latency-svc-b6bdv
    Aug 25 04:14:50.656: INFO: Got endpoints: latency-svc-vx47d [751.323673ms]
    Aug 25 04:14:50.666: INFO: Created: latency-svc-78fqp
    Aug 25 04:14:50.704: INFO: Got endpoints: latency-svc-5gp95 [749.172687ms]
    Aug 25 04:14:50.714: INFO: Created: latency-svc-c2pj7
    Aug 25 04:14:50.754: INFO: Got endpoints: latency-svc-cbdfq [750.452098ms]
    Aug 25 04:14:50.764: INFO: Created: latency-svc-c9m7d
    Aug 25 04:14:50.805: INFO: Got endpoints: latency-svc-c8snr [749.900252ms]
    Aug 25 04:14:50.815: INFO: Created: latency-svc-69496
    Aug 25 04:14:50.856: INFO: Got endpoints: latency-svc-xgst7 [749.637661ms]
    Aug 25 04:14:50.866: INFO: Created: latency-svc-v865f
    Aug 25 04:14:50.904: INFO: Got endpoints: latency-svc-56qfq [748.829562ms]
    Aug 25 04:14:50.915: INFO: Created: latency-svc-qb8w6
    Aug 25 04:14:50.953: INFO: Got endpoints: latency-svc-v88ms [748.306176ms]
    Aug 25 04:14:50.963: INFO: Created: latency-svc-s6zlt
    Aug 25 04:14:51.004: INFO: Got endpoints: latency-svc-6p7dv [749.405588ms]
    Aug 25 04:14:51.014: INFO: Created: latency-svc-9xth5
    Aug 25 04:14:51.055: INFO: Got endpoints: latency-svc-snd8v [749.82416ms]
    Aug 25 04:14:51.070: INFO: Created: latency-svc-s884r
    Aug 25 04:14:51.105: INFO: Got endpoints: latency-svc-qkndv [748.554029ms]
    Aug 25 04:14:51.115: INFO: Created: latency-svc-s5cjr
    Aug 25 04:14:51.155: INFO: Got endpoints: latency-svc-xs46d [749.587576ms]
    Aug 25 04:14:51.164: INFO: Created: latency-svc-8r8kq
    Aug 25 04:14:51.205: INFO: Got endpoints: latency-svc-gjwbm [749.562438ms]
    Aug 25 04:14:51.214: INFO: Created: latency-svc-dmf5b
    Aug 25 04:14:51.254: INFO: Got endpoints: latency-svc-qncbz [749.413779ms]
    Aug 25 04:14:51.263: INFO: Created: latency-svc-gcxzk
    Aug 25 04:14:51.305: INFO: Got endpoints: latency-svc-tbwcw [749.480951ms]
    Aug 25 04:14:51.315: INFO: Created: latency-svc-rxcpg
    Aug 25 04:14:51.354: INFO: Got endpoints: latency-svc-b6bdv [749.144466ms]
    Aug 25 04:14:51.363: INFO: Created: latency-svc-hk5g5
    Aug 25 04:14:51.406: INFO: Got endpoints: latency-svc-78fqp [749.420029ms]
    Aug 25 04:14:51.416: INFO: Created: latency-svc-j54hw
    Aug 25 04:14:51.455: INFO: Got endpoints: latency-svc-c2pj7 [750.725031ms]
    Aug 25 04:14:51.464: INFO: Created: latency-svc-q9tnh
    Aug 25 04:14:51.509: INFO: Got endpoints: latency-svc-c9m7d [754.527777ms]
    Aug 25 04:14:51.519: INFO: Created: latency-svc-jv8mp
    Aug 25 04:14:51.555: INFO: Got endpoints: latency-svc-69496 [749.668103ms]
    Aug 25 04:14:51.565: INFO: Created: latency-svc-tcgfd
    Aug 25 04:14:51.605: INFO: Got endpoints: latency-svc-v865f [749.418671ms]
    Aug 25 04:14:51.618: INFO: Created: latency-svc-6dt6k
    Aug 25 04:14:51.655: INFO: Got endpoints: latency-svc-qb8w6 [750.494044ms]
    Aug 25 04:14:51.665: INFO: Created: latency-svc-vprw9
    Aug 25 04:14:51.704: INFO: Got endpoints: latency-svc-s6zlt [751.080024ms]
    Aug 25 04:14:51.716: INFO: Created: latency-svc-98hsn
    Aug 25 04:14:51.755: INFO: Got endpoints: latency-svc-9xth5 [751.153167ms]
    Aug 25 04:14:51.765: INFO: Created: latency-svc-ggswq
    Aug 25 04:14:51.805: INFO: Got endpoints: latency-svc-s884r [750.736672ms]
    Aug 25 04:14:51.815: INFO: Created: latency-svc-mrmws
    Aug 25 04:14:51.855: INFO: Got endpoints: latency-svc-s5cjr [750.254123ms]
    Aug 25 04:14:51.865: INFO: Created: latency-svc-qgl5x
    Aug 25 04:14:51.904: INFO: Got endpoints: latency-svc-8r8kq [749.302537ms]
    Aug 25 04:14:51.914: INFO: Created: latency-svc-8xmvn
    Aug 25 04:14:51.953: INFO: Got endpoints: latency-svc-dmf5b [748.322764ms]
    Aug 25 04:14:51.963: INFO: Created: latency-svc-c5tqj
    Aug 25 04:14:52.005: INFO: Got endpoints: latency-svc-gcxzk [750.439006ms]
    Aug 25 04:14:52.015: INFO: Created: latency-svc-szp5v
    Aug 25 04:14:52.056: INFO: Got endpoints: latency-svc-rxcpg [751.08345ms]
    Aug 25 04:14:52.067: INFO: Created: latency-svc-mhklt
    Aug 25 04:14:52.104: INFO: Got endpoints: latency-svc-hk5g5 [749.690902ms]
    Aug 25 04:14:52.114: INFO: Created: latency-svc-n95pr
    Aug 25 04:14:52.155: INFO: Got endpoints: latency-svc-j54hw [748.996121ms]
    Aug 25 04:14:52.164: INFO: Created: latency-svc-2ktj2
    Aug 25 04:14:52.204: INFO: Got endpoints: latency-svc-q9tnh [749.621847ms]
    Aug 25 04:14:52.254: INFO: Got endpoints: latency-svc-jv8mp [745.075274ms]
    Aug 25 04:14:52.304: INFO: Got endpoints: latency-svc-tcgfd [749.051418ms]
    Aug 25 04:14:52.354: INFO: Got endpoints: latency-svc-6dt6k [748.898722ms]
    Aug 25 04:14:52.404: INFO: Got endpoints: latency-svc-vprw9 [748.505484ms]
    Aug 25 04:14:52.454: INFO: Got endpoints: latency-svc-98hsn [750.008172ms]
    Aug 25 04:14:52.504: INFO: Got endpoints: latency-svc-ggswq [749.128129ms]
    Aug 25 04:14:52.556: INFO: Got endpoints: latency-svc-mrmws [750.182721ms]
    Aug 25 04:14:52.605: INFO: Got endpoints: latency-svc-qgl5x [749.958614ms]
    Aug 25 04:14:52.655: INFO: Got endpoints: latency-svc-8xmvn [750.984023ms]
    Aug 25 04:14:52.705: INFO: Got endpoints: latency-svc-c5tqj [752.388716ms]
    Aug 25 04:14:52.755: INFO: Got endpoints: latency-svc-szp5v [750.096975ms]
    Aug 25 04:14:52.805: INFO: Got endpoints: latency-svc-mhklt [748.907028ms]
    Aug 25 04:14:52.855: INFO: Got endpoints: latency-svc-n95pr [751.392937ms]
    Aug 25 04:14:52.905: INFO: Got endpoints: latency-svc-2ktj2 [749.749844ms]
    Aug 25 04:14:52.905: INFO: Latencies: [14.406832ms 21.970254ms 24.504951ms 26.610668ms 30.406216ms 32.574205ms 41.924572ms 43.857911ms 44.663344ms 47.470073ms 50.115923ms 53.609226ms 54.81337ms 57.458075ms 58.670315ms 59.20421ms 65.142202ms 74.522225ms 81.420117ms 83.781003ms 83.821065ms 83.957272ms 84.110525ms 85.017258ms 86.056625ms 89.349354ms 89.49851ms 89.7417ms 90.005435ms 92.637353ms 92.696332ms 92.823655ms 105.830128ms 138.528663ms 184.490861ms 232.078028ms 272.436437ms 319.492815ms 365.106723ms 413.17546ms 458.431262ms 505.485462ms 547.114062ms 589.921259ms 650.263485ms 686.439105ms 733.637624ms 741.000379ms 745.075274ms 746.623219ms 746.916914ms 747.412349ms 747.667022ms 747.7143ms 747.991603ms 748.226567ms 748.306176ms 748.322667ms 748.322764ms 748.391326ms 748.44099ms 748.447468ms 748.505484ms 748.519861ms 748.554029ms 748.765769ms 748.768393ms 748.784633ms 748.789199ms 748.829562ms 748.898722ms 748.907028ms 748.909491ms 748.993916ms 748.99501ms 748.996121ms 749.00686ms 749.051418ms 749.076479ms 749.128129ms 749.144466ms 749.155711ms 749.172687ms 749.219529ms 749.271583ms 749.302537ms 749.351895ms 749.405588ms 749.40983ms 749.413779ms 749.418671ms 749.420029ms 749.459855ms 749.480951ms 749.507653ms 749.557987ms 749.562438ms 749.562923ms 749.587576ms 749.621847ms 749.637661ms 749.647177ms 749.647466ms 749.668103ms 749.689329ms 749.690902ms 749.7141ms 749.738965ms 749.749844ms 749.767797ms 749.82416ms 749.827589ms 749.839821ms 749.85553ms 749.856602ms 749.867941ms 749.878973ms 749.88464ms 749.900252ms 749.901966ms 749.938706ms 749.958614ms 749.96544ms 749.973691ms 749.980168ms 750.008172ms 750.011352ms 750.018488ms 750.029768ms 750.065196ms 750.096975ms 750.099189ms 750.102634ms 750.132806ms 750.134504ms 750.14691ms 750.154371ms 750.156695ms 750.16338ms 750.181138ms 750.182721ms 750.188651ms 750.192549ms 750.22497ms 750.254123ms 750.293433ms 750.349057ms 750.389065ms 750.433278ms 750.436492ms 750.439006ms 750.452098ms 750.494044ms 750.549583ms 750.561222ms 750.571954ms 750.581748ms 750.594673ms 750.646091ms 750.67089ms 750.679909ms 750.684116ms 750.725031ms 750.731099ms 750.736672ms 750.786751ms 750.834156ms 750.842163ms 750.876949ms 750.984023ms 750.987214ms 750.994536ms 751.004661ms 751.012307ms 751.025838ms 751.026286ms 751.038219ms 751.041477ms 751.045837ms 751.080024ms 751.08345ms 751.093488ms 751.100161ms 751.153167ms 751.184455ms 751.186359ms 751.190035ms 751.196938ms 751.323673ms 751.339618ms 751.392937ms 751.398218ms 751.476633ms 751.972388ms 752.388716ms 752.461072ms 752.821562ms 753.141634ms 754.527777ms 765.008666ms]
    Aug 25 04:14:52.905: INFO: 50 %ile: 749.637661ms
    Aug 25 04:14:52.905: INFO: 90 %ile: 751.08345ms
    Aug 25 04:14:52.905: INFO: 99 %ile: 754.527777ms
    Aug 25 04:14:52.905: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Aug 25 04:14:52.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-4396" for this suite. 08/25/22 04:14:52.911
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:14:52.919
Aug 25 04:14:52.919: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:14:52.92
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:52.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:52.936
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-86877236-5233-4b54-8d4d-e254f4251f68 08/25/22 04:14:52.942
STEP: Creating secret with name s-test-opt-upd-5b4ddb63-9666-4522-a2ef-dd2890239193 08/25/22 04:14:52.945
STEP: Creating the pod 08/25/22 04:14:52.949
Aug 25 04:14:52.957: INFO: Waiting up to 5m0s for pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e" in namespace "secrets-2433" to be "running and ready"
Aug 25 04:14:52.960: INFO: Pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324222ms
Aug 25 04:14:52.960: INFO: The phase of Pod pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:14:54.965: INFO: Pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008328142s
Aug 25 04:14:54.965: INFO: The phase of Pod pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e is Running (Ready = true)
Aug 25 04:14:54.965: INFO: Pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-86877236-5233-4b54-8d4d-e254f4251f68 08/25/22 04:14:54.985
STEP: Updating secret s-test-opt-upd-5b4ddb63-9666-4522-a2ef-dd2890239193 08/25/22 04:14:54.99
STEP: Creating secret with name s-test-opt-create-6b7cbf7e-9056-4afa-a40e-bc6f31f60ed2 08/25/22 04:14:54.995
STEP: waiting to observe update in volume 08/25/22 04:14:54.999
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:14:59.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2433" for this suite. 08/25/22 04:14:59.033
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":294,"skipped":5444,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:14:52.919
    Aug 25 04:14:52.919: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:14:52.92
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:52.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:52.936
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-86877236-5233-4b54-8d4d-e254f4251f68 08/25/22 04:14:52.942
    STEP: Creating secret with name s-test-opt-upd-5b4ddb63-9666-4522-a2ef-dd2890239193 08/25/22 04:14:52.945
    STEP: Creating the pod 08/25/22 04:14:52.949
    Aug 25 04:14:52.957: INFO: Waiting up to 5m0s for pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e" in namespace "secrets-2433" to be "running and ready"
    Aug 25 04:14:52.960: INFO: Pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324222ms
    Aug 25 04:14:52.960: INFO: The phase of Pod pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:14:54.965: INFO: Pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008328142s
    Aug 25 04:14:54.965: INFO: The phase of Pod pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e is Running (Ready = true)
    Aug 25 04:14:54.965: INFO: Pod "pod-secrets-a362b0d0-5d31-4d10-a12f-2b6392cfb48e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-86877236-5233-4b54-8d4d-e254f4251f68 08/25/22 04:14:54.985
    STEP: Updating secret s-test-opt-upd-5b4ddb63-9666-4522-a2ef-dd2890239193 08/25/22 04:14:54.99
    STEP: Creating secret with name s-test-opt-create-6b7cbf7e-9056-4afa-a40e-bc6f31f60ed2 08/25/22 04:14:54.995
    STEP: waiting to observe update in volume 08/25/22 04:14:54.999
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:14:59.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2433" for this suite. 08/25/22 04:14:59.033
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:14:59.04
Aug 25 04:14:59.040: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 04:14:59.042
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:59.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:59.058
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 04:14:59.077
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 04:14:59.417
STEP: Deploying the webhook pod 08/25/22 04:14:59.423
STEP: Wait for the deployment to be ready 08/25/22 04:14:59.436
Aug 25 04:14:59.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 25 04:15:01.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/25/22 04:15:03.467
STEP: Verifying the service has paired with the endpoint 08/25/22 04:15:03.477
Aug 25 04:15:04.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 25 04:15:05.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 25 04:15:06.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 25 04:15:07.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 25 04:15:08.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 25 04:15:09.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 25 04:15:10.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Aug 25 04:15:10.482: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8859-crds.webhook.example.com via the AdmissionRegistration API 08/25/22 04:15:10.998
STEP: Creating a custom resource that should be mutated by the webhook 08/25/22 04:15:11.015
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:15:13.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8713" for this suite. 08/25/22 04:15:13.61
STEP: Destroying namespace "webhook-8713-markers" for this suite. 08/25/22 04:15:13.615
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":295,"skipped":5468,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [14.603 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:14:59.04
    Aug 25 04:14:59.040: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 04:14:59.042
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:14:59.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:14:59.058
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 04:14:59.077
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 04:14:59.417
    STEP: Deploying the webhook pod 08/25/22 04:14:59.423
    STEP: Wait for the deployment to be ready 08/25/22 04:14:59.436
    Aug 25 04:14:59.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 25 04:15:01.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 25, 4, 14, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/25/22 04:15:03.467
    STEP: Verifying the service has paired with the endpoint 08/25/22 04:15:03.477
    Aug 25 04:15:04.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Aug 25 04:15:05.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Aug 25 04:15:06.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Aug 25 04:15:07.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Aug 25 04:15:08.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Aug 25 04:15:09.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Aug 25 04:15:10.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Aug 25 04:15:10.482: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8859-crds.webhook.example.com via the AdmissionRegistration API 08/25/22 04:15:10.998
    STEP: Creating a custom resource that should be mutated by the webhook 08/25/22 04:15:11.015
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:15:13.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8713" for this suite. 08/25/22 04:15:13.61
    STEP: Destroying namespace "webhook-8713-markers" for this suite. 08/25/22 04:15:13.615
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:13.643
Aug 25 04:15:13.643: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-runtime 08/25/22 04:15:13.645
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:13.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:13.659
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 08/25/22 04:15:13.662
STEP: wait for the container to reach Failed 08/25/22 04:15:13.667
STEP: get the container status 08/25/22 04:15:17.692
STEP: the container should be terminated 08/25/22 04:15:17.696
STEP: the termination message should be set 08/25/22 04:15:17.696
Aug 25 04:15:17.696: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/25/22 04:15:17.696
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 25 04:15:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8693" for this suite. 08/25/22 04:15:17.711
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":296,"skipped":5469,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.072 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:13.643
    Aug 25 04:15:13.643: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-runtime 08/25/22 04:15:13.645
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:13.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:13.659
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 08/25/22 04:15:13.662
    STEP: wait for the container to reach Failed 08/25/22 04:15:13.667
    STEP: get the container status 08/25/22 04:15:17.692
    STEP: the container should be terminated 08/25/22 04:15:17.696
    STEP: the termination message should be set 08/25/22 04:15:17.696
    Aug 25 04:15:17.696: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/25/22 04:15:17.696
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 25 04:15:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8693" for this suite. 08/25/22 04:15:17.711
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:17.717
Aug 25 04:15:17.717: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:15:17.718
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:17.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:17.735
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-d4eeced5-3ee4-45f3-9afe-6ffd7a4e6268 08/25/22 04:15:17.739
STEP: Creating a pod to test consume configMaps 08/25/22 04:15:17.743
Aug 25 04:15:17.750: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75" in namespace "projected-7364" to be "Succeeded or Failed"
Aug 25 04:15:17.752: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698527ms
Aug 25 04:15:19.758: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008354353s
Aug 25 04:15:21.759: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009075022s
STEP: Saw pod success 08/25/22 04:15:21.759
Aug 25 04:15:21.759: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75" satisfied condition "Succeeded or Failed"
Aug 25 04:15:21.763: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75 container agnhost-container: <nil>
STEP: delete the pod 08/25/22 04:15:21.77
Aug 25 04:15:21.776: INFO: Waiting for pod pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75 to disappear
Aug 25 04:15:21.779: INFO: Pod pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 04:15:21.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7364" for this suite. 08/25/22 04:15:21.782
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":297,"skipped":5483,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.069 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:17.717
    Aug 25 04:15:17.717: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:15:17.718
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:17.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:17.735
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-d4eeced5-3ee4-45f3-9afe-6ffd7a4e6268 08/25/22 04:15:17.739
    STEP: Creating a pod to test consume configMaps 08/25/22 04:15:17.743
    Aug 25 04:15:17.750: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75" in namespace "projected-7364" to be "Succeeded or Failed"
    Aug 25 04:15:17.752: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698527ms
    Aug 25 04:15:19.758: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008354353s
    Aug 25 04:15:21.759: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009075022s
    STEP: Saw pod success 08/25/22 04:15:21.759
    Aug 25 04:15:21.759: INFO: Pod "pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75" satisfied condition "Succeeded or Failed"
    Aug 25 04:15:21.763: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75 container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 04:15:21.77
    Aug 25 04:15:21.776: INFO: Waiting for pod pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75 to disappear
    Aug 25 04:15:21.779: INFO: Pod pod-projected-configmaps-c7a20a45-0b55-45dd-9cd1-4ab3dbf70f75 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 04:15:21.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7364" for this suite. 08/25/22 04:15:21.782
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:21.788
Aug 25 04:15:21.788: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:15:21.79
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:21.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:21.804
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-aa633c43-fd9a-459f-b55a-cac1f83d5389 08/25/22 04:15:21.821
STEP: Creating a pod to test consume secrets 08/25/22 04:15:21.825
Aug 25 04:15:21.833: INFO: Waiting up to 5m0s for pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b" in namespace "secrets-2959" to be "Succeeded or Failed"
Aug 25 04:15:21.836: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527412ms
Aug 25 04:15:23.841: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007273071s
Aug 25 04:15:25.842: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008501038s
STEP: Saw pod success 08/25/22 04:15:25.842
Aug 25 04:15:25.842: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b" satisfied condition "Succeeded or Failed"
Aug 25 04:15:25.846: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 04:15:25.852
Aug 25 04:15:25.859: INFO: Waiting for pod pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b to disappear
Aug 25 04:15:25.862: INFO: Pod pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:15:25.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2959" for this suite. 08/25/22 04:15:25.866
STEP: Destroying namespace "secret-namespace-7683" for this suite. 08/25/22 04:15:25.87
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":298,"skipped":5495,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:21.788
    Aug 25 04:15:21.788: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:15:21.79
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:21.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:21.804
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-aa633c43-fd9a-459f-b55a-cac1f83d5389 08/25/22 04:15:21.821
    STEP: Creating a pod to test consume secrets 08/25/22 04:15:21.825
    Aug 25 04:15:21.833: INFO: Waiting up to 5m0s for pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b" in namespace "secrets-2959" to be "Succeeded or Failed"
    Aug 25 04:15:21.836: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527412ms
    Aug 25 04:15:23.841: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007273071s
    Aug 25 04:15:25.842: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008501038s
    STEP: Saw pod success 08/25/22 04:15:25.842
    Aug 25 04:15:25.842: INFO: Pod "pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b" satisfied condition "Succeeded or Failed"
    Aug 25 04:15:25.846: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:15:25.852
    Aug 25 04:15:25.859: INFO: Waiting for pod pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b to disappear
    Aug 25 04:15:25.862: INFO: Pod pod-secrets-1db52899-ce04-406b-8878-2d87eb13ab4b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:15:25.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2959" for this suite. 08/25/22 04:15:25.866
    STEP: Destroying namespace "secret-namespace-7683" for this suite. 08/25/22 04:15:25.87
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:25.874
Aug 25 04:15:25.874: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pod-network-test 08/25/22 04:15:25.874
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:25.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:25.886
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-8306 08/25/22 04:15:25.89
STEP: creating a selector 08/25/22 04:15:25.89
STEP: Creating the service pods in kubernetes 08/25/22 04:15:25.89
Aug 25 04:15:25.890: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 25 04:15:25.902: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8306" to be "running and ready"
Aug 25 04:15:25.906: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657566ms
Aug 25 04:15:25.906: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:15:27.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009405675s
Aug 25 04:15:27.912: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 04:15:29.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009778061s
Aug 25 04:15:29.912: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 04:15:31.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00918382s
Aug 25 04:15:31.912: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 04:15:33.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00896343s
Aug 25 04:15:33.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 04:15:35.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008679994s
Aug 25 04:15:35.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 25 04:15:37.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008346677s
Aug 25 04:15:37.911: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 25 04:15:37.911: INFO: Pod "netserver-0" satisfied condition "running and ready"
STEP: Creating test pods 08/25/22 04:15:37.915
Aug 25 04:15:37.921: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8306" to be "running"
Aug 25 04:15:37.925: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935401ms
Aug 25 04:15:39.931: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010262614s
Aug 25 04:15:39.931: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 25 04:15:39.936: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
Aug 25 04:15:39.936: INFO: Breadth first check of 192.168.0.10 on host 86.109.11.217...
Aug 25 04:15:39.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.0.13:9080/dial?request=hostname&protocol=udp&host=192.168.0.10&port=8081&tries=1'] Namespace:pod-network-test-8306 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 04:15:39.939: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 04:15:39.940: INFO: ExecWithOptions: Clientset creation
Aug 25 04:15:39.940: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8306/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.0.13%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.0.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 25 04:15:40.062: INFO: Waiting for responses: map[]
Aug 25 04:15:40.062: INFO: reached 192.168.0.10 after 0/1 tries
Aug 25 04:15:40.062: INFO: Going to retry 0 out of 1 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 25 04:15:40.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8306" for this suite. 08/25/22 04:15:40.066
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":299,"skipped":5506,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [14.197 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:25.874
    Aug 25 04:15:25.874: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pod-network-test 08/25/22 04:15:25.874
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:25.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:25.886
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-8306 08/25/22 04:15:25.89
    STEP: creating a selector 08/25/22 04:15:25.89
    STEP: Creating the service pods in kubernetes 08/25/22 04:15:25.89
    Aug 25 04:15:25.890: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 25 04:15:25.902: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8306" to be "running and ready"
    Aug 25 04:15:25.906: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657566ms
    Aug 25 04:15:25.906: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:15:27.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009405675s
    Aug 25 04:15:27.912: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 04:15:29.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009778061s
    Aug 25 04:15:29.912: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 04:15:31.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00918382s
    Aug 25 04:15:31.912: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 04:15:33.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00896343s
    Aug 25 04:15:33.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 04:15:35.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008679994s
    Aug 25 04:15:35.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 25 04:15:37.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008346677s
    Aug 25 04:15:37.911: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 25 04:15:37.911: INFO: Pod "netserver-0" satisfied condition "running and ready"
    STEP: Creating test pods 08/25/22 04:15:37.915
    Aug 25 04:15:37.921: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8306" to be "running"
    Aug 25 04:15:37.925: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935401ms
    Aug 25 04:15:39.931: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010262614s
    Aug 25 04:15:39.931: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 25 04:15:39.936: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
    Aug 25 04:15:39.936: INFO: Breadth first check of 192.168.0.10 on host 86.109.11.217...
    Aug 25 04:15:39.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.0.13:9080/dial?request=hostname&protocol=udp&host=192.168.0.10&port=8081&tries=1'] Namespace:pod-network-test-8306 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 04:15:39.939: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 04:15:39.940: INFO: ExecWithOptions: Clientset creation
    Aug 25 04:15:39.940: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8306/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.0.13%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.0.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 25 04:15:40.062: INFO: Waiting for responses: map[]
    Aug 25 04:15:40.062: INFO: reached 192.168.0.10 after 0/1 tries
    Aug 25 04:15:40.062: INFO: Going to retry 0 out of 1 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 25 04:15:40.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8306" for this suite. 08/25/22 04:15:40.066
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:40.071
Aug 25 04:15:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svcaccounts 08/25/22 04:15:40.073
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:40.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:40.089
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 08/25/22 04:15:40.094
STEP: watching for the ServiceAccount to be added 08/25/22 04:15:40.099
STEP: patching the ServiceAccount 08/25/22 04:15:40.101
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/25/22 04:15:40.104
STEP: deleting the ServiceAccount 08/25/22 04:15:40.108
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 25 04:15:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5247" for this suite. 08/25/22 04:15:40.119
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":300,"skipped":5507,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.052 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:40.071
    Aug 25 04:15:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svcaccounts 08/25/22 04:15:40.073
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:40.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:40.089
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 08/25/22 04:15:40.094
    STEP: watching for the ServiceAccount to be added 08/25/22 04:15:40.099
    STEP: patching the ServiceAccount 08/25/22 04:15:40.101
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/25/22 04:15:40.104
    STEP: deleting the ServiceAccount 08/25/22 04:15:40.108
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 25 04:15:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5247" for this suite. 08/25/22 04:15:40.119
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:40.128
Aug 25 04:15:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename namespaces 08/25/22 04:15:40.129
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:40.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:40.144
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 08/25/22 04:15:40.148
Aug 25 04:15:40.151: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/25/22 04:15:40.151
Aug 25 04:15:40.156: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/25/22 04:15:40.156
Aug 25 04:15:40.164: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:15:40.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9949" for this suite. 08/25/22 04:15:40.169
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":301,"skipped":5577,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.044 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:40.128
    Aug 25 04:15:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename namespaces 08/25/22 04:15:40.129
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:40.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:40.144
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 08/25/22 04:15:40.148
    Aug 25 04:15:40.151: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/25/22 04:15:40.151
    Aug 25 04:15:40.156: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/25/22 04:15:40.156
    Aug 25 04:15:40.164: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:15:40.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9949" for this suite. 08/25/22 04:15:40.169
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:40.175
Aug 25 04:15:40.175: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename job 08/25/22 04:15:40.176
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:40.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:40.19
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 08/25/22 04:15:40.195
STEP: Ensuring job reaches completions 08/25/22 04:15:40.201
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 25 04:15:54.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1621" for this suite. 08/25/22 04:15:54.211
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":302,"skipped":5594,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [14.042 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:40.175
    Aug 25 04:15:40.175: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename job 08/25/22 04:15:40.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:40.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:40.19
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 08/25/22 04:15:40.195
    STEP: Ensuring job reaches completions 08/25/22 04:15:40.201
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 25 04:15:54.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1621" for this suite. 08/25/22 04:15:54.211
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:54.217
Aug 25 04:15:54.218: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubectl 08/25/22 04:15:54.219
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:54.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:54.234
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Aug 25 04:15:54.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-4947 version'
Aug 25 04:15:54.310: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 25 04:15:54.310: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:38:15Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 25 04:15:54.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4947" for this suite. 08/25/22 04:15:54.315
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":303,"skipped":5604,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [0.102 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:54.217
    Aug 25 04:15:54.218: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubectl 08/25/22 04:15:54.219
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:54.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:54.234
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Aug 25 04:15:54.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=kubectl-4947 version'
    Aug 25 04:15:54.310: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 25 04:15:54.310: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:38:15Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 25 04:15:54.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4947" for this suite. 08/25/22 04:15:54.315
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:54.324
Aug 25 04:15:54.324: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:15:54.325
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:54.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:54.34
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 08/25/22 04:15:54.344
Aug 25 04:15:54.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a" in namespace "downward-api-7078" to be "Succeeded or Failed"
Aug 25 04:15:54.354: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578226ms
Aug 25 04:15:56.359: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a": Phase="Running", Reason="", readiness=false. Elapsed: 2.007923373s
Aug 25 04:15:58.360: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008396737s
STEP: Saw pod success 08/25/22 04:15:58.36
Aug 25 04:15:58.360: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a" satisfied condition "Succeeded or Failed"
Aug 25 04:15:58.364: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a container client-container: <nil>
STEP: delete the pod 08/25/22 04:15:58.369
Aug 25 04:15:58.384: INFO: Waiting for pod downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a to disappear
Aug 25 04:15:58.387: INFO: Pod downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 04:15:58.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7078" for this suite. 08/25/22 04:15:58.389
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":304,"skipped":5656,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:54.324
    Aug 25 04:15:54.324: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:15:54.325
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:54.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:54.34
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 08/25/22 04:15:54.344
    Aug 25 04:15:54.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a" in namespace "downward-api-7078" to be "Succeeded or Failed"
    Aug 25 04:15:54.354: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578226ms
    Aug 25 04:15:56.359: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a": Phase="Running", Reason="", readiness=false. Elapsed: 2.007923373s
    Aug 25 04:15:58.360: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008396737s
    STEP: Saw pod success 08/25/22 04:15:58.36
    Aug 25 04:15:58.360: INFO: Pod "downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a" satisfied condition "Succeeded or Failed"
    Aug 25 04:15:58.364: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a container client-container: <nil>
    STEP: delete the pod 08/25/22 04:15:58.369
    Aug 25 04:15:58.384: INFO: Waiting for pod downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a to disappear
    Aug 25 04:15:58.387: INFO: Pod downwardapi-volume-98069000-a197-4a57-9453-1d0630d3e68a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 04:15:58.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7078" for this suite. 08/25/22 04:15:58.389
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:15:58.394
Aug 25 04:15:58.395: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-preemption 08/25/22 04:15:58.395
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:58.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:58.408
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 25 04:15:58.421: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 25 04:16:58.507: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 08/25/22 04:16:58.511
Aug 25 04:16:58.537: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 25 04:16:58.542: INFO: Created pod: pod0-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/25/22 04:16:58.542
Aug 25 04:16:58.543: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8358" to be "running"
Aug 25 04:16:58.546: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48428ms
Aug 25 04:17:00.551: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008446756s
Aug 25 04:17:02.553: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.010010775s
Aug 25 04:17:02.553: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 25 04:17:02.553: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8358" to be "running"
Aug 25 04:17:02.557: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.259999ms
Aug 25 04:17:02.557: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/25/22 04:17:02.557
Aug 25 04:17:02.563: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8358" to be "running"
Aug 25 04:17:02.566: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6753ms
Aug 25 04:17:04.571: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008146536s
Aug 25 04:17:06.571: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007779694s
Aug 25 04:17:06.571: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:17:06.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8358" for this suite. 08/25/22 04:17:06.585
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":305,"skipped":5707,"failed":1,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]"]}
------------------------------
• [SLOW TEST] [68.246 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:15:58.394
    Aug 25 04:15:58.395: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-preemption 08/25/22 04:15:58.395
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:15:58.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:15:58.408
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 25 04:15:58.421: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 25 04:16:58.507: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 08/25/22 04:16:58.511
    Aug 25 04:16:58.537: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 25 04:16:58.542: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/25/22 04:16:58.542
    Aug 25 04:16:58.543: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8358" to be "running"
    Aug 25 04:16:58.546: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48428ms
    Aug 25 04:17:00.551: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008446756s
    Aug 25 04:17:02.553: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.010010775s
    Aug 25 04:17:02.553: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 25 04:17:02.553: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8358" to be "running"
    Aug 25 04:17:02.557: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.259999ms
    Aug 25 04:17:02.557: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/25/22 04:17:02.557
    Aug 25 04:17:02.563: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8358" to be "running"
    Aug 25 04:17:02.566: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6753ms
    Aug 25 04:17:04.571: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008146536s
    Aug 25 04:17:06.571: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007779694s
    Aug 25 04:17:06.571: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:17:06.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8358" for this suite. 08/25/22 04:17:06.585
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:06.641
Aug 25 04:17:06.641: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 04:17:06.643
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:06.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:06.655
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Aug 25 04:17:06.670: FAIL: Conformance test suite needs a cluster with at least 2 nodes.
Expected
    <int>: 1
to be >
    <int>: 1

Full Stack Trace
k8s.io/kubernetes/test/e2e/apps.glob..func4.9()
	test/e2e/apps/daemon_set.go:434 +0x1b6
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Aug 25 04:17:06.675: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54317"},"items":null}

Aug 25 04:17:06.677: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54317"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Collecting events from namespace "daemonsets-325". 08/25/22 04:17:06.682
STEP: Found 0 events. 08/25/22 04:17:06.685
Aug 25 04:17:06.687: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Aug 25 04:17:06.687: INFO: 
Aug 25 04:17:06.691: INFO: 
Logging node info for node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:17:06.694: INFO: Node Info: &Node{ObjectMeta:{bobymcbobs-c849-control-plane-p55pp    c5dc1f57-bb1e-4562-b424-f31ab182c240 54230 0 2022-08-25 02:40:28 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:bobymcbobs-c849-control-plane-p55pp kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[cluster.x-k8s.io/cluster-name:bobymcbobs-c849 cluster.x-k8s.io/cluster-namespace:sharingio-pair cluster.x-k8s.io/machine:bobymcbobs-c849-control-plane-fmvtt cluster.x-k8s.io/owner-kind:KubeadmControlPlane cluster.x-k8s.io/owner-name:bobymcbobs-c849-control-plane kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2022-08-25 02:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}}} } {kubeadm Update v1 2022-08-25 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2022-08-25 02:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}} } {kube-utils Update v1 2022-08-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:spec":{"f:providerID":{}}} } {manager Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cluster.x-k8s.io/cluster-name":{},"f:cluster.x-k8s.io/cluster-namespace":{},"f:cluster.x-k8s.io/machine":{},"f:cluster.x-k8s.io/owner-kind":{},"f:cluster.x-k8s.io/owner-name":{}}}} } {e2e.test Update v1 2022-08-25 04:16:58 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}}}} status} {kubelet Update v1 2022-08-25 04:16:59 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{}}} status}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{234139500544 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404313366528 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Allocatable:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{210725550141 0} {<nil>} 210725550141 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404208508928 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-08-25 02:40:57 +0000 UTC,LastTransitionTime:2022-08-25 02:40:57 +0000 UTC,Reason:WeaveIsUp,Message:Weave pod has set this,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:52 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:86.109.11.217,},NodeAddress{Type:Hostname,Address:bobymcbobs-c849-control-plane-p55pp,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4b0c7bda23ca4aa6a943cda55f9fa1e2,SystemUUID:4c4c4544-0053-4710-8038-b8c04f483033,BootID:217f066f-7295-498b-9b6e-db50783a6a97,KernelVersion:5.4.0-109-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:containerd://1.6.7,KubeletVersion:v1.25.0,KubeProxyVersion:v1.25.0,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.gitlab.com/sharingio/environment/environment@sha256:37f277951643b2b0e70c1a7c72b1fa03fd7adca706c6e10202ac6a9ea81d1a45 registry.gitlab.com/sharingio/environment/environment:2022.07.25.1600],SizeBytes:2667236715,},ContainerImage{Names:[registry.gitlab.com/ii/nz/reveal-multiplex@sha256:299c977cc6734bf88fa5bdc22f8f6d464f0435437baee92aef5b33534d5ccf1f registry.gitlab.com/ii/nz/reveal-multiplex:latest],SizeBytes:216223066,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e registry.k8s.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:112030526,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:6f72b851544986cb0921b53ea655ec04c36131248f16d4ad110cb3ca0c369dc1 registry.k8s.io/etcd:3.5.4-0],SizeBytes:102157811,},ContainerImage{Names:[docker.io/linuxserver/mariadb@sha256:bceed8b46f875fd8c5fcf60d9201e1e8093f8ab181234d2bf3824c6621b897b6 docker.io/linuxserver/mariadb:alpine-version-10.5.12-r0],SizeBytes:89567136,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:8b45c5f7ab38f1ae9847e81f91aafa78d9905ae37cdae6fe3edaa43f3e6dfdfa registry.k8s.io/conformance:v1.25.0],SizeBytes:78887805,},ContainerImage{Names:[docker.io/fluxcd/helm-operator@sha256:8e63dcdbe0ce672215e1946bd64c88a362194dad901420632f529a295c2b8068 docker.io/fluxcd/helm-operator:1.4.0],SizeBytes:77258104,},ContainerImage{Names:[docker.io/envoyproxy/envoy@sha256:1f343072a58e74644b7adc8d2d877071f846fc77166295a6d2686aee6cf58162 docker.io/envoyproxy/envoy:v1.22.2],SizeBytes:51349232,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c registry.k8s.io/e2e-test-images/nautilus:1.5],SizeBytes:49642095,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 registry.k8s.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:41902010,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[quay.io/metallb/speaker@sha256:58cb99053bb33db1944f8b67c9d3335afe98c8ba810fca45f19cfde1442064c4 quay.io/metallb/speaker:v0.10.2],SizeBytes:39339636,},ContainerImage{Names:[docker.io/pschiffe/pdns-mysql@sha256:f9cabc76207919fcd82bcc1243df2ca510f89e61f6f5d2e2f54541e2b7d65d24 docker.io/pschiffe/pdns-mysql:4.3-alpine],SizeBytes:36903432,},ContainerImage{Names:[quay.io/metallb/controller@sha256:21164241c045443595439f2821a193ef823dbbd1ccc8959e1c363c44cc6d1e18 quay.io/metallb/controller:v0.10.2],SizeBytes:36020654,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:f6902791fb9aa6e283ed7d1d743417b3c425eec73151517813bef1539a66aefa registry.k8s.io/kube-apiserver:v1.25.0],SizeBytes:34227200,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:66ce7d460e53f942bb4729f656d66fe475ec3d41728de986b6d790eee6d8205d registry.k8s.io/kube-controller-manager:v1.25.0],SizeBytes:31259842,},ContainerImage{Names:[docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63 docker.io/weaveworks/weave-kube:2.8.1],SizeBytes:30924173,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[docker.io/appscode/kubed@sha256:d694910be47b07f941e44cf60514aa5284944b1271a456e51c45208fae296ee9 docker.io/appscode/kubed:v0.12.0],SizeBytes:25358728,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/sample-apiserver@sha256:f9c93b92b6ff750b41a93c4e4fe0bfe384597aeb841e2539d5444815c55b2d8f registry.k8s.io/e2e-test-images/sample-apiserver:1.17.5],SizeBytes:24316368,},ContainerImage{Names:[k8s.gcr.io/external-dns/external-dns@sha256:d9569d17d224513f252520d3c92d6519aedc40524f4395a9185c7e66de3cdab7 k8s.gcr.io/external-dns/external-dns:v0.10.0],SizeBytes:21974457,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:1b1f3456bb19866aa1655c607514b85cd2b6efdfea4d93ea55e79475ff2765f9 registry.k8s.io/kube-proxy:v1.25.0],SizeBytes:20262263,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c],SizeBytes:18081586,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/operator/cmd/operator@sha256:2cbbbe9a7873fc9414a09c3ed558dde13a8105e70e4c83300e3a0a820534897f],SizeBytes:17654221,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:c2ac18db63de4982496e6c8650e20cecf4dd32172516b5934d051c7c084063da docker.io/sonobuoy/sonobuoy:v0.56.10],SizeBytes:17407841,},ContainerImage{Names:[quay.io/jetstack/cert-manager-controller@sha256:51027a4cc4d30e197e3506daf3a4fa2d2a0bc2826469f8a87848dfd279e031c0 quay.io/jetstack/cert-manager-controller:v1.7.1],SizeBytes:17112219,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2],SizeBytes:16259613,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7],SizeBytes:16056815,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:5526e650784165aa2ed2af1846e0e8bf37d5c52815ad6865bbe5d99d78eca32e],SizeBytes:16004457,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b],SizeBytes:15960524,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb],SizeBytes:15874009,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/controller@sha256:08409982c2dda0992cc16c88a9f16dab4efff54a29deb3072617fec6efcef6c4],SizeBytes:15830636,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:9330c53feca7b51b25e427fa96afd5d1460b3233e9fa92e20c895c067da56ac1 registry.k8s.io/kube-scheduler:v1.25.0],SizeBytes:15793363,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-contour/cmd/controller@sha256:baf500920abd51e33c7497799f5b241560783ad9a3bb6fec746770eb734d488a],SizeBytes:15760951,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0],SizeBytes:15432856,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/webhook@sha256:31da8a42d9f28999bdeeea4a0a312e0e6c52ab1f18c97c88a7e2c27627482835],SizeBytes:15312462,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a registry.k8s.io/coredns/coredns:v1.9.3],SizeBytes:14837849,},ContainerImage{Names:[quay.io/jetstack/cert-manager-webhook@sha256:a926d60b6f23553ca5d11ac9cd66bcc692136e838613c8bc0d60c6c35a3cbcfc quay.io/jetstack/cert-manager-webhook:v1.7.1],SizeBytes:13636913,},ContainerImage{Names:[docker.io/rancher/local-path-provisioner@sha256:9666b1635fec95d4e2251661e135c90678b8f45fd0f8324c55db99c80e2a958c docker.io/rancher/local-path-provisioner:v0.0.19],SizeBytes:13585626,},ContainerImage{Names:[ghcr.io/projectcontour/contour@sha256:c64e1f72fad4b495d6e0967c2b3063c06ba2d79c9098331ba0219094204023c7 ghcr.io/projectcontour/contour:v1.21.1],SizeBytes:13407229,},ContainerImage{Names:[docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c docker.io/weaveworks/weave-npc:2.8.1],SizeBytes:12814131,},ContainerImage{Names:[quay.io/jetstack/cert-manager-cainjector@sha256:985743eeed2b62f68ee06e583f1d5a371e1c35af4b1980a1b2571d29174cce47 quay.io/jetstack/cert-manager-cainjector:v1.7.1],SizeBytes:12094880,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/pkg/apiextensions/storageversion/cmd/migrate@sha256:b333601675dc5f8349b3133bd9fcc7a620db0a5076892ea00338b19fc1e073fb],SizeBytes:11564327,},ContainerImage{Names:[docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96 docker.io/library/registry:2.8.1],SizeBytes:9204128,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exposer@sha256:d5234c7aee6d1281a72b4b1f09124a662ca230ece854def9449c988390f29897 registry.gitlab.com/sharingio/environment/exposer:2022.07.25.1600],SizeBytes:8641265,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 registry.k8s.io/e2e-test-images/nginx:1.14-2],SizeBytes:6979041,},ContainerImage{Names:[registry.gitlab.com/safesurfer/go-http-server@sha256:5a092b5372f23ec18117411b63ffd60f73dce967204087b24cc4b65b97e2c0b4 registry.gitlab.com/safesurfer/go-http-server:1.6.0],SizeBytes:4512154,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac registry.k8s.io/e2e-test-images/nonewprivs:1.3],SizeBytes:3263463,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Aug 25 04:17:06.695: INFO: 
Logging kubelet events for node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:17:06.698: INFO: 
Logging pods the kubelet thinks is on node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:17:06.726: INFO: activator-88b7df5c-cbxk8 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container activator ready: true, restart count 0
Aug 25 04:17:06.726: INFO: contour-588fc6cc6d-f66zz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.726: INFO: controller-7fd644cbc6-dfb6n started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.726: INFO: environment-0 started at 2022-08-25 03:25:19 +0000 UTC (0+2 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container environment ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 04:17:06.726: INFO: contour-588fc6cc6d-dq6wf started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.726: INFO: coredns-565d847f94-lzgv5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:17:06.726: INFO: webhook-69d55f5549-ghp24 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:17:06.726: INFO: local-path-provisioner-56c55476f9-xbkr9 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 04:17:06.726: INFO: helm-operator-7959478576-6mcjz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 04:17:06.726: INFO: powerdns-5c6dbcf579-rgzxs started at 2022-08-25 03:24:37 +0000 UTC (1+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Init container powerdns-init-db ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 04:17:06.726: INFO: reveal-multiplex-6bbbf59d6-k9lt8 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 04:17:06.726: INFO: weave-net-75fql started at 2022-08-25 02:40:45 +0000 UTC (1+2 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Init container weave-init ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container weave ready: true, restart count 1
Aug 25 04:17:06.726: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 04:17:06.726: INFO: net-certmanager-controller-6c8cb88879-86vmg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.726: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 04:17:06.726: INFO: public-html-go-http-server-55b9f584d4-4jrp5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 04:17:06.726: INFO: kubed-568bdbc6c4-vx7m6 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container kubed ready: true, restart count 0
Aug 25 04:17:06.726: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:17:06.726: INFO: domainmapping-webhook-77f466bbc9-97ft7 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 04:17:06.726: INFO: net-certmanager-webhook-79cfb96f68-kfnlr started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:17:06.726: INFO: preemptor-pod started at 2022-08-25 04:17:04 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container preemptor-pod ready: true, restart count 0
Aug 25 04:17:06.726: INFO: contour-8597b78798-lb4wb started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.726: INFO: envoy-7sxnc started at 2022-08-25 03:25:38 +0000 UTC (1+2 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Init container envoy-initconfig ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:17:06.726: INFO: contour-8597b78798-5755z started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.726: INFO: cert-manager-66bd77df8f-qlhqg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:17:06.726: INFO: environment-exposer-5bb6d44465-8btrx started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 04:17:06.726: INFO: autoscaler-hpa-776c44cc57-rcqj4 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 04:17:06.726: INFO: metrics-server-5cb46dccf6-w9d6b started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 04:17:06.726: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 25 04:17:06.726: INFO: controller-77cc7ff558-mrwjl started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.726: INFO: net-contour-controller-5c5fd89fdb-lnf94 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.726: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 04:17:06.726: INFO: kube-proxy-b4lgh started at 2022-08-25 02:40:45 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 04:17:06.726: INFO: envoy-cxlbl started at 2022-08-25 03:25:39 +0000 UTC (1+2 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Init container envoy-initconfig ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:17:06.726: INFO: distribution-7f9dff85c4-c276b started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container distribution ready: true, restart count 0
Aug 25 04:17:06.726: INFO: powerdns-db-7c897fddf4-l5cjb started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 04:17:06.726: INFO: coredns-565d847f94-684lz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:17:06.726: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 04:17:06.726: INFO: domain-mapping-5bcd85fbc6-2gshd started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 04:17:06.726: INFO: knative-operator-594876444d-qzjs4 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 04:17:06.726: INFO: sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 04:17:06.726: INFO: pod0-1-sched-preemption-medium-priority started at 2022-08-25 04:16:59 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Aug 25 04:17:06.726: INFO: external-dns-67d79cbcd4-bf29h started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 04:17:06.726: INFO: etcd-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container etcd ready: true, restart count 0
Aug 25 04:17:06.726: INFO: autoscaler-7bf8ff94db-98bhj started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 04:17:06.726: INFO: cert-manager-cainjector-6495667ff4-x6lll started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:17:06.726: INFO: sonobuoy-e2e-job-39386ef5d0694a49 started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container e2e ready: true, restart count 0
Aug 25 04:17:06.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:17:06.726: INFO: speaker-f82r7 started at 2022-08-25 03:25:09 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:17:06.726: INFO: 	Container speaker ready: true, restart count 0
Aug 25 04:17:06.848: INFO: 
Latency metrics for node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:17:06.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-325" for this suite. 08/25/22 04:17:06.852
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"FAILED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":305,"skipped":5709,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [FAILED] [0.215 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  [It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:06.641
    Aug 25 04:17:06.641: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 04:17:06.643
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:06.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:06.655
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Aug 25 04:17:06.670: FAIL: Conformance test suite needs a cluster with at least 2 nodes.
    Expected
        <int>: 1
    to be >
        <int>: 1

    Full Stack Trace
    k8s.io/kubernetes/test/e2e/apps.glob..func4.9()
    	test/e2e/apps/daemon_set.go:434 +0x1b6
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Aug 25 04:17:06.675: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54317"},"items":null}

    Aug 25 04:17:06.677: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54317"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    STEP: Collecting events from namespace "daemonsets-325". 08/25/22 04:17:06.682
    STEP: Found 0 events. 08/25/22 04:17:06.685
    Aug 25 04:17:06.687: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
    Aug 25 04:17:06.687: INFO: 
    Aug 25 04:17:06.691: INFO: 
    Logging node info for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:17:06.694: INFO: Node Info: &Node{ObjectMeta:{bobymcbobs-c849-control-plane-p55pp    c5dc1f57-bb1e-4562-b424-f31ab182c240 54230 0 2022-08-25 02:40:28 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:bobymcbobs-c849-control-plane-p55pp kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[cluster.x-k8s.io/cluster-name:bobymcbobs-c849 cluster.x-k8s.io/cluster-namespace:sharingio-pair cluster.x-k8s.io/machine:bobymcbobs-c849-control-plane-fmvtt cluster.x-k8s.io/owner-kind:KubeadmControlPlane cluster.x-k8s.io/owner-name:bobymcbobs-c849-control-plane kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2022-08-25 02:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}}} } {kubeadm Update v1 2022-08-25 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2022-08-25 02:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}} } {kube-utils Update v1 2022-08-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:spec":{"f:providerID":{}}} } {manager Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cluster.x-k8s.io/cluster-name":{},"f:cluster.x-k8s.io/cluster-namespace":{},"f:cluster.x-k8s.io/machine":{},"f:cluster.x-k8s.io/owner-kind":{},"f:cluster.x-k8s.io/owner-name":{}}}} } {e2e.test Update v1 2022-08-25 04:16:58 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}}}} status} {kubelet Update v1 2022-08-25 04:16:59 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{}}} status}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{234139500544 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404313366528 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Allocatable:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{210725550141 0} {<nil>} 210725550141 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404208508928 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-08-25 02:40:57 +0000 UTC,LastTransitionTime:2022-08-25 02:40:57 +0000 UTC,Reason:WeaveIsUp,Message:Weave pod has set this,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:52 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:86.109.11.217,},NodeAddress{Type:Hostname,Address:bobymcbobs-c849-control-plane-p55pp,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4b0c7bda23ca4aa6a943cda55f9fa1e2,SystemUUID:4c4c4544-0053-4710-8038-b8c04f483033,BootID:217f066f-7295-498b-9b6e-db50783a6a97,KernelVersion:5.4.0-109-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:containerd://1.6.7,KubeletVersion:v1.25.0,KubeProxyVersion:v1.25.0,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.gitlab.com/sharingio/environment/environment@sha256:37f277951643b2b0e70c1a7c72b1fa03fd7adca706c6e10202ac6a9ea81d1a45 registry.gitlab.com/sharingio/environment/environment:2022.07.25.1600],SizeBytes:2667236715,},ContainerImage{Names:[registry.gitlab.com/ii/nz/reveal-multiplex@sha256:299c977cc6734bf88fa5bdc22f8f6d464f0435437baee92aef5b33534d5ccf1f registry.gitlab.com/ii/nz/reveal-multiplex:latest],SizeBytes:216223066,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e registry.k8s.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:112030526,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:6f72b851544986cb0921b53ea655ec04c36131248f16d4ad110cb3ca0c369dc1 registry.k8s.io/etcd:3.5.4-0],SizeBytes:102157811,},ContainerImage{Names:[docker.io/linuxserver/mariadb@sha256:bceed8b46f875fd8c5fcf60d9201e1e8093f8ab181234d2bf3824c6621b897b6 docker.io/linuxserver/mariadb:alpine-version-10.5.12-r0],SizeBytes:89567136,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:8b45c5f7ab38f1ae9847e81f91aafa78d9905ae37cdae6fe3edaa43f3e6dfdfa registry.k8s.io/conformance:v1.25.0],SizeBytes:78887805,},ContainerImage{Names:[docker.io/fluxcd/helm-operator@sha256:8e63dcdbe0ce672215e1946bd64c88a362194dad901420632f529a295c2b8068 docker.io/fluxcd/helm-operator:1.4.0],SizeBytes:77258104,},ContainerImage{Names:[docker.io/envoyproxy/envoy@sha256:1f343072a58e74644b7adc8d2d877071f846fc77166295a6d2686aee6cf58162 docker.io/envoyproxy/envoy:v1.22.2],SizeBytes:51349232,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c registry.k8s.io/e2e-test-images/nautilus:1.5],SizeBytes:49642095,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 registry.k8s.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:41902010,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[quay.io/metallb/speaker@sha256:58cb99053bb33db1944f8b67c9d3335afe98c8ba810fca45f19cfde1442064c4 quay.io/metallb/speaker:v0.10.2],SizeBytes:39339636,},ContainerImage{Names:[docker.io/pschiffe/pdns-mysql@sha256:f9cabc76207919fcd82bcc1243df2ca510f89e61f6f5d2e2f54541e2b7d65d24 docker.io/pschiffe/pdns-mysql:4.3-alpine],SizeBytes:36903432,},ContainerImage{Names:[quay.io/metallb/controller@sha256:21164241c045443595439f2821a193ef823dbbd1ccc8959e1c363c44cc6d1e18 quay.io/metallb/controller:v0.10.2],SizeBytes:36020654,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:f6902791fb9aa6e283ed7d1d743417b3c425eec73151517813bef1539a66aefa registry.k8s.io/kube-apiserver:v1.25.0],SizeBytes:34227200,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:66ce7d460e53f942bb4729f656d66fe475ec3d41728de986b6d790eee6d8205d registry.k8s.io/kube-controller-manager:v1.25.0],SizeBytes:31259842,},ContainerImage{Names:[docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63 docker.io/weaveworks/weave-kube:2.8.1],SizeBytes:30924173,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[docker.io/appscode/kubed@sha256:d694910be47b07f941e44cf60514aa5284944b1271a456e51c45208fae296ee9 docker.io/appscode/kubed:v0.12.0],SizeBytes:25358728,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/sample-apiserver@sha256:f9c93b92b6ff750b41a93c4e4fe0bfe384597aeb841e2539d5444815c55b2d8f registry.k8s.io/e2e-test-images/sample-apiserver:1.17.5],SizeBytes:24316368,},ContainerImage{Names:[k8s.gcr.io/external-dns/external-dns@sha256:d9569d17d224513f252520d3c92d6519aedc40524f4395a9185c7e66de3cdab7 k8s.gcr.io/external-dns/external-dns:v0.10.0],SizeBytes:21974457,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:1b1f3456bb19866aa1655c607514b85cd2b6efdfea4d93ea55e79475ff2765f9 registry.k8s.io/kube-proxy:v1.25.0],SizeBytes:20262263,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c],SizeBytes:18081586,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/operator/cmd/operator@sha256:2cbbbe9a7873fc9414a09c3ed558dde13a8105e70e4c83300e3a0a820534897f],SizeBytes:17654221,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:c2ac18db63de4982496e6c8650e20cecf4dd32172516b5934d051c7c084063da docker.io/sonobuoy/sonobuoy:v0.56.10],SizeBytes:17407841,},ContainerImage{Names:[quay.io/jetstack/cert-manager-controller@sha256:51027a4cc4d30e197e3506daf3a4fa2d2a0bc2826469f8a87848dfd279e031c0 quay.io/jetstack/cert-manager-controller:v1.7.1],SizeBytes:17112219,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2],SizeBytes:16259613,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7],SizeBytes:16056815,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:5526e650784165aa2ed2af1846e0e8bf37d5c52815ad6865bbe5d99d78eca32e],SizeBytes:16004457,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b],SizeBytes:15960524,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb],SizeBytes:15874009,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/controller@sha256:08409982c2dda0992cc16c88a9f16dab4efff54a29deb3072617fec6efcef6c4],SizeBytes:15830636,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:9330c53feca7b51b25e427fa96afd5d1460b3233e9fa92e20c895c067da56ac1 registry.k8s.io/kube-scheduler:v1.25.0],SizeBytes:15793363,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-contour/cmd/controller@sha256:baf500920abd51e33c7497799f5b241560783ad9a3bb6fec746770eb734d488a],SizeBytes:15760951,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0],SizeBytes:15432856,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/webhook@sha256:31da8a42d9f28999bdeeea4a0a312e0e6c52ab1f18c97c88a7e2c27627482835],SizeBytes:15312462,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a registry.k8s.io/coredns/coredns:v1.9.3],SizeBytes:14837849,},ContainerImage{Names:[quay.io/jetstack/cert-manager-webhook@sha256:a926d60b6f23553ca5d11ac9cd66bcc692136e838613c8bc0d60c6c35a3cbcfc quay.io/jetstack/cert-manager-webhook:v1.7.1],SizeBytes:13636913,},ContainerImage{Names:[docker.io/rancher/local-path-provisioner@sha256:9666b1635fec95d4e2251661e135c90678b8f45fd0f8324c55db99c80e2a958c docker.io/rancher/local-path-provisioner:v0.0.19],SizeBytes:13585626,},ContainerImage{Names:[ghcr.io/projectcontour/contour@sha256:c64e1f72fad4b495d6e0967c2b3063c06ba2d79c9098331ba0219094204023c7 ghcr.io/projectcontour/contour:v1.21.1],SizeBytes:13407229,},ContainerImage{Names:[docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c docker.io/weaveworks/weave-npc:2.8.1],SizeBytes:12814131,},ContainerImage{Names:[quay.io/jetstack/cert-manager-cainjector@sha256:985743eeed2b62f68ee06e583f1d5a371e1c35af4b1980a1b2571d29174cce47 quay.io/jetstack/cert-manager-cainjector:v1.7.1],SizeBytes:12094880,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/pkg/apiextensions/storageversion/cmd/migrate@sha256:b333601675dc5f8349b3133bd9fcc7a620db0a5076892ea00338b19fc1e073fb],SizeBytes:11564327,},ContainerImage{Names:[docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96 docker.io/library/registry:2.8.1],SizeBytes:9204128,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exposer@sha256:d5234c7aee6d1281a72b4b1f09124a662ca230ece854def9449c988390f29897 registry.gitlab.com/sharingio/environment/exposer:2022.07.25.1600],SizeBytes:8641265,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 registry.k8s.io/e2e-test-images/nginx:1.14-2],SizeBytes:6979041,},ContainerImage{Names:[registry.gitlab.com/safesurfer/go-http-server@sha256:5a092b5372f23ec18117411b63ffd60f73dce967204087b24cc4b65b97e2c0b4 registry.gitlab.com/safesurfer/go-http-server:1.6.0],SizeBytes:4512154,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac registry.k8s.io/e2e-test-images/nonewprivs:1.3],SizeBytes:3263463,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
    Aug 25 04:17:06.695: INFO: 
    Logging kubelet events for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:17:06.698: INFO: 
    Logging pods the kubelet thinks is on node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:17:06.726: INFO: activator-88b7df5c-cbxk8 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container activator ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: contour-588fc6cc6d-f66zz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: controller-7fd644cbc6-dfb6n started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: environment-0 started at 2022-08-25 03:25:19 +0000 UTC (0+2 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container environment ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: contour-588fc6cc6d-dq6wf started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: coredns-565d847f94-lzgv5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: webhook-69d55f5549-ghp24 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: local-path-provisioner-56c55476f9-xbkr9 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: helm-operator-7959478576-6mcjz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: powerdns-5c6dbcf579-rgzxs started at 2022-08-25 03:24:37 +0000 UTC (1+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Init container powerdns-init-db ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: reveal-multiplex-6bbbf59d6-k9lt8 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: weave-net-75fql started at 2022-08-25 02:40:45 +0000 UTC (1+2 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Init container weave-init ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container weave ready: true, restart count 1
    Aug 25 04:17:06.726: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: net-certmanager-controller-6c8cb88879-86vmg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: public-html-go-http-server-55b9f584d4-4jrp5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: kubed-568bdbc6c4-vx7m6 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: domainmapping-webhook-77f466bbc9-97ft7 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: net-certmanager-webhook-79cfb96f68-kfnlr started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: preemptor-pod started at 2022-08-25 04:17:04 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container preemptor-pod ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: contour-8597b78798-lb4wb started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: envoy-7sxnc started at 2022-08-25 03:25:38 +0000 UTC (1+2 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Init container envoy-initconfig ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: contour-8597b78798-5755z started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: cert-manager-66bd77df8f-qlhqg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: environment-exposer-5bb6d44465-8btrx started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: autoscaler-hpa-776c44cc57-rcqj4 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: metrics-server-5cb46dccf6-w9d6b started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: controller-77cc7ff558-mrwjl started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: net-contour-controller-5c5fd89fdb-lnf94 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: kube-proxy-b4lgh started at 2022-08-25 02:40:45 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: envoy-cxlbl started at 2022-08-25 03:25:39 +0000 UTC (1+2 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Init container envoy-initconfig ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: distribution-7f9dff85c4-c276b started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: powerdns-db-7c897fddf4-l5cjb started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: coredns-565d847f94-684lz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: domain-mapping-5bcd85fbc6-2gshd started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: knative-operator-594876444d-qzjs4 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: pod0-1-sched-preemption-medium-priority started at 2022-08-25 04:16:59 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: external-dns-67d79cbcd4-bf29h started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: etcd-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: autoscaler-7bf8ff94db-98bhj started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: cert-manager-cainjector-6495667ff4-x6lll started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: sonobuoy-e2e-job-39386ef5d0694a49 started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:17:06.726: INFO: speaker-f82r7 started at 2022-08-25 03:25:09 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:17:06.726: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 04:17:06.848: INFO: 
    Latency metrics for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:17:06.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-325" for this suite. 08/25/22 04:17:06.852
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output

  Aug 25 04:17:06.670: Conformance test suite needs a cluster with at least 2 nodes.
  Expected
      <int>: 1
  to be >
      <int>: 1
  In [It] at: test/e2e/apps/daemon_set.go:434
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:06.857
Aug 25 04:17:06.857: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-pred 08/25/22 04:17:06.858
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:06.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:06.873
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 25 04:17:06.876: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 25 04:17:06.883: INFO: Waiting for terminating namespaces to be deleted...
Aug 25 04:17:06.886: INFO: 
Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
Aug 25 04:17:06.908: INFO: environment-0 from bobymcbobs started at 2022-08-25 03:25:19 +0000 UTC (2 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container environment ready: true, restart count 0
Aug 25 04:17:06.908: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 04:17:06.908: INFO: cert-manager-66bd77df8f-qlhqg from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:17:06.908: INFO: cert-manager-cainjector-6495667ff4-x6lll from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:17:06.908: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 from cert-manager started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:17:06.908: INFO: contour-588fc6cc6d-dq6wf from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.908: INFO: contour-588fc6cc6d-f66zz from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.908: INFO: envoy-cxlbl from contour-external started at 2022-08-25 03:25:39 +0000 UTC (2 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:17:06.908: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:17:06.908: INFO: contour-8597b78798-5755z from contour-internal started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.908: INFO: contour-8597b78798-lb4wb from contour-internal started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:17:06.908: INFO: envoy-7sxnc from contour-internal started at 2022-08-25 03:25:38 +0000 UTC (2 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:17:06.908: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:17:06.908: INFO: external-dns-67d79cbcd4-bf29h from external-dns started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 04:17:06.908: INFO: helm-operator-7959478576-6mcjz from helm-operator started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 04:17:06.908: INFO: knative-operator-594876444d-qzjs4 from knative-operator started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 04:17:06.908: INFO: activator-88b7df5c-cbxk8 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container activator ready: true, restart count 0
Aug 25 04:17:06.908: INFO: autoscaler-7bf8ff94db-98bhj from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.908: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 04:17:06.908: INFO: autoscaler-hpa-776c44cc57-rcqj4 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 04:17:06.909: INFO: controller-7fd644cbc6-dfb6n from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.909: INFO: domain-mapping-5bcd85fbc6-2gshd from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 04:17:06.909: INFO: domainmapping-webhook-77f466bbc9-97ft7 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 04:17:06.909: INFO: net-certmanager-controller-6c8cb88879-86vmg from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.909: INFO: net-certmanager-webhook-79cfb96f68-kfnlr from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:17:06.909: INFO: net-contour-controller-5c5fd89fdb-lnf94 from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.909: INFO: webhook-69d55f5549-ghp24 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:17:06.909: INFO: coredns-565d847f94-684lz from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:17:06.909: INFO: coredns-565d847f94-lzgv5 from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:17:06.909: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container etcd ready: true, restart count 0
Aug 25 04:17:06.909: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 04:17:06.909: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 04:17:06.909: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 04:17:06.909: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 04:17:06.909: INFO: kubed-568bdbc6c4-vx7m6 from kube-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container kubed ready: true, restart count 0
Aug 25 04:17:06.909: INFO: metrics-server-5cb46dccf6-w9d6b from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 04:17:06.909: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container weave ready: true, restart count 1
Aug 25 04:17:06.909: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 04:17:06.909: INFO: local-path-provisioner-56c55476f9-xbkr9 from local-path-storage started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 04:17:06.909: INFO: controller-77cc7ff558-mrwjl from metallb-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:17:06.909: INFO: speaker-f82r7 from metallb-system started at 2022-08-25 03:25:09 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container speaker ready: true, restart count 0
Aug 25 04:17:06.909: INFO: distribution-7f9dff85c4-c276b from pair-system started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container distribution ready: true, restart count 0
Aug 25 04:17:06.909: INFO: environment-exposer-5bb6d44465-8btrx from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 04:17:06.909: INFO: powerdns-5c6dbcf579-rgzxs from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 04:17:06.909: INFO: powerdns-db-7c897fddf4-l5cjb from pair-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 04:17:06.909: INFO: public-html-go-http-server-55b9f584d4-4jrp5 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 04:17:06.909: INFO: reveal-multiplex-6bbbf59d6-k9lt8 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 04:17:06.909: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-8358 started at 2022-08-25 04:16:59 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Aug 25 04:17:06.909: INFO: preemptor-pod from sched-preemption-8358 started at 2022-08-25 04:17:04 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container preemptor-pod ready: true, restart count 0
Aug 25 04:17:06.909: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 04:17:06.909: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container e2e ready: true, restart count 0
Aug 25 04:17:06.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:17:06.909: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 04:17:06.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:17:06.909: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/25/22 04:17:06.909
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.170e7b6e78950359], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.] 08/25/22 04:17:06.949
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:17:07.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1429" for this suite. 08/25/22 04:17:07.955
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":306,"skipped":5713,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [1.102 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:06.857
    Aug 25 04:17:06.857: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-pred 08/25/22 04:17:06.858
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:06.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:06.873
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 25 04:17:06.876: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 25 04:17:06.883: INFO: Waiting for terminating namespaces to be deleted...
    Aug 25 04:17:06.886: INFO: 
    Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
    Aug 25 04:17:06.908: INFO: environment-0 from bobymcbobs started at 2022-08-25 03:25:19 +0000 UTC (2 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container environment ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: cert-manager-66bd77df8f-qlhqg from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: cert-manager-cainjector-6495667ff4-x6lll from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 from cert-manager started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: contour-588fc6cc6d-dq6wf from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: contour-588fc6cc6d-f66zz from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: envoy-cxlbl from contour-external started at 2022-08-25 03:25:39 +0000 UTC (2 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: contour-8597b78798-5755z from contour-internal started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: contour-8597b78798-lb4wb from contour-internal started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: envoy-7sxnc from contour-internal started at 2022-08-25 03:25:38 +0000 UTC (2 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: external-dns-67d79cbcd4-bf29h from external-dns started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: helm-operator-7959478576-6mcjz from helm-operator started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: knative-operator-594876444d-qzjs4 from knative-operator started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: activator-88b7df5c-cbxk8 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container activator ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: autoscaler-7bf8ff94db-98bhj from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.908: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 04:17:06.908: INFO: autoscaler-hpa-776c44cc57-rcqj4 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: controller-7fd644cbc6-dfb6n from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: domain-mapping-5bcd85fbc6-2gshd from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: domainmapping-webhook-77f466bbc9-97ft7 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: net-certmanager-controller-6c8cb88879-86vmg from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: net-certmanager-webhook-79cfb96f68-kfnlr from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: net-contour-controller-5c5fd89fdb-lnf94 from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: webhook-69d55f5549-ghp24 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: coredns-565d847f94-684lz from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: coredns-565d847f94-lzgv5 from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: kubed-568bdbc6c4-vx7m6 from kube-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: metrics-server-5cb46dccf6-w9d6b from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container weave ready: true, restart count 1
    Aug 25 04:17:06.909: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: local-path-provisioner-56c55476f9-xbkr9 from local-path-storage started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: controller-77cc7ff558-mrwjl from metallb-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: speaker-f82r7 from metallb-system started at 2022-08-25 03:25:09 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: distribution-7f9dff85c4-c276b from pair-system started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: environment-exposer-5bb6d44465-8btrx from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: powerdns-5c6dbcf579-rgzxs from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: powerdns-db-7c897fddf4-l5cjb from pair-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: public-html-go-http-server-55b9f584d4-4jrp5 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: reveal-multiplex-6bbbf59d6-k9lt8 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-8358 started at 2022-08-25 04:16:59 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: preemptor-pod from sched-preemption-8358 started at 2022-08-25 04:17:04 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container preemptor-pod ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 04:17:06.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:17:06.909: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/25/22 04:17:06.909
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.170e7b6e78950359], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.] 08/25/22 04:17:06.949
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:17:07.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1429" for this suite. 08/25/22 04:17:07.955
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:07.96
Aug 25 04:17:07.960: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 04:17:07.961
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:07.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:07.976
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Aug 25 04:17:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/25/22 04:17:12.118
Aug 25 04:17:12.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 create -f -'
Aug 25 04:17:13.067: INFO: stderr: ""
Aug 25 04:17:13.067: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 25 04:17:13.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 delete e2e-test-crd-publish-openapi-6503-crds test-cr'
Aug 25 04:17:13.150: INFO: stderr: ""
Aug 25 04:17:13.150: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 25 04:17:13.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 apply -f -'
Aug 25 04:17:13.414: INFO: stderr: ""
Aug 25 04:17:13.414: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 25 04:17:13.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 delete e2e-test-crd-publish-openapi-6503-crds test-cr'
Aug 25 04:17:13.497: INFO: stderr: ""
Aug 25 04:17:13.497: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/25/22 04:17:13.497
Aug 25 04:17:13.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 explain e2e-test-crd-publish-openapi-6503-crds'
Aug 25 04:17:13.757: INFO: stderr: ""
Aug 25 04:17:13.757: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6503-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:17:17.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5103" for this suite. 08/25/22 04:17:17.804
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":307,"skipped":5737,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [9.848 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:07.96
    Aug 25 04:17:07.960: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename crd-publish-openapi 08/25/22 04:17:07.961
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:07.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:07.976
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Aug 25 04:17:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/25/22 04:17:12.118
    Aug 25 04:17:12.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 create -f -'
    Aug 25 04:17:13.067: INFO: stderr: ""
    Aug 25 04:17:13.067: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 25 04:17:13.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 delete e2e-test-crd-publish-openapi-6503-crds test-cr'
    Aug 25 04:17:13.150: INFO: stderr: ""
    Aug 25 04:17:13.150: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 25 04:17:13.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 apply -f -'
    Aug 25 04:17:13.414: INFO: stderr: ""
    Aug 25 04:17:13.414: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 25 04:17:13.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 --namespace=crd-publish-openapi-5103 delete e2e-test-crd-publish-openapi-6503-crds test-cr'
    Aug 25 04:17:13.497: INFO: stderr: ""
    Aug 25 04:17:13.497: INFO: stdout: "e2e-test-crd-publish-openapi-6503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/25/22 04:17:13.497
    Aug 25 04:17:13.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=crd-publish-openapi-5103 explain e2e-test-crd-publish-openapi-6503-crds'
    Aug 25 04:17:13.757: INFO: stderr: ""
    Aug 25 04:17:13.757: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6503-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:17:17.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5103" for this suite. 08/25/22 04:17:17.804
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:17.81
Aug 25 04:17:17.810: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-runtime 08/25/22 04:17:17.811
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:17.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:17.827
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 08/25/22 04:17:17.831
STEP: wait for the container to reach Succeeded 08/25/22 04:17:17.838
STEP: get the container status 08/25/22 04:17:21.861
STEP: the container should be terminated 08/25/22 04:17:21.866
STEP: the termination message should be set 08/25/22 04:17:21.866
Aug 25 04:17:21.866: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/25/22 04:17:21.866
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 25 04:17:21.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6034" for this suite. 08/25/22 04:17:21.882
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":308,"skipped":5748,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [4.077 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:17.81
    Aug 25 04:17:17.810: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-runtime 08/25/22 04:17:17.811
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:17.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:17.827
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 08/25/22 04:17:17.831
    STEP: wait for the container to reach Succeeded 08/25/22 04:17:17.838
    STEP: get the container status 08/25/22 04:17:21.861
    STEP: the container should be terminated 08/25/22 04:17:21.866
    STEP: the termination message should be set 08/25/22 04:17:21.866
    Aug 25 04:17:21.866: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/25/22 04:17:21.866
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 25 04:17:21.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6034" for this suite. 08/25/22 04:17:21.882
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:21.888
Aug 25 04:17:21.889: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 04:17:21.89
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:21.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:21.907
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Aug 25 04:17:21.910: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: creating the pod 08/25/22 04:17:21.91
STEP: submitting the pod to kubernetes 08/25/22 04:17:21.91
Aug 25 04:17:21.916: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a" in namespace "pods-641" to be "running and ready"
Aug 25 04:17:21.919: INFO: Pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153645ms
Aug 25 04:17:21.919: INFO: The phase of Pod pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:17:23.924: INFO: Pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008576245s
Aug 25 04:17:23.924: INFO: The phase of Pod pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a is Running (Ready = true)
Aug 25 04:17:23.924: INFO: Pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 04:17:23.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-641" for this suite. 08/25/22 04:17:23.944
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":309,"skipped":5765,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [2.061 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:21.888
    Aug 25 04:17:21.889: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 04:17:21.89
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:21.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:21.907
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Aug 25 04:17:21.910: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: creating the pod 08/25/22 04:17:21.91
    STEP: submitting the pod to kubernetes 08/25/22 04:17:21.91
    Aug 25 04:17:21.916: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a" in namespace "pods-641" to be "running and ready"
    Aug 25 04:17:21.919: INFO: Pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153645ms
    Aug 25 04:17:21.919: INFO: The phase of Pod pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:17:23.924: INFO: Pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008576245s
    Aug 25 04:17:23.924: INFO: The phase of Pod pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a is Running (Ready = true)
    Aug 25 04:17:23.924: INFO: Pod "pod-logs-websocket-641ae5f9-31c8-4e09-91c4-76a46ae3bb4a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 04:17:23.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-641" for this suite. 08/25/22 04:17:23.944
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:23.95
Aug 25 04:17:23.950: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename replicaset 08/25/22 04:17:23.952
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:23.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:23.967
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/25/22 04:17:23.975
STEP: Verify that the required pods have come up. 08/25/22 04:17:23.979
Aug 25 04:17:23.983: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 25 04:17:28.987: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/25/22 04:17:28.987
STEP: Getting /status 08/25/22 04:17:28.987
Aug 25 04:17:28.991: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/25/22 04:17:28.991
Aug 25 04:17:28.999: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/25/22 04:17:28.999
Aug 25 04:17:29.001: INFO: Observed &ReplicaSet event: ADDED
Aug 25 04:17:29.001: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.002: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.002: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.002: INFO: Found replicaset test-rs in namespace replicaset-4310 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 25 04:17:29.002: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/25/22 04:17:29.002
Aug 25 04:17:29.002: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 25 04:17:29.009: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/25/22 04:17:29.009
Aug 25 04:17:29.011: INFO: Observed &ReplicaSet event: ADDED
Aug 25 04:17:29.011: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.011: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.012: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.012: INFO: Observed replicaset test-rs in namespace replicaset-4310 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 25 04:17:29.012: INFO: Observed &ReplicaSet event: MODIFIED
Aug 25 04:17:29.012: INFO: Found replicaset test-rs in namespace replicaset-4310 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 25 04:17:29.012: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 25 04:17:29.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4310" for this suite. 08/25/22 04:17:29.016
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":310,"skipped":5768,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [5.070 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:23.95
    Aug 25 04:17:23.950: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename replicaset 08/25/22 04:17:23.952
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:23.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:23.967
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/25/22 04:17:23.975
    STEP: Verify that the required pods have come up. 08/25/22 04:17:23.979
    Aug 25 04:17:23.983: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 25 04:17:28.987: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/25/22 04:17:28.987
    STEP: Getting /status 08/25/22 04:17:28.987
    Aug 25 04:17:28.991: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/25/22 04:17:28.991
    Aug 25 04:17:28.999: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/25/22 04:17:28.999
    Aug 25 04:17:29.001: INFO: Observed &ReplicaSet event: ADDED
    Aug 25 04:17:29.001: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.002: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.002: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.002: INFO: Found replicaset test-rs in namespace replicaset-4310 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 25 04:17:29.002: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/25/22 04:17:29.002
    Aug 25 04:17:29.002: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 25 04:17:29.009: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/25/22 04:17:29.009
    Aug 25 04:17:29.011: INFO: Observed &ReplicaSet event: ADDED
    Aug 25 04:17:29.011: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.011: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.012: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.012: INFO: Observed replicaset test-rs in namespace replicaset-4310 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 25 04:17:29.012: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 25 04:17:29.012: INFO: Found replicaset test-rs in namespace replicaset-4310 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 25 04:17:29.012: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 25 04:17:29.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4310" for this suite. 08/25/22 04:17:29.016
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:17:29.022
Aug 25 04:17:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 04:17:29.024
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:29.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:29.037
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/25/22 04:17:29.042
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6611;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6611;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +notcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_tcp@PTR;sleep 1; done
 08/25/22 04:17:29.053
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6611;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6611;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +notcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_tcp@PTR;sleep 1; done
 08/25/22 04:17:29.054
STEP: creating a pod to probe DNS 08/25/22 04:17:29.054
STEP: submitting the pod to kubernetes 08/25/22 04:17:29.054
Aug 25 04:17:29.061: INFO: Waiting up to 15m0s for pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f" in namespace "dns-6611" to be "running"
Aug 25 04:17:29.065: INFO: Pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45418ms
Aug 25 04:17:31.071: INFO: Pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009329837s
Aug 25 04:17:31.071: INFO: Pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f" satisfied condition "running"
STEP: retrieving the pod 08/25/22 04:17:31.071
STEP: looking for the results for each expected name from probers 08/25/22 04:17:31.075
Aug 25 04:17:31.079: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.083: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.086: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.092: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.095: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.101: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.117: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.121: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.123: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.126: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.129: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.138: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:31.151: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

Aug 25 04:17:36.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.162: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.168: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.171: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.190: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.193: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.196: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.199: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.202: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.208: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.211: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:36.223: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

Aug 25 04:17:41.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.162: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.169: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.175: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.178: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.193: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.197: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.200: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.203: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.206: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.212: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:41.227: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

Aug 25 04:17:46.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.161: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.170: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.173: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.176: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.192: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.195: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.198: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.201: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.204: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.212: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:46.225: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

Aug 25 04:17:51.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.157: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.163: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.169: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.172: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.175: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.195: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.199: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.202: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.206: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.213: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:51.223: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

Aug 25 04:17:56.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.169: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.175: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.192: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.195: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.198: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.201: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.204: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.213: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
Aug 25 04:17:56.224: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

Aug 25 04:18:01.231: INFO: DNS probes using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f succeeded

STEP: deleting the pod 08/25/22 04:18:01.231
STEP: deleting the test service 08/25/22 04:18:01.239
STEP: deleting the test headless service 08/25/22 04:18:01.25
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 04:18:01.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6611" for this suite. 08/25/22 04:18:01.259
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":311,"skipped":5792,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [32.240 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:17:29.022
    Aug 25 04:17:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 04:17:29.024
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:17:29.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:17:29.037
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/25/22 04:17:29.042
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6611;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6611;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +notcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_tcp@PTR;sleep 1; done
     08/25/22 04:17:29.053
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6611;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6611;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6611.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6611.svc;check="$$(dig +notcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.67.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.67.127_tcp@PTR;sleep 1; done
     08/25/22 04:17:29.054
    STEP: creating a pod to probe DNS 08/25/22 04:17:29.054
    STEP: submitting the pod to kubernetes 08/25/22 04:17:29.054
    Aug 25 04:17:29.061: INFO: Waiting up to 15m0s for pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f" in namespace "dns-6611" to be "running"
    Aug 25 04:17:29.065: INFO: Pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45418ms
    Aug 25 04:17:31.071: INFO: Pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009329837s
    Aug 25 04:17:31.071: INFO: Pod "dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 04:17:31.071
    STEP: looking for the results for each expected name from probers 08/25/22 04:17:31.075
    Aug 25 04:17:31.079: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.083: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.086: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.092: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.095: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.101: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.117: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.121: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.123: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.126: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.129: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.138: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:31.151: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

    Aug 25 04:17:36.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.162: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.168: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.171: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.190: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.193: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.196: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.199: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.202: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.208: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.211: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:36.223: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

    Aug 25 04:17:41.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.162: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.169: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.175: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.178: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.193: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.197: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.200: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.203: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.206: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.212: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:41.227: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

    Aug 25 04:17:46.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.161: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.170: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.173: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.176: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.192: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.195: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.198: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.201: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.204: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.212: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:46.225: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

    Aug 25 04:17:51.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.157: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.163: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.169: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.172: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.175: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.195: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.199: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.202: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.206: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.213: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:51.223: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

    Aug 25 04:17:56.155: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.159: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.169: INFO: Unable to read wheezy_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.175: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.192: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.195: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.198: INFO: Unable to read jessie_udp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.201: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611 from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.204: INFO: Unable to read jessie_udp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.213: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc from pod dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f: the server could not find the requested resource (get pods dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f)
    Aug 25 04:17:56.224: INFO: Lookups using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6611 wheezy_tcp@dns-test-service.dns-6611 wheezy_udp@dns-test-service.dns-6611.svc wheezy_tcp@dns-test-service.dns-6611.svc wheezy_udp@_http._tcp.dns-test-service.dns-6611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6611 jessie_tcp@dns-test-service.dns-6611 jessie_udp@dns-test-service.dns-6611.svc jessie_tcp@dns-test-service.dns-6611.svc jessie_udp@_http._tcp.dns-test-service.dns-6611.svc jessie_tcp@_http._tcp.dns-test-service.dns-6611.svc]

    Aug 25 04:18:01.231: INFO: DNS probes using dns-6611/dns-test-03b857ad-8e2e-48ab-b4db-f461e887860f succeeded

    STEP: deleting the pod 08/25/22 04:18:01.231
    STEP: deleting the test service 08/25/22 04:18:01.239
    STEP: deleting the test headless service 08/25/22 04:18:01.25
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 04:18:01.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6611" for this suite. 08/25/22 04:18:01.259
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:18:01.263
Aug 25 04:18:01.263: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:18:01.264
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:01.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:01.285
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-cded0030-6f1e-4ff2-af36-cc2d68c299ed 08/25/22 04:18:01.29
STEP: Creating a pod to test consume configMaps 08/25/22 04:18:01.294
Aug 25 04:18:01.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f" in namespace "projected-6800" to be "Succeeded or Failed"
Aug 25 04:18:01.303: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.450832ms
Aug 25 04:18:03.308: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008599524s
Aug 25 04:18:05.309: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009031633s
STEP: Saw pod success 08/25/22 04:18:05.309
Aug 25 04:18:05.309: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f" satisfied condition "Succeeded or Failed"
Aug 25 04:18:05.313: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/25/22 04:18:05.318
Aug 25 04:18:05.325: INFO: Waiting for pod pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f to disappear
Aug 25 04:18:05.329: INFO: Pod pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 04:18:05.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6800" for this suite. 08/25/22 04:18:05.333
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":312,"skipped":5825,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [4.074 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:18:01.263
    Aug 25 04:18:01.263: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:18:01.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:01.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:01.285
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-cded0030-6f1e-4ff2-af36-cc2d68c299ed 08/25/22 04:18:01.29
    STEP: Creating a pod to test consume configMaps 08/25/22 04:18:01.294
    Aug 25 04:18:01.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f" in namespace "projected-6800" to be "Succeeded or Failed"
    Aug 25 04:18:01.303: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.450832ms
    Aug 25 04:18:03.308: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008599524s
    Aug 25 04:18:05.309: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009031633s
    STEP: Saw pod success 08/25/22 04:18:05.309
    Aug 25 04:18:05.309: INFO: Pod "pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f" satisfied condition "Succeeded or Failed"
    Aug 25 04:18:05.313: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:18:05.318
    Aug 25 04:18:05.325: INFO: Waiting for pod pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f to disappear
    Aug 25 04:18:05.329: INFO: Pod pod-projected-configmaps-eb16e083-e158-4acf-942e-ba8252ffb65f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 04:18:05.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6800" for this suite. 08/25/22 04:18:05.333
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:18:05.338
Aug 25 04:18:05.338: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 04:18:05.339
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:05.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:05.353
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Aug 25 04:18:05.363: INFO: Waiting up to 5m0s for pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1" in namespace "pods-4688" to be "running and ready"
Aug 25 04:18:05.367: INFO: Pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848097ms
Aug 25 04:18:05.367: INFO: The phase of Pod server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:18:07.373: INFO: Pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010371575s
Aug 25 04:18:07.373: INFO: The phase of Pod server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1 is Running (Ready = true)
Aug 25 04:18:07.373: INFO: Pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1" satisfied condition "running and ready"
Aug 25 04:18:07.390: INFO: Waiting up to 5m0s for pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a" in namespace "pods-4688" to be "Succeeded or Failed"
Aug 25 04:18:07.394: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51241ms
Aug 25 04:18:09.399: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008701549s
Aug 25 04:18:11.400: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009421861s
STEP: Saw pod success 08/25/22 04:18:11.4
Aug 25 04:18:11.400: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a" satisfied condition "Succeeded or Failed"
Aug 25 04:18:11.404: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a container env3cont: <nil>
STEP: delete the pod 08/25/22 04:18:11.412
Aug 25 04:18:11.420: INFO: Waiting for pod client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a to disappear
Aug 25 04:18:11.424: INFO: Pod client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 25 04:18:11.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4688" for this suite. 08/25/22 04:18:11.428
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":313,"skipped":5826,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:18:05.338
    Aug 25 04:18:05.338: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 04:18:05.339
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:05.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:05.353
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Aug 25 04:18:05.363: INFO: Waiting up to 5m0s for pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1" in namespace "pods-4688" to be "running and ready"
    Aug 25 04:18:05.367: INFO: Pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848097ms
    Aug 25 04:18:05.367: INFO: The phase of Pod server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:18:07.373: INFO: Pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010371575s
    Aug 25 04:18:07.373: INFO: The phase of Pod server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1 is Running (Ready = true)
    Aug 25 04:18:07.373: INFO: Pod "server-envvars-3eece525-6200-479a-bb9d-6e5f64f288d1" satisfied condition "running and ready"
    Aug 25 04:18:07.390: INFO: Waiting up to 5m0s for pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a" in namespace "pods-4688" to be "Succeeded or Failed"
    Aug 25 04:18:07.394: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51241ms
    Aug 25 04:18:09.399: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008701549s
    Aug 25 04:18:11.400: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009421861s
    STEP: Saw pod success 08/25/22 04:18:11.4
    Aug 25 04:18:11.400: INFO: Pod "client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a" satisfied condition "Succeeded or Failed"
    Aug 25 04:18:11.404: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a container env3cont: <nil>
    STEP: delete the pod 08/25/22 04:18:11.412
    Aug 25 04:18:11.420: INFO: Waiting for pod client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a to disappear
    Aug 25 04:18:11.424: INFO: Pod client-envvars-7372cac1-d106-4d61-bafe-cf07f3ea029a no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 25 04:18:11.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4688" for this suite. 08/25/22 04:18:11.428
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:18:11.434
Aug 25 04:18:11.435: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 04:18:11.436
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:11.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:11.45
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/25/22 04:18:11.456
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local;sleep 1; done
 08/25/22 04:18:11.46
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local;sleep 1; done
 08/25/22 04:18:11.46
STEP: creating a pod to probe DNS 08/25/22 04:18:11.46
STEP: submitting the pod to kubernetes 08/25/22 04:18:11.46
Aug 25 04:18:11.467: INFO: Waiting up to 15m0s for pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd" in namespace "dns-5349" to be "running"
Aug 25 04:18:11.470: INFO: Pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8454ms
Aug 25 04:18:13.475: INFO: Pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007597626s
Aug 25 04:18:13.475: INFO: Pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd" satisfied condition "running"
STEP: retrieving the pod 08/25/22 04:18:13.475
STEP: looking for the results for each expected name from probers 08/25/22 04:18:13.479
Aug 25 04:18:13.483: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.491: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.494: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.498: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.500: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.504: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.506: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.509: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:13.509: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

Aug 25 04:18:18.515: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.518: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.521: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.524: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.527: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.534: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.536: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:18.536: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

Aug 25 04:18:23.516: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.522: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.525: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.528: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.529: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.533: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.534: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:23.534: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

Aug 25 04:18:28.515: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.519: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.522: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.525: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.528: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.534: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.537: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:28.537: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

Aug 25 04:18:33.514: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.518: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.521: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.524: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.527: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.529: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.532: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.535: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:33.535: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

Aug 25 04:18:38.516: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.519: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.522: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.525: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.528: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.530: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.533: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.536: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
Aug 25 04:18:38.536: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

Aug 25 04:18:43.534: INFO: DNS probes using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd succeeded

STEP: deleting the pod 08/25/22 04:18:43.535
STEP: deleting the test headless service 08/25/22 04:18:43.547
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 04:18:43.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5349" for this suite. 08/25/22 04:18:43.576
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":314,"skipped":5854,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [32.146 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:18:11.434
    Aug 25 04:18:11.435: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 04:18:11.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:11.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:11.45
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/25/22 04:18:11.456
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local;sleep 1; done
     08/25/22 04:18:11.46
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local;sleep 1; done
     08/25/22 04:18:11.46
    STEP: creating a pod to probe DNS 08/25/22 04:18:11.46
    STEP: submitting the pod to kubernetes 08/25/22 04:18:11.46
    Aug 25 04:18:11.467: INFO: Waiting up to 15m0s for pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd" in namespace "dns-5349" to be "running"
    Aug 25 04:18:11.470: INFO: Pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8454ms
    Aug 25 04:18:13.475: INFO: Pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007597626s
    Aug 25 04:18:13.475: INFO: Pod "dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 04:18:13.475
    STEP: looking for the results for each expected name from probers 08/25/22 04:18:13.479
    Aug 25 04:18:13.483: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.491: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.494: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.498: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.500: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.504: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.506: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.509: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:13.509: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

    Aug 25 04:18:18.515: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.518: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.521: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.524: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.527: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.534: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.536: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:18.536: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

    Aug 25 04:18:23.516: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.522: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.525: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.528: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.529: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.533: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.534: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:23.534: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

    Aug 25 04:18:28.515: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.519: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.522: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.525: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.528: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.531: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.534: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.537: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:28.537: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

    Aug 25 04:18:33.514: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.518: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.521: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.524: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.527: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.529: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.532: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.535: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:33.535: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

    Aug 25 04:18:38.516: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.519: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.522: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.525: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.528: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.530: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.533: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.536: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local from pod dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd: the server could not find the requested resource (get pods dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd)
    Aug 25 04:18:38.536: INFO: Lookups using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5349.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5349.svc.cluster.local jessie_udp@dns-test-service-2.dns-5349.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5349.svc.cluster.local]

    Aug 25 04:18:43.534: INFO: DNS probes using dns-5349/dns-test-d85afee7-bd5d-45d5-b286-980b4f0da6fd succeeded

    STEP: deleting the pod 08/25/22 04:18:43.535
    STEP: deleting the test headless service 08/25/22 04:18:43.547
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 04:18:43.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5349" for this suite. 08/25/22 04:18:43.576
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:18:43.584
Aug 25 04:18:43.584: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 04:18:43.585
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:43.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:43.61
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 08/25/22 04:18:43.614
STEP: Ensuring ResourceQuota status is calculated 08/25/22 04:18:43.618
STEP: Creating a ResourceQuota with not best effort scope 08/25/22 04:18:45.622
STEP: Ensuring ResourceQuota status is calculated 08/25/22 04:18:45.627
STEP: Creating a best-effort pod 08/25/22 04:18:47.633
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/25/22 04:18:47.645
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/25/22 04:18:49.65
STEP: Deleting the pod 08/25/22 04:18:51.655
STEP: Ensuring resource quota status released the pod usage 08/25/22 04:18:51.663
STEP: Creating a not best-effort pod 08/25/22 04:18:53.668
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/25/22 04:18:53.679
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/25/22 04:18:55.683
STEP: Deleting the pod 08/25/22 04:18:57.693
STEP: Ensuring resource quota status released the pod usage 08/25/22 04:18:57.7
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 04:18:59.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6692" for this suite. 08/25/22 04:18:59.716
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":315,"skipped":5915,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [16.137 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:18:43.584
    Aug 25 04:18:43.584: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 04:18:43.585
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:43.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:43.61
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 08/25/22 04:18:43.614
    STEP: Ensuring ResourceQuota status is calculated 08/25/22 04:18:43.618
    STEP: Creating a ResourceQuota with not best effort scope 08/25/22 04:18:45.622
    STEP: Ensuring ResourceQuota status is calculated 08/25/22 04:18:45.627
    STEP: Creating a best-effort pod 08/25/22 04:18:47.633
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/25/22 04:18:47.645
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/25/22 04:18:49.65
    STEP: Deleting the pod 08/25/22 04:18:51.655
    STEP: Ensuring resource quota status released the pod usage 08/25/22 04:18:51.663
    STEP: Creating a not best-effort pod 08/25/22 04:18:53.668
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/25/22 04:18:53.679
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/25/22 04:18:55.683
    STEP: Deleting the pod 08/25/22 04:18:57.693
    STEP: Ensuring resource quota status released the pod usage 08/25/22 04:18:57.7
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 04:18:59.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6692" for this suite. 08/25/22 04:18:59.716
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:18:59.722
Aug 25 04:18:59.723: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:18:59.724
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:59.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:59.74
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 08/25/22 04:18:59.744
Aug 25 04:18:59.750: INFO: Waiting up to 5m0s for pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321" in namespace "downward-api-1494" to be "Succeeded or Failed"
Aug 25 04:18:59.753: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870035ms
Aug 25 04:19:01.759: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008157291s
Aug 25 04:19:03.760: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009224496s
STEP: Saw pod success 08/25/22 04:19:03.76
Aug 25 04:19:03.760: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321" satisfied condition "Succeeded or Failed"
Aug 25 04:19:03.764: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321 container dapi-container: <nil>
STEP: delete the pod 08/25/22 04:19:03.769
Aug 25 04:19:03.776: INFO: Waiting for pod downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321 to disappear
Aug 25 04:19:03.779: INFO: Pod downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 25 04:19:03.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1494" for this suite. 08/25/22 04:19:03.784
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":316,"skipped":5930,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:18:59.722
    Aug 25 04:18:59.723: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:18:59.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:18:59.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:18:59.74
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 08/25/22 04:18:59.744
    Aug 25 04:18:59.750: INFO: Waiting up to 5m0s for pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321" in namespace "downward-api-1494" to be "Succeeded or Failed"
    Aug 25 04:18:59.753: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870035ms
    Aug 25 04:19:01.759: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008157291s
    Aug 25 04:19:03.760: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009224496s
    STEP: Saw pod success 08/25/22 04:19:03.76
    Aug 25 04:19:03.760: INFO: Pod "downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321" satisfied condition "Succeeded or Failed"
    Aug 25 04:19:03.764: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 04:19:03.769
    Aug 25 04:19:03.776: INFO: Waiting for pod downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321 to disappear
    Aug 25 04:19:03.779: INFO: Pod downward-api-e6bc67bd-5ea7-482c-b6a5-6d54ab32d321 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 25 04:19:03.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1494" for this suite. 08/25/22 04:19:03.784
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:03.792
Aug 25 04:19:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 04:19:03.793
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:03.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:03.807
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/25/22 04:19:03.817
Aug 25 04:19:03.826: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8903" to be "running and ready"
Aug 25 04:19:03.829: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921245ms
Aug 25 04:19:03.829: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:19:05.834: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007482614s
Aug 25 04:19:05.834: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 25 04:19:05.834: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 08/25/22 04:19:05.838
Aug 25 04:19:05.844: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8903" to be "running and ready"
Aug 25 04:19:05.847: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319978ms
Aug 25 04:19:05.847: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:19:07.852: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008317724s
Aug 25 04:19:07.852: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 25 04:19:07.852: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/25/22 04:19:07.856
Aug 25 04:19:07.862: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 25 04:19:07.867: INFO: Pod pod-with-prestop-http-hook still exists
Aug 25 04:19:09.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 25 04:19:09.873: INFO: Pod pod-with-prestop-http-hook still exists
Aug 25 04:19:11.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 25 04:19:11.872: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/25/22 04:19:11.872
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 25 04:19:11.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8903" for this suite. 08/25/22 04:19:11.882
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":317,"skipped":5971,"failed":2,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
------------------------------
• [SLOW TEST] [8.096 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:03.792
    Aug 25 04:19:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 04:19:03.793
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:03.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:03.807
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/25/22 04:19:03.817
    Aug 25 04:19:03.826: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8903" to be "running and ready"
    Aug 25 04:19:03.829: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921245ms
    Aug 25 04:19:03.829: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:19:05.834: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007482614s
    Aug 25 04:19:05.834: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 25 04:19:05.834: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 08/25/22 04:19:05.838
    Aug 25 04:19:05.844: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8903" to be "running and ready"
    Aug 25 04:19:05.847: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319978ms
    Aug 25 04:19:05.847: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:19:07.852: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008317724s
    Aug 25 04:19:07.852: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 25 04:19:07.852: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/25/22 04:19:07.856
    Aug 25 04:19:07.862: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 25 04:19:07.867: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 25 04:19:09.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 25 04:19:09.873: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 25 04:19:11.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 25 04:19:11.872: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/25/22 04:19:11.872
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 25 04:19:11.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8903" for this suite. 08/25/22 04:19:11.882
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:11.89
Aug 25 04:19:11.890: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename conformance-tests 08/25/22 04:19:11.891
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:11.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:11.906
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/25/22 04:19:11.91
Aug 25 04:19:11.911: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 25 04:19:11.918: FAIL: Conformance requires at least two nodes

Full Stack Trace

[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Collecting events from namespace "conformance-tests-243". 08/25/22 04:19:11.918
STEP: Found 0 events. 08/25/22 04:19:11.922
Aug 25 04:19:11.925: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Aug 25 04:19:11.925: INFO: 
Aug 25 04:19:11.929: INFO: 
Logging node info for node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:19:11.932: INFO: Node Info: &Node{ObjectMeta:{bobymcbobs-c849-control-plane-p55pp    c5dc1f57-bb1e-4562-b424-f31ab182c240 54230 0 2022-08-25 02:40:28 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:bobymcbobs-c849-control-plane-p55pp kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[cluster.x-k8s.io/cluster-name:bobymcbobs-c849 cluster.x-k8s.io/cluster-namespace:sharingio-pair cluster.x-k8s.io/machine:bobymcbobs-c849-control-plane-fmvtt cluster.x-k8s.io/owner-kind:KubeadmControlPlane cluster.x-k8s.io/owner-name:bobymcbobs-c849-control-plane kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2022-08-25 02:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}}} } {kubeadm Update v1 2022-08-25 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2022-08-25 02:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}} } {kube-utils Update v1 2022-08-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:spec":{"f:providerID":{}}} } {manager Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cluster.x-k8s.io/cluster-name":{},"f:cluster.x-k8s.io/cluster-namespace":{},"f:cluster.x-k8s.io/machine":{},"f:cluster.x-k8s.io/owner-kind":{},"f:cluster.x-k8s.io/owner-name":{}}}} } {e2e.test Update v1 2022-08-25 04:16:58 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}}}} status} {kubelet Update v1 2022-08-25 04:16:59 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{}}} status}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{234139500544 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404313366528 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Allocatable:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{210725550141 0} {<nil>} 210725550141 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404208508928 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-08-25 02:40:57 +0000 UTC,LastTransitionTime:2022-08-25 02:40:57 +0000 UTC,Reason:WeaveIsUp,Message:Weave pod has set this,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:52 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:86.109.11.217,},NodeAddress{Type:Hostname,Address:bobymcbobs-c849-control-plane-p55pp,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4b0c7bda23ca4aa6a943cda55f9fa1e2,SystemUUID:4c4c4544-0053-4710-8038-b8c04f483033,BootID:217f066f-7295-498b-9b6e-db50783a6a97,KernelVersion:5.4.0-109-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:containerd://1.6.7,KubeletVersion:v1.25.0,KubeProxyVersion:v1.25.0,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.gitlab.com/sharingio/environment/environment@sha256:37f277951643b2b0e70c1a7c72b1fa03fd7adca706c6e10202ac6a9ea81d1a45 registry.gitlab.com/sharingio/environment/environment:2022.07.25.1600],SizeBytes:2667236715,},ContainerImage{Names:[registry.gitlab.com/ii/nz/reveal-multiplex@sha256:299c977cc6734bf88fa5bdc22f8f6d464f0435437baee92aef5b33534d5ccf1f registry.gitlab.com/ii/nz/reveal-multiplex:latest],SizeBytes:216223066,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e registry.k8s.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:112030526,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:6f72b851544986cb0921b53ea655ec04c36131248f16d4ad110cb3ca0c369dc1 registry.k8s.io/etcd:3.5.4-0],SizeBytes:102157811,},ContainerImage{Names:[docker.io/linuxserver/mariadb@sha256:bceed8b46f875fd8c5fcf60d9201e1e8093f8ab181234d2bf3824c6621b897b6 docker.io/linuxserver/mariadb:alpine-version-10.5.12-r0],SizeBytes:89567136,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:8b45c5f7ab38f1ae9847e81f91aafa78d9905ae37cdae6fe3edaa43f3e6dfdfa registry.k8s.io/conformance:v1.25.0],SizeBytes:78887805,},ContainerImage{Names:[docker.io/fluxcd/helm-operator@sha256:8e63dcdbe0ce672215e1946bd64c88a362194dad901420632f529a295c2b8068 docker.io/fluxcd/helm-operator:1.4.0],SizeBytes:77258104,},ContainerImage{Names:[docker.io/envoyproxy/envoy@sha256:1f343072a58e74644b7adc8d2d877071f846fc77166295a6d2686aee6cf58162 docker.io/envoyproxy/envoy:v1.22.2],SizeBytes:51349232,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c registry.k8s.io/e2e-test-images/nautilus:1.5],SizeBytes:49642095,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 registry.k8s.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:41902010,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[quay.io/metallb/speaker@sha256:58cb99053bb33db1944f8b67c9d3335afe98c8ba810fca45f19cfde1442064c4 quay.io/metallb/speaker:v0.10.2],SizeBytes:39339636,},ContainerImage{Names:[docker.io/pschiffe/pdns-mysql@sha256:f9cabc76207919fcd82bcc1243df2ca510f89e61f6f5d2e2f54541e2b7d65d24 docker.io/pschiffe/pdns-mysql:4.3-alpine],SizeBytes:36903432,},ContainerImage{Names:[quay.io/metallb/controller@sha256:21164241c045443595439f2821a193ef823dbbd1ccc8959e1c363c44cc6d1e18 quay.io/metallb/controller:v0.10.2],SizeBytes:36020654,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:f6902791fb9aa6e283ed7d1d743417b3c425eec73151517813bef1539a66aefa registry.k8s.io/kube-apiserver:v1.25.0],SizeBytes:34227200,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:66ce7d460e53f942bb4729f656d66fe475ec3d41728de986b6d790eee6d8205d registry.k8s.io/kube-controller-manager:v1.25.0],SizeBytes:31259842,},ContainerImage{Names:[docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63 docker.io/weaveworks/weave-kube:2.8.1],SizeBytes:30924173,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[docker.io/appscode/kubed@sha256:d694910be47b07f941e44cf60514aa5284944b1271a456e51c45208fae296ee9 docker.io/appscode/kubed:v0.12.0],SizeBytes:25358728,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/sample-apiserver@sha256:f9c93b92b6ff750b41a93c4e4fe0bfe384597aeb841e2539d5444815c55b2d8f registry.k8s.io/e2e-test-images/sample-apiserver:1.17.5],SizeBytes:24316368,},ContainerImage{Names:[k8s.gcr.io/external-dns/external-dns@sha256:d9569d17d224513f252520d3c92d6519aedc40524f4395a9185c7e66de3cdab7 k8s.gcr.io/external-dns/external-dns:v0.10.0],SizeBytes:21974457,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:1b1f3456bb19866aa1655c607514b85cd2b6efdfea4d93ea55e79475ff2765f9 registry.k8s.io/kube-proxy:v1.25.0],SizeBytes:20262263,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c],SizeBytes:18081586,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/operator/cmd/operator@sha256:2cbbbe9a7873fc9414a09c3ed558dde13a8105e70e4c83300e3a0a820534897f],SizeBytes:17654221,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:c2ac18db63de4982496e6c8650e20cecf4dd32172516b5934d051c7c084063da docker.io/sonobuoy/sonobuoy:v0.56.10],SizeBytes:17407841,},ContainerImage{Names:[quay.io/jetstack/cert-manager-controller@sha256:51027a4cc4d30e197e3506daf3a4fa2d2a0bc2826469f8a87848dfd279e031c0 quay.io/jetstack/cert-manager-controller:v1.7.1],SizeBytes:17112219,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2],SizeBytes:16259613,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7],SizeBytes:16056815,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:5526e650784165aa2ed2af1846e0e8bf37d5c52815ad6865bbe5d99d78eca32e],SizeBytes:16004457,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b],SizeBytes:15960524,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb],SizeBytes:15874009,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/controller@sha256:08409982c2dda0992cc16c88a9f16dab4efff54a29deb3072617fec6efcef6c4],SizeBytes:15830636,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:9330c53feca7b51b25e427fa96afd5d1460b3233e9fa92e20c895c067da56ac1 registry.k8s.io/kube-scheduler:v1.25.0],SizeBytes:15793363,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-contour/cmd/controller@sha256:baf500920abd51e33c7497799f5b241560783ad9a3bb6fec746770eb734d488a],SizeBytes:15760951,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0],SizeBytes:15432856,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/webhook@sha256:31da8a42d9f28999bdeeea4a0a312e0e6c52ab1f18c97c88a7e2c27627482835],SizeBytes:15312462,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a registry.k8s.io/coredns/coredns:v1.9.3],SizeBytes:14837849,},ContainerImage{Names:[quay.io/jetstack/cert-manager-webhook@sha256:a926d60b6f23553ca5d11ac9cd66bcc692136e838613c8bc0d60c6c35a3cbcfc quay.io/jetstack/cert-manager-webhook:v1.7.1],SizeBytes:13636913,},ContainerImage{Names:[docker.io/rancher/local-path-provisioner@sha256:9666b1635fec95d4e2251661e135c90678b8f45fd0f8324c55db99c80e2a958c docker.io/rancher/local-path-provisioner:v0.0.19],SizeBytes:13585626,},ContainerImage{Names:[ghcr.io/projectcontour/contour@sha256:c64e1f72fad4b495d6e0967c2b3063c06ba2d79c9098331ba0219094204023c7 ghcr.io/projectcontour/contour:v1.21.1],SizeBytes:13407229,},ContainerImage{Names:[docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c docker.io/weaveworks/weave-npc:2.8.1],SizeBytes:12814131,},ContainerImage{Names:[quay.io/jetstack/cert-manager-cainjector@sha256:985743eeed2b62f68ee06e583f1d5a371e1c35af4b1980a1b2571d29174cce47 quay.io/jetstack/cert-manager-cainjector:v1.7.1],SizeBytes:12094880,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/pkg/apiextensions/storageversion/cmd/migrate@sha256:b333601675dc5f8349b3133bd9fcc7a620db0a5076892ea00338b19fc1e073fb],SizeBytes:11564327,},ContainerImage{Names:[docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96 docker.io/library/registry:2.8.1],SizeBytes:9204128,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exposer@sha256:d5234c7aee6d1281a72b4b1f09124a662ca230ece854def9449c988390f29897 registry.gitlab.com/sharingio/environment/exposer:2022.07.25.1600],SizeBytes:8641265,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 registry.k8s.io/e2e-test-images/nginx:1.14-2],SizeBytes:6979041,},ContainerImage{Names:[registry.gitlab.com/safesurfer/go-http-server@sha256:5a092b5372f23ec18117411b63ffd60f73dce967204087b24cc4b65b97e2c0b4 registry.gitlab.com/safesurfer/go-http-server:1.6.0],SizeBytes:4512154,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac registry.k8s.io/e2e-test-images/nonewprivs:1.3],SizeBytes:3263463,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Aug 25 04:19:11.933: INFO: 
Logging kubelet events for node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:19:11.936: INFO: 
Logging pods the kubelet thinks is on node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:19:11.964: INFO: powerdns-db-7c897fddf4-l5cjb started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 04:19:11.964: INFO: coredns-565d847f94-684lz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:19:11.964: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 04:19:11.964: INFO: knative-operator-594876444d-qzjs4 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 04:19:11.964: INFO: domain-mapping-5bcd85fbc6-2gshd started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 04:19:11.964: INFO: autoscaler-7bf8ff94db-98bhj started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 04:19:11.964: INFO: cert-manager-cainjector-6495667ff4-x6lll started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:19:11.964: INFO: sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 04:19:11.964: INFO: external-dns-67d79cbcd4-bf29h started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 04:19:11.964: INFO: etcd-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container etcd ready: true, restart count 0
Aug 25 04:19:11.964: INFO: sonobuoy-e2e-job-39386ef5d0694a49 started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container e2e ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:19:11.964: INFO: speaker-f82r7 started at 2022-08-25 03:25:09 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container speaker ready: true, restart count 0
Aug 25 04:19:11.964: INFO: contour-588fc6cc6d-f66zz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:19:11.964: INFO: controller-7fd644cbc6-dfb6n started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:19:11.964: INFO: activator-88b7df5c-cbxk8 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container activator ready: true, restart count 0
Aug 25 04:19:11.964: INFO: contour-588fc6cc6d-dq6wf started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:19:11.964: INFO: coredns-565d847f94-lzgv5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:19:11.964: INFO: environment-0 started at 2022-08-25 03:25:19 +0000 UTC (0+2 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container environment ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 04:19:11.964: INFO: helm-operator-7959478576-6mcjz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 04:19:11.964: INFO: webhook-69d55f5549-ghp24 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:19:11.964: INFO: local-path-provisioner-56c55476f9-xbkr9 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 04:19:11.964: INFO: powerdns-5c6dbcf579-rgzxs started at 2022-08-25 03:24:37 +0000 UTC (1+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Init container powerdns-init-db ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 04:19:11.964: INFO: reveal-multiplex-6bbbf59d6-k9lt8 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 04:19:11.964: INFO: weave-net-75fql started at 2022-08-25 02:40:45 +0000 UTC (1+2 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Init container weave-init ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container weave ready: true, restart count 1
Aug 25 04:19:11.964: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 04:19:11.964: INFO: pod-handle-http-request started at 2022-08-25 04:19:03 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 25 04:19:11.964: INFO: net-certmanager-controller-6c8cb88879-86vmg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:19:11.964: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 04:19:11.964: INFO: domainmapping-webhook-77f466bbc9-97ft7 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 04:19:11.964: INFO: net-certmanager-webhook-79cfb96f68-kfnlr started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:19:11.964: INFO: public-html-go-http-server-55b9f584d4-4jrp5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 04:19:11.964: INFO: kubed-568bdbc6c4-vx7m6 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container kubed ready: true, restart count 0
Aug 25 04:19:11.964: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:19:11.964: INFO: envoy-7sxnc started at 2022-08-25 03:25:38 +0000 UTC (1+2 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Init container envoy-initconfig ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:19:11.964: INFO: contour-8597b78798-lb4wb started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:19:11.964: INFO: contour-8597b78798-5755z started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:19:11.964: INFO: environment-exposer-5bb6d44465-8btrx started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 04:19:11.964: INFO: cert-manager-66bd77df8f-qlhqg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:19:11.964: INFO: kube-proxy-b4lgh started at 2022-08-25 02:40:45 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 04:19:11.964: INFO: envoy-cxlbl started at 2022-08-25 03:25:39 +0000 UTC (1+2 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Init container envoy-initconfig ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:19:11.964: INFO: autoscaler-hpa-776c44cc57-rcqj4 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 04:19:11.964: INFO: metrics-server-5cb46dccf6-w9d6b started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 04:19:11.964: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:19:11.964: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 25 04:19:11.964: INFO: controller-77cc7ff558-mrwjl started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:19:11.964: INFO: net-contour-controller-5c5fd89fdb-lnf94 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:19:11.964: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 04:19:11.964: INFO: distribution-7f9dff85c4-c276b started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
Aug 25 04:19:11.964: INFO: 	Container distribution ready: true, restart count 0
Aug 25 04:19:12.080: INFO: 
Latency metrics for node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:19:12.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-243" for this suite. 08/25/22 04:19:12.085
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"FAILED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":317,"skipped":5993,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [FAILED] [0.201 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  [It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:11.89
    Aug 25 04:19:11.890: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename conformance-tests 08/25/22 04:19:11.891
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:11.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:11.906
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/25/22 04:19:11.91
    Aug 25 04:19:11.911: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 25 04:19:11.918: FAIL: Conformance requires at least two nodes

    Full Stack Trace

    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    STEP: Collecting events from namespace "conformance-tests-243". 08/25/22 04:19:11.918
    STEP: Found 0 events. 08/25/22 04:19:11.922
    Aug 25 04:19:11.925: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
    Aug 25 04:19:11.925: INFO: 
    Aug 25 04:19:11.929: INFO: 
    Logging node info for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:19:11.932: INFO: Node Info: &Node{ObjectMeta:{bobymcbobs-c849-control-plane-p55pp    c5dc1f57-bb1e-4562-b424-f31ab182c240 54230 0 2022-08-25 02:40:28 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:bobymcbobs-c849-control-plane-p55pp kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[cluster.x-k8s.io/cluster-name:bobymcbobs-c849 cluster.x-k8s.io/cluster-namespace:sharingio-pair cluster.x-k8s.io/machine:bobymcbobs-c849-control-plane-fmvtt cluster.x-k8s.io/owner-kind:KubeadmControlPlane cluster.x-k8s.io/owner-name:bobymcbobs-c849-control-plane kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2022-08-25 02:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}}} } {kubeadm Update v1 2022-08-25 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2022-08-25 02:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}} } {kube-utils Update v1 2022-08-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:spec":{"f:providerID":{}}} } {manager Update v1 2022-08-25 02:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cluster.x-k8s.io/cluster-name":{},"f:cluster.x-k8s.io/cluster-namespace":{},"f:cluster.x-k8s.io/machine":{},"f:cluster.x-k8s.io/owner-kind":{},"f:cluster.x-k8s.io/owner-name":{}}}} } {e2e.test Update v1 2022-08-25 04:16:58 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}}}} status} {kubelet Update v1 2022-08-25 04:16:59 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{}}} status}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:equinixmetal://c1ed2964-4a90-4582-970e-56709955e2f3,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{234139500544 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404313366528 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Allocatable:ResourceList{cpu: {{64 0} {<nil>} 64 DecimalSI},ephemeral-storage: {{210725550141 0} {<nil>} 210725550141 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{404208508928 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{5 0} {<nil>} 5 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-08-25 02:40:57 +0000 UTC,LastTransitionTime:2022-08-25 02:40:57 +0000 UTC,Reason:WeaveIsUp,Message:Weave pod has set this,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-08-25 04:16:59 +0000 UTC,LastTransitionTime:2022-08-25 02:40:52 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:86.109.11.217,},NodeAddress{Type:Hostname,Address:bobymcbobs-c849-control-plane-p55pp,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4b0c7bda23ca4aa6a943cda55f9fa1e2,SystemUUID:4c4c4544-0053-4710-8038-b8c04f483033,BootID:217f066f-7295-498b-9b6e-db50783a6a97,KernelVersion:5.4.0-109-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:containerd://1.6.7,KubeletVersion:v1.25.0,KubeProxyVersion:v1.25.0,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.gitlab.com/sharingio/environment/environment@sha256:37f277951643b2b0e70c1a7c72b1fa03fd7adca706c6e10202ac6a9ea81d1a45 registry.gitlab.com/sharingio/environment/environment:2022.07.25.1600],SizeBytes:2667236715,},ContainerImage{Names:[registry.gitlab.com/ii/nz/reveal-multiplex@sha256:299c977cc6734bf88fa5bdc22f8f6d464f0435437baee92aef5b33534d5ccf1f registry.gitlab.com/ii/nz/reveal-multiplex:latest],SizeBytes:216223066,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e registry.k8s.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:112030526,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:6f72b851544986cb0921b53ea655ec04c36131248f16d4ad110cb3ca0c369dc1 registry.k8s.io/etcd:3.5.4-0],SizeBytes:102157811,},ContainerImage{Names:[docker.io/linuxserver/mariadb@sha256:bceed8b46f875fd8c5fcf60d9201e1e8093f8ab181234d2bf3824c6621b897b6 docker.io/linuxserver/mariadb:alpine-version-10.5.12-r0],SizeBytes:89567136,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:8b45c5f7ab38f1ae9847e81f91aafa78d9905ae37cdae6fe3edaa43f3e6dfdfa registry.k8s.io/conformance:v1.25.0],SizeBytes:78887805,},ContainerImage{Names:[docker.io/fluxcd/helm-operator@sha256:8e63dcdbe0ce672215e1946bd64c88a362194dad901420632f529a295c2b8068 docker.io/fluxcd/helm-operator:1.4.0],SizeBytes:77258104,},ContainerImage{Names:[docker.io/envoyproxy/envoy@sha256:1f343072a58e74644b7adc8d2d877071f846fc77166295a6d2686aee6cf58162 docker.io/envoyproxy/envoy:v1.22.2],SizeBytes:51349232,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c registry.k8s.io/e2e-test-images/nautilus:1.5],SizeBytes:49642095,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 registry.k8s.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:41902010,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[quay.io/metallb/speaker@sha256:58cb99053bb33db1944f8b67c9d3335afe98c8ba810fca45f19cfde1442064c4 quay.io/metallb/speaker:v0.10.2],SizeBytes:39339636,},ContainerImage{Names:[docker.io/pschiffe/pdns-mysql@sha256:f9cabc76207919fcd82bcc1243df2ca510f89e61f6f5d2e2f54541e2b7d65d24 docker.io/pschiffe/pdns-mysql:4.3-alpine],SizeBytes:36903432,},ContainerImage{Names:[quay.io/metallb/controller@sha256:21164241c045443595439f2821a193ef823dbbd1ccc8959e1c363c44cc6d1e18 quay.io/metallb/controller:v0.10.2],SizeBytes:36020654,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:f6902791fb9aa6e283ed7d1d743417b3c425eec73151517813bef1539a66aefa registry.k8s.io/kube-apiserver:v1.25.0],SizeBytes:34227200,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:66ce7d460e53f942bb4729f656d66fe475ec3d41728de986b6d790eee6d8205d registry.k8s.io/kube-controller-manager:v1.25.0],SizeBytes:31259842,},ContainerImage{Names:[docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63 docker.io/weaveworks/weave-kube:2.8.1],SizeBytes:30924173,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[docker.io/appscode/kubed@sha256:d694910be47b07f941e44cf60514aa5284944b1271a456e51c45208fae296ee9 docker.io/appscode/kubed:v0.12.0],SizeBytes:25358728,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/sample-apiserver@sha256:f9c93b92b6ff750b41a93c4e4fe0bfe384597aeb841e2539d5444815c55b2d8f registry.k8s.io/e2e-test-images/sample-apiserver:1.17.5],SizeBytes:24316368,},ContainerImage{Names:[k8s.gcr.io/external-dns/external-dns@sha256:d9569d17d224513f252520d3c92d6519aedc40524f4395a9185c7e66de3cdab7 k8s.gcr.io/external-dns/external-dns:v0.10.0],SizeBytes:21974457,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:1b1f3456bb19866aa1655c607514b85cd2b6efdfea4d93ea55e79475ff2765f9 registry.k8s.io/kube-proxy:v1.25.0],SizeBytes:20262263,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c],SizeBytes:18081586,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/operator/cmd/operator@sha256:2cbbbe9a7873fc9414a09c3ed558dde13a8105e70e4c83300e3a0a820534897f],SizeBytes:17654221,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:c2ac18db63de4982496e6c8650e20cecf4dd32172516b5934d051c7c084063da docker.io/sonobuoy/sonobuoy:v0.56.10],SizeBytes:17407841,},ContainerImage{Names:[quay.io/jetstack/cert-manager-controller@sha256:51027a4cc4d30e197e3506daf3a4fa2d2a0bc2826469f8a87848dfd279e031c0 quay.io/jetstack/cert-manager-controller:v1.7.1],SizeBytes:17112219,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2],SizeBytes:16259613,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7],SizeBytes:16056815,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:5526e650784165aa2ed2af1846e0e8bf37d5c52815ad6865bbe5d99d78eca32e],SizeBytes:16004457,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b],SizeBytes:15960524,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb],SizeBytes:15874009,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/controller@sha256:08409982c2dda0992cc16c88a9f16dab4efff54a29deb3072617fec6efcef6c4],SizeBytes:15830636,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:9330c53feca7b51b25e427fa96afd5d1460b3233e9fa92e20c895c067da56ac1 registry.k8s.io/kube-scheduler:v1.25.0],SizeBytes:15793363,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-contour/cmd/controller@sha256:baf500920abd51e33c7497799f5b241560783ad9a3bb6fec746770eb734d488a],SizeBytes:15760951,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0],SizeBytes:15432856,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/net-certmanager/cmd/webhook@sha256:31da8a42d9f28999bdeeea4a0a312e0e6c52ab1f18c97c88a7e2c27627482835],SizeBytes:15312462,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a registry.k8s.io/coredns/coredns:v1.9.3],SizeBytes:14837849,},ContainerImage{Names:[quay.io/jetstack/cert-manager-webhook@sha256:a926d60b6f23553ca5d11ac9cd66bcc692136e838613c8bc0d60c6c35a3cbcfc quay.io/jetstack/cert-manager-webhook:v1.7.1],SizeBytes:13636913,},ContainerImage{Names:[docker.io/rancher/local-path-provisioner@sha256:9666b1635fec95d4e2251661e135c90678b8f45fd0f8324c55db99c80e2a958c docker.io/rancher/local-path-provisioner:v0.0.19],SizeBytes:13585626,},ContainerImage{Names:[ghcr.io/projectcontour/contour@sha256:c64e1f72fad4b495d6e0967c2b3063c06ba2d79c9098331ba0219094204023c7 ghcr.io/projectcontour/contour:v1.21.1],SizeBytes:13407229,},ContainerImage{Names:[docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c docker.io/weaveworks/weave-npc:2.8.1],SizeBytes:12814131,},ContainerImage{Names:[quay.io/jetstack/cert-manager-cainjector@sha256:985743eeed2b62f68ee06e583f1d5a371e1c35af4b1980a1b2571d29174cce47 quay.io/jetstack/cert-manager-cainjector:v1.7.1],SizeBytes:12094880,},ContainerImage{Names:[gcr.io/knative-releases/knative.dev/pkg/apiextensions/storageversion/cmd/migrate@sha256:b333601675dc5f8349b3133bd9fcc7a620db0a5076892ea00338b19fc1e073fb],SizeBytes:11564327,},ContainerImage{Names:[docker.io/library/registry@sha256:83bb78d7b28f1ac99c68133af32c93e9a1c149bcd3cb6e683a3ee56e312f1c96 docker.io/library/registry:2.8.1],SizeBytes:9204128,},ContainerImage{Names:[registry.gitlab.com/sharingio/environment/exposer@sha256:d5234c7aee6d1281a72b4b1f09124a662ca230ece854def9449c988390f29897 registry.gitlab.com/sharingio/environment/exposer:2022.07.25.1600],SizeBytes:8641265,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 registry.k8s.io/e2e-test-images/nginx:1.14-2],SizeBytes:6979041,},ContainerImage{Names:[registry.gitlab.com/safesurfer/go-http-server@sha256:5a092b5372f23ec18117411b63ffd60f73dce967204087b24cc4b65b97e2c0b4 registry.gitlab.com/safesurfer/go-http-server:1.6.0],SizeBytes:4512154,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac registry.k8s.io/e2e-test-images/nonewprivs:1.3],SizeBytes:3263463,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
    Aug 25 04:19:11.933: INFO: 
    Logging kubelet events for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:19:11.936: INFO: 
    Logging pods the kubelet thinks is on node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:19:11.964: INFO: powerdns-db-7c897fddf4-l5cjb started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: coredns-565d847f94-684lz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: knative-operator-594876444d-qzjs4 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: domain-mapping-5bcd85fbc6-2gshd started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: autoscaler-7bf8ff94db-98bhj started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: cert-manager-cainjector-6495667ff4-x6lll started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: external-dns-67d79cbcd4-bf29h started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: etcd-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:33 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: sonobuoy-e2e-job-39386ef5d0694a49 started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: speaker-f82r7 started at 2022-08-25 03:25:09 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: contour-588fc6cc6d-f66zz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: controller-7fd644cbc6-dfb6n started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: activator-88b7df5c-cbxk8 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container activator ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: contour-588fc6cc6d-dq6wf started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: coredns-565d847f94-lzgv5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: environment-0 started at 2022-08-25 03:25:19 +0000 UTC (0+2 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container environment ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: helm-operator-7959478576-6mcjz started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: webhook-69d55f5549-ghp24 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: local-path-provisioner-56c55476f9-xbkr9 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: powerdns-5c6dbcf579-rgzxs started at 2022-08-25 03:24:37 +0000 UTC (1+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Init container powerdns-init-db ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: reveal-multiplex-6bbbf59d6-k9lt8 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: weave-net-75fql started at 2022-08-25 02:40:45 +0000 UTC (1+2 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Init container weave-init ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container weave ready: true, restart count 1
    Aug 25 04:19:11.964: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: pod-handle-http-request started at 2022-08-25 04:19:03 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container agnhost-container ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: net-certmanager-controller-6c8cb88879-86vmg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: domainmapping-webhook-77f466bbc9-97ft7 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: net-certmanager-webhook-79cfb96f68-kfnlr started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: public-html-go-http-server-55b9f584d4-4jrp5 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: kubed-568bdbc6c4-vx7m6 started at 2022-08-25 03:24:38 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: envoy-7sxnc started at 2022-08-25 03:25:38 +0000 UTC (1+2 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Init container envoy-initconfig ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: contour-8597b78798-lb4wb started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: contour-8597b78798-5755z started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: environment-exposer-5bb6d44465-8btrx started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: cert-manager-66bd77df8f-qlhqg started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: kube-proxy-b4lgh started at 2022-08-25 02:40:45 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: envoy-cxlbl started at 2022-08-25 03:25:39 +0000 UTC (1+2 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Init container envoy-initconfig ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: autoscaler-hpa-776c44cc57-rcqj4 started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: metrics-server-5cb46dccf6-w9d6b started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj started at 2022-08-25 02:48:37 +0000 UTC (0+2 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: controller-77cc7ff558-mrwjl started at 2022-08-25 03:24:37 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: net-contour-controller-5c5fd89fdb-lnf94 started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp started at 2022-08-25 02:40:32 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 04:19:11.964: INFO: distribution-7f9dff85c4-c276b started at 2022-08-25 03:24:39 +0000 UTC (0+1 container statuses recorded)
    Aug 25 04:19:11.964: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 04:19:12.080: INFO: 
    Latency metrics for node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:19:12.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-243" for this suite. 08/25/22 04:19:12.085
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output

  Aug 25 04:19:11.918: Conformance requires at least two nodes
  In [It] at: vendor/github.com/onsi/ginkgo/v2/internal/suite.go:596
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:12.094
Aug 25 04:19:12.094: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename watch 08/25/22 04:19:12.095
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:12.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:12.11
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/25/22 04:19:12.115
STEP: creating a new configmap 08/25/22 04:19:12.118
STEP: modifying the configmap once 08/25/22 04:19:12.12
STEP: closing the watch once it receives two notifications 08/25/22 04:19:12.125
Aug 25 04:19:12.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55453 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 04:19:12.126: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55454 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/25/22 04:19:12.126
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/25/22 04:19:12.132
STEP: deleting the configmap 08/25/22 04:19:12.133
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/25/22 04:19:12.136
Aug 25 04:19:12.137: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55455 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 25 04:19:12.137: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55456 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 25 04:19:12.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9417" for this suite. 08/25/22 04:19:12.141
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":318,"skipped":6041,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.051 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:12.094
    Aug 25 04:19:12.094: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename watch 08/25/22 04:19:12.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:12.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:12.11
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/25/22 04:19:12.115
    STEP: creating a new configmap 08/25/22 04:19:12.118
    STEP: modifying the configmap once 08/25/22 04:19:12.12
    STEP: closing the watch once it receives two notifications 08/25/22 04:19:12.125
    Aug 25 04:19:12.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55453 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 04:19:12.126: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55454 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/25/22 04:19:12.126
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/25/22 04:19:12.132
    STEP: deleting the configmap 08/25/22 04:19:12.133
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/25/22 04:19:12.136
    Aug 25 04:19:12.137: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55455 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 25 04:19:12.137: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9417  b93dc40d-72c4-4c60-ab46-c6c64f96509c 55456 0 2022-08-25 04:19:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-08-25 04:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 25 04:19:12.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9417" for this suite. 08/25/22 04:19:12.141
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:12.146
Aug 25 04:19:12.147: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:19:12.148
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:12.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:12.162
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-2b62ffbf-f4e6-46b6-8124-9d74be3f8a99 08/25/22 04:19:12.166
STEP: Creating a pod to test consume secrets 08/25/22 04:19:12.169
Aug 25 04:19:12.177: INFO: Waiting up to 5m0s for pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2" in namespace "secrets-4990" to be "Succeeded or Failed"
Aug 25 04:19:12.180: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158234ms
Aug 25 04:19:14.186: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009321098s
Aug 25 04:19:16.184: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007580621s
STEP: Saw pod success 08/25/22 04:19:16.184
Aug 25 04:19:16.184: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2" satisfied condition "Succeeded or Failed"
Aug 25 04:19:16.188: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2 container secret-volume-test: <nil>
STEP: delete the pod 08/25/22 04:19:16.194
Aug 25 04:19:16.201: INFO: Waiting for pod pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2 to disappear
Aug 25 04:19:16.204: INFO: Pod pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:19:16.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4990" for this suite. 08/25/22 04:19:16.208
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":319,"skipped":6061,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:12.146
    Aug 25 04:19:12.147: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:19:12.148
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:12.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:12.162
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-2b62ffbf-f4e6-46b6-8124-9d74be3f8a99 08/25/22 04:19:12.166
    STEP: Creating a pod to test consume secrets 08/25/22 04:19:12.169
    Aug 25 04:19:12.177: INFO: Waiting up to 5m0s for pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2" in namespace "secrets-4990" to be "Succeeded or Failed"
    Aug 25 04:19:12.180: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158234ms
    Aug 25 04:19:14.186: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009321098s
    Aug 25 04:19:16.184: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007580621s
    STEP: Saw pod success 08/25/22 04:19:16.184
    Aug 25 04:19:16.184: INFO: Pod "pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2" satisfied condition "Succeeded or Failed"
    Aug 25 04:19:16.188: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2 container secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:19:16.194
    Aug 25 04:19:16.201: INFO: Waiting for pod pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2 to disappear
    Aug 25 04:19:16.204: INFO: Pod pod-secrets-78232d21-8deb-4b62-9967-becff5bd39c2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:19:16.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4990" for this suite. 08/25/22 04:19:16.208
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:16.213
Aug 25 04:19:16.213: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:19:16.215
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:16.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:16.228
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 08/25/22 04:19:16.232
Aug 25 04:19:16.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b" in namespace "downward-api-9748" to be "Succeeded or Failed"
Aug 25 04:19:16.242: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345932ms
Aug 25 04:19:18.249: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b": Phase="Running", Reason="", readiness=false. Elapsed: 2.011405563s
Aug 25 04:19:20.248: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009769044s
STEP: Saw pod success 08/25/22 04:19:20.248
Aug 25 04:19:20.248: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b" satisfied condition "Succeeded or Failed"
Aug 25 04:19:20.252: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b container client-container: <nil>
STEP: delete the pod 08/25/22 04:19:20.257
Aug 25 04:19:20.264: INFO: Waiting for pod downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b to disappear
Aug 25 04:19:20.267: INFO: Pod downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 04:19:20.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9748" for this suite. 08/25/22 04:19:20.271
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":320,"skipped":6061,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.062 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:16.213
    Aug 25 04:19:16.213: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:19:16.215
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:16.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:16.228
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 08/25/22 04:19:16.232
    Aug 25 04:19:16.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b" in namespace "downward-api-9748" to be "Succeeded or Failed"
    Aug 25 04:19:16.242: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345932ms
    Aug 25 04:19:18.249: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b": Phase="Running", Reason="", readiness=false. Elapsed: 2.011405563s
    Aug 25 04:19:20.248: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009769044s
    STEP: Saw pod success 08/25/22 04:19:20.248
    Aug 25 04:19:20.248: INFO: Pod "downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b" satisfied condition "Succeeded or Failed"
    Aug 25 04:19:20.252: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b container client-container: <nil>
    STEP: delete the pod 08/25/22 04:19:20.257
    Aug 25 04:19:20.264: INFO: Waiting for pod downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b to disappear
    Aug 25 04:19:20.267: INFO: Pod downwardapi-volume-def5b1af-9244-4e8e-b0c6-dd1b485bf95b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 04:19:20.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9748" for this suite. 08/25/22 04:19:20.271
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:20.276
Aug 25 04:19:20.276: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename dns 08/25/22 04:19:20.278
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:20.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:20.292
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/25/22 04:19:20.295
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/25/22 04:19:20.299
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/25/22 04:19:20.299
STEP: creating a pod to probe DNS 08/25/22 04:19:20.299
STEP: submitting the pod to kubernetes 08/25/22 04:19:20.299
Aug 25 04:19:20.306: INFO: Waiting up to 15m0s for pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b" in namespace "dns-8436" to be "running"
Aug 25 04:19:20.309: INFO: Pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755742ms
Aug 25 04:19:22.315: INFO: Pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008498915s
Aug 25 04:19:22.315: INFO: Pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b" satisfied condition "running"
STEP: retrieving the pod 08/25/22 04:19:22.315
STEP: looking for the results for each expected name from probers 08/25/22 04:19:22.319
Aug 25 04:19:22.334: INFO: DNS probes using dns-8436/dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b succeeded

STEP: deleting the pod 08/25/22 04:19:22.334
STEP: deleting the test headless service 08/25/22 04:19:22.342
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 25 04:19:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8436" for this suite. 08/25/22 04:19:22.353
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":321,"skipped":6067,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [2.081 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:20.276
    Aug 25 04:19:20.276: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename dns 08/25/22 04:19:20.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:20.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:20.292
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/25/22 04:19:20.295
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/25/22 04:19:20.299
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8436.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/25/22 04:19:20.299
    STEP: creating a pod to probe DNS 08/25/22 04:19:20.299
    STEP: submitting the pod to kubernetes 08/25/22 04:19:20.299
    Aug 25 04:19:20.306: INFO: Waiting up to 15m0s for pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b" in namespace "dns-8436" to be "running"
    Aug 25 04:19:20.309: INFO: Pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755742ms
    Aug 25 04:19:22.315: INFO: Pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008498915s
    Aug 25 04:19:22.315: INFO: Pod "dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b" satisfied condition "running"
    STEP: retrieving the pod 08/25/22 04:19:22.315
    STEP: looking for the results for each expected name from probers 08/25/22 04:19:22.319
    Aug 25 04:19:22.334: INFO: DNS probes using dns-8436/dns-test-32810034-24c9-48f7-bda9-1bf12ecd322b succeeded

    STEP: deleting the pod 08/25/22 04:19:22.334
    STEP: deleting the test headless service 08/25/22 04:19:22.342
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 25 04:19:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8436" for this suite. 08/25/22 04:19:22.353
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:22.358
Aug 25 04:19:22.358: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename subpath 08/25/22 04:19:22.359
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:22.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:22.372
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/25/22 04:19:22.376
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-6h48 08/25/22 04:19:22.383
STEP: Creating a pod to test atomic-volume-subpath 08/25/22 04:19:22.383
Aug 25 04:19:22.392: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6h48" in namespace "subpath-6795" to be "Succeeded or Failed"
Aug 25 04:19:22.395: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197654ms
Aug 25 04:19:24.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008702538s
Aug 25 04:19:26.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 4.008469911s
Aug 25 04:19:28.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 6.009202093s
Aug 25 04:19:30.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 8.008774891s
Aug 25 04:19:32.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 10.008512469s
Aug 25 04:19:34.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 12.008357971s
Aug 25 04:19:36.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 14.007926101s
Aug 25 04:19:38.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 16.009298271s
Aug 25 04:19:40.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 18.007973482s
Aug 25 04:19:42.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 20.008200075s
Aug 25 04:19:44.402: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=false. Elapsed: 22.009640207s
Aug 25 04:19:46.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007788026s
STEP: Saw pod success 08/25/22 04:19:46.4
Aug 25 04:19:46.400: INFO: Pod "pod-subpath-test-configmap-6h48" satisfied condition "Succeeded or Failed"
Aug 25 04:19:46.404: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-configmap-6h48 container test-container-subpath-configmap-6h48: <nil>
STEP: delete the pod 08/25/22 04:19:46.409
Aug 25 04:19:46.417: INFO: Waiting for pod pod-subpath-test-configmap-6h48 to disappear
Aug 25 04:19:46.420: INFO: Pod pod-subpath-test-configmap-6h48 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6h48 08/25/22 04:19:46.42
Aug 25 04:19:46.420: INFO: Deleting pod "pod-subpath-test-configmap-6h48" in namespace "subpath-6795"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 25 04:19:46.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6795" for this suite. 08/25/22 04:19:46.429
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":322,"skipped":6071,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [24.076 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:22.358
    Aug 25 04:19:22.358: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename subpath 08/25/22 04:19:22.359
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:22.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:22.372
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/25/22 04:19:22.376
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-6h48 08/25/22 04:19:22.383
    STEP: Creating a pod to test atomic-volume-subpath 08/25/22 04:19:22.383
    Aug 25 04:19:22.392: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6h48" in namespace "subpath-6795" to be "Succeeded or Failed"
    Aug 25 04:19:22.395: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197654ms
    Aug 25 04:19:24.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008702538s
    Aug 25 04:19:26.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 4.008469911s
    Aug 25 04:19:28.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 6.009202093s
    Aug 25 04:19:30.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 8.008774891s
    Aug 25 04:19:32.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 10.008512469s
    Aug 25 04:19:34.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 12.008357971s
    Aug 25 04:19:36.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 14.007926101s
    Aug 25 04:19:38.401: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 16.009298271s
    Aug 25 04:19:40.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 18.007973482s
    Aug 25 04:19:42.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=true. Elapsed: 20.008200075s
    Aug 25 04:19:44.402: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Running", Reason="", readiness=false. Elapsed: 22.009640207s
    Aug 25 04:19:46.400: INFO: Pod "pod-subpath-test-configmap-6h48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007788026s
    STEP: Saw pod success 08/25/22 04:19:46.4
    Aug 25 04:19:46.400: INFO: Pod "pod-subpath-test-configmap-6h48" satisfied condition "Succeeded or Failed"
    Aug 25 04:19:46.404: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-subpath-test-configmap-6h48 container test-container-subpath-configmap-6h48: <nil>
    STEP: delete the pod 08/25/22 04:19:46.409
    Aug 25 04:19:46.417: INFO: Waiting for pod pod-subpath-test-configmap-6h48 to disappear
    Aug 25 04:19:46.420: INFO: Pod pod-subpath-test-configmap-6h48 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6h48 08/25/22 04:19:46.42
    Aug 25 04:19:46.420: INFO: Deleting pod "pod-subpath-test-configmap-6h48" in namespace "subpath-6795"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 25 04:19:46.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6795" for this suite. 08/25/22 04:19:46.429
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:46.435
Aug 25 04:19:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename emptydir 08/25/22 04:19:46.436
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:46.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:46.451
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/25/22 04:19:46.456
Aug 25 04:19:46.461: INFO: Waiting up to 5m0s for pod "pod-d4579baf-4807-4468-ac82-969d90de4e33" in namespace "emptydir-5162" to be "Succeeded or Failed"
Aug 25 04:19:46.464: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024256ms
Aug 25 04:19:48.470: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33": Phase="Running", Reason="", readiness=false. Elapsed: 2.008688188s
Aug 25 04:19:50.469: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008045479s
STEP: Saw pod success 08/25/22 04:19:50.469
Aug 25 04:19:50.469: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33" satisfied condition "Succeeded or Failed"
Aug 25 04:19:50.473: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-d4579baf-4807-4468-ac82-969d90de4e33 container test-container: <nil>
STEP: delete the pod 08/25/22 04:19:50.478
Aug 25 04:19:50.486: INFO: Waiting for pod pod-d4579baf-4807-4468-ac82-969d90de4e33 to disappear
Aug 25 04:19:50.489: INFO: Pod pod-d4579baf-4807-4468-ac82-969d90de4e33 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 25 04:19:50.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5162" for this suite. 08/25/22 04:19:50.493
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":323,"skipped":6078,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:46.435
    Aug 25 04:19:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename emptydir 08/25/22 04:19:46.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:46.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:46.451
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/25/22 04:19:46.456
    Aug 25 04:19:46.461: INFO: Waiting up to 5m0s for pod "pod-d4579baf-4807-4468-ac82-969d90de4e33" in namespace "emptydir-5162" to be "Succeeded or Failed"
    Aug 25 04:19:46.464: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024256ms
    Aug 25 04:19:48.470: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33": Phase="Running", Reason="", readiness=false. Elapsed: 2.008688188s
    Aug 25 04:19:50.469: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008045479s
    STEP: Saw pod success 08/25/22 04:19:50.469
    Aug 25 04:19:50.469: INFO: Pod "pod-d4579baf-4807-4468-ac82-969d90de4e33" satisfied condition "Succeeded or Failed"
    Aug 25 04:19:50.473: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-d4579baf-4807-4468-ac82-969d90de4e33 container test-container: <nil>
    STEP: delete the pod 08/25/22 04:19:50.478
    Aug 25 04:19:50.486: INFO: Waiting for pod pod-d4579baf-4807-4468-ac82-969d90de4e33 to disappear
    Aug 25 04:19:50.489: INFO: Pod pod-d4579baf-4807-4468-ac82-969d90de4e33 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 25 04:19:50.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5162" for this suite. 08/25/22 04:19:50.493
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:50.497
Aug 25 04:19:50.497: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename svcaccounts 08/25/22 04:19:50.499
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:50.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:50.515
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Aug 25 04:19:50.528: INFO: Waiting up to 5m0s for pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b" in namespace "svcaccounts-4828" to be "running"
Aug 25 04:19:50.530: INFO: Pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.23903ms
Aug 25 04:19:52.536: INFO: Pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00784045s
Aug 25 04:19:52.536: INFO: Pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b" satisfied condition "running"
STEP: reading a file in the container 08/25/22 04:19:52.536
Aug 25 04:19:52.536: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/25/22 04:19:52.711
Aug 25 04:19:52.711: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/25/22 04:19:52.896
Aug 25 04:19:52.896: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 25 04:19:53.092: INFO: Got root ca configmap in namespace "svcaccounts-4828"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 25 04:19:53.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4828" for this suite. 08/25/22 04:19:53.101
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":324,"skipped":6082,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [2.609 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:50.497
    Aug 25 04:19:50.497: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename svcaccounts 08/25/22 04:19:50.499
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:50.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:50.515
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Aug 25 04:19:50.528: INFO: Waiting up to 5m0s for pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b" in namespace "svcaccounts-4828" to be "running"
    Aug 25 04:19:50.530: INFO: Pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.23903ms
    Aug 25 04:19:52.536: INFO: Pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00784045s
    Aug 25 04:19:52.536: INFO: Pod "pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b" satisfied condition "running"
    STEP: reading a file in the container 08/25/22 04:19:52.536
    Aug 25 04:19:52.536: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/25/22 04:19:52.711
    Aug 25 04:19:52.711: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/25/22 04:19:52.896
    Aug 25 04:19:52.896: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-4e0d28ae-0abc-4830-9ee9-d77220cb4c0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 25 04:19:53.092: INFO: Got root ca configmap in namespace "svcaccounts-4828"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 25 04:19:53.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4828" for this suite. 08/25/22 04:19:53.101
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:53.107
Aug 25 04:19:53.107: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:19:53.108
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:53.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:53.126
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-d17999db-a88d-41a8-9299-fcffff491f4a 08/25/22 04:19:53.133
STEP: Creating secret with name s-test-opt-upd-53c462c9-d612-4b98-a0ff-84222d5cb41c 08/25/22 04:19:53.137
STEP: Creating the pod 08/25/22 04:19:53.142
Aug 25 04:19:53.150: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3" in namespace "projected-9941" to be "running and ready"
Aug 25 04:19:53.153: INFO: Pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054902ms
Aug 25 04:19:53.153: INFO: The phase of Pod pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:19:55.158: INFO: Pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008174573s
Aug 25 04:19:55.158: INFO: The phase of Pod pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3 is Running (Ready = true)
Aug 25 04:19:55.158: INFO: Pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d17999db-a88d-41a8-9299-fcffff491f4a 08/25/22 04:19:55.175
STEP: Updating secret s-test-opt-upd-53c462c9-d612-4b98-a0ff-84222d5cb41c 08/25/22 04:19:55.18
STEP: Creating secret with name s-test-opt-create-a65f641a-840e-41db-933d-5ff82c57df30 08/25/22 04:19:55.184
STEP: waiting to observe update in volume 08/25/22 04:19:55.189
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 04:19:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9941" for this suite. 08/25/22 04:19:57.216
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":325,"skipped":6086,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.114 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:53.107
    Aug 25 04:19:53.107: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:19:53.108
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:53.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:53.126
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-d17999db-a88d-41a8-9299-fcffff491f4a 08/25/22 04:19:53.133
    STEP: Creating secret with name s-test-opt-upd-53c462c9-d612-4b98-a0ff-84222d5cb41c 08/25/22 04:19:53.137
    STEP: Creating the pod 08/25/22 04:19:53.142
    Aug 25 04:19:53.150: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3" in namespace "projected-9941" to be "running and ready"
    Aug 25 04:19:53.153: INFO: Pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054902ms
    Aug 25 04:19:53.153: INFO: The phase of Pod pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:19:55.158: INFO: Pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008174573s
    Aug 25 04:19:55.158: INFO: The phase of Pod pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3 is Running (Ready = true)
    Aug 25 04:19:55.158: INFO: Pod "pod-projected-secrets-2b3daf67-0341-4182-977c-22ea0b1967c3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d17999db-a88d-41a8-9299-fcffff491f4a 08/25/22 04:19:55.175
    STEP: Updating secret s-test-opt-upd-53c462c9-d612-4b98-a0ff-84222d5cb41c 08/25/22 04:19:55.18
    STEP: Creating secret with name s-test-opt-create-a65f641a-840e-41db-933d-5ff82c57df30 08/25/22 04:19:55.184
    STEP: waiting to observe update in volume 08/25/22 04:19:55.189
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 04:19:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9941" for this suite. 08/25/22 04:19:57.216
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:57.223
Aug 25 04:19:57.223: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 04:19:57.224
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:57.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:57.241
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 25 04:19:57.245: INFO: Creating deployment "test-recreate-deployment"
Aug 25 04:19:57.250: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 25 04:19:57.256: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 25 04:19:59.266: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 25 04:19:59.270: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 25 04:19:59.279: INFO: Updating deployment test-recreate-deployment
Aug 25 04:19:59.279: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 04:19:59.327: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2951  065c65ea-3083-428f-9666-35870f3829fe 55986 2 2022-08-25 04:19:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005654318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-25 04:19:59 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-08-25 04:19:59 +0000 UTC,LastTransitionTime:2022-08-25 04:19:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 25 04:19:59.332: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-2951  f1043f1b-72ce-443b-bc59-f40df56bb69d 55984 1 2022-08-25 04:19:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 065c65ea-3083-428f-9666-35870f3829fe 0xc0056547e0 0xc0056547e1}] [] [{kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"065c65ea-3083-428f-9666-35870f3829fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005654878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 25 04:19:59.332: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 25 04:19:59.332: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-2951  2179c25b-67ba-4f8b-a7be-714163f574b3 55975 2 2022-08-25 04:19:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 065c65ea-3083-428f-9666-35870f3829fe 0xc0056546c7 0xc0056546c8}] [] [{kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"065c65ea-3083-428f-9666-35870f3829fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005654778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 25 04:19:59.336: INFO: Pod "test-recreate-deployment-9d58999df-klh9m" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-klh9m test-recreate-deployment-9d58999df- deployment-2951  e8dbfa77-97c7-460a-adc9-5cdb63964b96 55987 0 2022-08-25 04:19:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df f1043f1b-72ce-443b-bc59-f40df56bb69d 0xc005654cf0 0xc005654cf1}] [] [{kube-controller-manager Update v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f1043f1b-72ce-443b-bc59-f40df56bb69d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vlbbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vlbbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 04:19:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 04:19:59.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2951" for this suite. 08/25/22 04:19:59.339
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":326,"skipped":6103,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [2.120 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:57.223
    Aug 25 04:19:57.223: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 04:19:57.224
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:57.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:57.241
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 25 04:19:57.245: INFO: Creating deployment "test-recreate-deployment"
    Aug 25 04:19:57.250: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 25 04:19:57.256: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 25 04:19:59.266: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 25 04:19:59.270: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 25 04:19:59.279: INFO: Updating deployment test-recreate-deployment
    Aug 25 04:19:59.279: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 04:19:59.327: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2951  065c65ea-3083-428f-9666-35870f3829fe 55986 2 2022-08-25 04:19:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005654318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-25 04:19:59 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-08-25 04:19:59 +0000 UTC,LastTransitionTime:2022-08-25 04:19:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 25 04:19:59.332: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-2951  f1043f1b-72ce-443b-bc59-f40df56bb69d 55984 1 2022-08-25 04:19:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 065c65ea-3083-428f-9666-35870f3829fe 0xc0056547e0 0xc0056547e1}] [] [{kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"065c65ea-3083-428f-9666-35870f3829fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005654878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 04:19:59.332: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 25 04:19:59.332: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-2951  2179c25b-67ba-4f8b-a7be-714163f574b3 55975 2 2022-08-25 04:19:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 065c65ea-3083-428f-9666-35870f3829fe 0xc0056546c7 0xc0056546c8}] [] [{kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"065c65ea-3083-428f-9666-35870f3829fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005654778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 04:19:59.336: INFO: Pod "test-recreate-deployment-9d58999df-klh9m" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-klh9m test-recreate-deployment-9d58999df- deployment-2951  e8dbfa77-97c7-460a-adc9-5cdb63964b96 55987 0 2022-08-25 04:19:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df f1043f1b-72ce-443b-bc59-f40df56bb69d 0xc005654cf0 0xc005654cf1}] [] [{kube-controller-manager Update v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f1043f1b-72ce-443b-bc59-f40df56bb69d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 04:19:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vlbbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vlbbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:19:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:,StartTime:2022-08-25 04:19:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 04:19:59.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2951" for this suite. 08/25/22 04:19:59.339
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:19:59.344
Aug 25 04:19:59.344: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename endpointslicemirroring 08/25/22 04:19:59.344
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:59.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:59.356
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/25/22 04:19:59.368
Aug 25 04:19:59.375: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/25/22 04:20:01.379
Aug 25 04:20:01.388: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/25/22 04:20:03.393
Aug 25 04:20:03.402: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Aug 25 04:20:05.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6029" for this suite. 08/25/22 04:20:05.412
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":327,"skipped":6125,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [6.074 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:19:59.344
    Aug 25 04:19:59.344: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename endpointslicemirroring 08/25/22 04:19:59.344
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:19:59.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:19:59.356
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/25/22 04:19:59.368
    Aug 25 04:19:59.375: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/25/22 04:20:01.379
    Aug 25 04:20:01.388: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/25/22 04:20:03.393
    Aug 25 04:20:03.402: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Aug 25 04:20:05.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-6029" for this suite. 08/25/22 04:20:05.412
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:20:05.42
Aug 25 04:20:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:20:05.421
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:05.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:05.438
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-fe40939f-40a3-484a-921c-924d00a90201 08/25/22 04:20:05.442
STEP: Creating a pod to test consume secrets 08/25/22 04:20:05.446
Aug 25 04:20:05.453: INFO: Waiting up to 5m0s for pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd" in namespace "secrets-602" to be "Succeeded or Failed"
Aug 25 04:20:05.456: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434869ms
Aug 25 04:20:07.462: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329371s
Aug 25 04:20:09.461: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865373s
STEP: Saw pod success 08/25/22 04:20:09.461
Aug 25 04:20:09.461: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd" satisfied condition "Succeeded or Failed"
Aug 25 04:20:09.465: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd container secret-env-test: <nil>
STEP: delete the pod 08/25/22 04:20:09.47
Aug 25 04:20:09.476: INFO: Waiting for pod pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd to disappear
Aug 25 04:20:09.479: INFO: Pod pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:20:09.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-602" for this suite. 08/25/22 04:20:09.482
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":328,"skipped":6141,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.067 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:20:05.42
    Aug 25 04:20:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:20:05.421
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:05.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:05.438
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-fe40939f-40a3-484a-921c-924d00a90201 08/25/22 04:20:05.442
    STEP: Creating a pod to test consume secrets 08/25/22 04:20:05.446
    Aug 25 04:20:05.453: INFO: Waiting up to 5m0s for pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd" in namespace "secrets-602" to be "Succeeded or Failed"
    Aug 25 04:20:05.456: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434869ms
    Aug 25 04:20:07.462: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329371s
    Aug 25 04:20:09.461: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865373s
    STEP: Saw pod success 08/25/22 04:20:09.461
    Aug 25 04:20:09.461: INFO: Pod "pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd" satisfied condition "Succeeded or Failed"
    Aug 25 04:20:09.465: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd container secret-env-test: <nil>
    STEP: delete the pod 08/25/22 04:20:09.47
    Aug 25 04:20:09.476: INFO: Waiting for pod pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd to disappear
    Aug 25 04:20:09.479: INFO: Pod pod-secrets-3dc0ab1b-82c6-4ca2-95c6-16d7cd3d92fd no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:20:09.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-602" for this suite. 08/25/22 04:20:09.482
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:20:09.488
Aug 25 04:20:09.488: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:20:09.49
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:09.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:09.505
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-7760/configmap-test-90aed77f-0b10-4795-aa79-9c73060ae896 08/25/22 04:20:09.509
STEP: Creating a pod to test consume configMaps 08/25/22 04:20:09.512
Aug 25 04:20:09.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6" in namespace "configmap-7760" to be "Succeeded or Failed"
Aug 25 04:20:09.520: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438411ms
Aug 25 04:20:11.525: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007706308s
Aug 25 04:20:13.526: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00794273s
STEP: Saw pod success 08/25/22 04:20:13.526
Aug 25 04:20:13.526: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6" satisfied condition "Succeeded or Failed"
Aug 25 04:20:13.530: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6 container env-test: <nil>
STEP: delete the pod 08/25/22 04:20:13.535
Aug 25 04:20:13.544: INFO: Waiting for pod pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6 to disappear
Aug 25 04:20:13.547: INFO: Pod pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:20:13.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7760" for this suite. 08/25/22 04:20:13.551
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":329,"skipped":6157,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.068 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:20:09.488
    Aug 25 04:20:09.488: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:20:09.49
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:09.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:09.505
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-7760/configmap-test-90aed77f-0b10-4795-aa79-9c73060ae896 08/25/22 04:20:09.509
    STEP: Creating a pod to test consume configMaps 08/25/22 04:20:09.512
    Aug 25 04:20:09.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6" in namespace "configmap-7760" to be "Succeeded or Failed"
    Aug 25 04:20:09.520: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438411ms
    Aug 25 04:20:11.525: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007706308s
    Aug 25 04:20:13.526: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00794273s
    STEP: Saw pod success 08/25/22 04:20:13.526
    Aug 25 04:20:13.526: INFO: Pod "pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6" satisfied condition "Succeeded or Failed"
    Aug 25 04:20:13.530: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6 container env-test: <nil>
    STEP: delete the pod 08/25/22 04:20:13.535
    Aug 25 04:20:13.544: INFO: Waiting for pod pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6 to disappear
    Aug 25 04:20:13.547: INFO: Pod pod-configmaps-005b54db-08a0-4f88-9e9a-bc4d8d4246a6 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:20:13.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7760" for this suite. 08/25/22 04:20:13.551
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:20:13.559
Aug 25 04:20:13.559: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename deployment 08/25/22 04:20:13.56
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:13.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:13.575
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 25 04:20:13.579: INFO: Creating simple deployment test-new-deployment
Aug 25 04:20:13.588: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/25/22 04:20:15.603
STEP: updating a scale subresource 08/25/22 04:20:15.606
STEP: verifying the deployment Spec.Replicas was modified 08/25/22 04:20:15.612
STEP: Patch a scale subresource 08/25/22 04:20:15.616
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 25 04:20:15.628: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7396  67863925-d2ba-4c80-bd76-147ab4114bcb 56209 3 2022-08-25 04:20:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-08-25 04:20:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a93f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-25 04:20:14 +0000 UTC,LastTransitionTime:2022-08-25 04:20:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-08-25 04:20:14 +0000 UTC,LastTransitionTime:2022-08-25 04:20:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 25 04:20:15.632: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7396  3c316d3a-91de-42db-b767-b198ae1a6c8d 56214 2 2022-08-25 04:20:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 67863925-d2ba-4c80-bd76-147ab4114bcb 0xc0009a99c7 0xc0009a99c8}] [] [{kube-controller-manager Update apps/v1 2022-08-25 04:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67863925-d2ba-4c80-bd76-147ab4114bcb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:20:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a9a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 25 04:20:15.637: INFO: Pod "test-new-deployment-845c8977d9-dsw8z" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-dsw8z test-new-deployment-845c8977d9- deployment-7396  292b1065-416c-4d2c-bab9-beb5550521ea 56213 0 2022-08-25 04:20:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3c316d3a-91de-42db-b767-b198ae1a6c8d 0xc003946cd7 0xc003946cd8}] [] [{kube-controller-manager Update v1 2022-08-25 04:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c316d3a-91de-42db-b767-b198ae1a6c8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9jmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9jmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 25 04:20:15.637: INFO: Pod "test-new-deployment-845c8977d9-wbc8t" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-wbc8t test-new-deployment-845c8977d9- deployment-7396  2e4d7557-0cac-4e51-abab-f3421b389216 56182 0 2022-08-25 04:20:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3c316d3a-91de-42db-b767-b198ae1a6c8d 0xc003946e30 0xc003946e31}] [] [{kube-controller-manager Update v1 2022-08-25 04:20:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c316d3a-91de-42db-b767-b198ae1a6c8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 04:20:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fl5kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fl5kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.10,StartTime:2022-08-25 04:20:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 04:20:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://97bc3020ad804e9b56965e727388fdf70cc3809633e4a870d8121a7bfe56c88a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 25 04:20:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7396" for this suite. 08/25/22 04:20:15.641
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":330,"skipped":6190,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [2.086 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:20:13.559
    Aug 25 04:20:13.559: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename deployment 08/25/22 04:20:13.56
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:13.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:13.575
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 25 04:20:13.579: INFO: Creating simple deployment test-new-deployment
    Aug 25 04:20:13.588: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/25/22 04:20:15.603
    STEP: updating a scale subresource 08/25/22 04:20:15.606
    STEP: verifying the deployment Spec.Replicas was modified 08/25/22 04:20:15.612
    STEP: Patch a scale subresource 08/25/22 04:20:15.616
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 25 04:20:15.628: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-7396  67863925-d2ba-4c80-bd76-147ab4114bcb 56209 3 2022-08-25 04:20:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-08-25 04:20:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a93f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-25 04:20:14 +0000 UTC,LastTransitionTime:2022-08-25 04:20:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-08-25 04:20:14 +0000 UTC,LastTransitionTime:2022-08-25 04:20:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 25 04:20:15.632: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7396  3c316d3a-91de-42db-b767-b198ae1a6c8d 56214 2 2022-08-25 04:20:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 67863925-d2ba-4c80-bd76-147ab4114bcb 0xc0009a99c7 0xc0009a99c8}] [] [{kube-controller-manager Update apps/v1 2022-08-25 04:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67863925-d2ba-4c80-bd76-147ab4114bcb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-25 04:20:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a9a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 25 04:20:15.637: INFO: Pod "test-new-deployment-845c8977d9-dsw8z" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-dsw8z test-new-deployment-845c8977d9- deployment-7396  292b1065-416c-4d2c-bab9-beb5550521ea 56213 0 2022-08-25 04:20:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3c316d3a-91de-42db-b767-b198ae1a6c8d 0xc003946cd7 0xc003946cd8}] [] [{kube-controller-manager Update v1 2022-08-25 04:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c316d3a-91de-42db-b767-b198ae1a6c8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9jmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9jmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 25 04:20:15.637: INFO: Pod "test-new-deployment-845c8977d9-wbc8t" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-wbc8t test-new-deployment-845c8977d9- deployment-7396  2e4d7557-0cac-4e51-abab-f3421b389216 56182 0 2022-08-25 04:20:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 3c316d3a-91de-42db-b767-b198ae1a6c8d 0xc003946e30 0xc003946e31}] [] [{kube-controller-manager Update v1 2022-08-25 04:20:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c316d3a-91de-42db-b767-b198ae1a6c8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-25 04:20:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fl5kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fl5kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bobymcbobs-c849-control-plane-p55pp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-25 04:20:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:86.109.11.217,PodIP:192.168.0.10,StartTime:2022-08-25 04:20:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-25 04:20:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://97bc3020ad804e9b56965e727388fdf70cc3809633e4a870d8121a7bfe56c88a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 25 04:20:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7396" for this suite. 08/25/22 04:20:15.641
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:20:15.646
Aug 25 04:20:15.646: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:20:15.647
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:15.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:15.66
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-6ccf5dc4-c0c3-4b66-ae1f-3ce207c08a60 08/25/22 04:20:15.662
STEP: Creating a pod to test consume secrets 08/25/22 04:20:15.665
Aug 25 04:20:15.669: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e" in namespace "projected-6035" to be "Succeeded or Failed"
Aug 25 04:20:15.672: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595143ms
Aug 25 04:20:17.677: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e": Phase="Running", Reason="", readiness=false. Elapsed: 2.00757833s
Aug 25 04:20:19.677: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007411912s
STEP: Saw pod success 08/25/22 04:20:19.677
Aug 25 04:20:19.677: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e" satisfied condition "Succeeded or Failed"
Aug 25 04:20:19.681: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e container projected-secret-volume-test: <nil>
STEP: delete the pod 08/25/22 04:20:19.686
Aug 25 04:20:19.693: INFO: Waiting for pod pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e to disappear
Aug 25 04:20:19.696: INFO: Pod pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 25 04:20:19.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6035" for this suite. 08/25/22 04:20:19.701
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":331,"skipped":6210,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.060 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:20:15.646
    Aug 25 04:20:15.646: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:20:15.647
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:15.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:15.66
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-6ccf5dc4-c0c3-4b66-ae1f-3ce207c08a60 08/25/22 04:20:15.662
    STEP: Creating a pod to test consume secrets 08/25/22 04:20:15.665
    Aug 25 04:20:15.669: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e" in namespace "projected-6035" to be "Succeeded or Failed"
    Aug 25 04:20:15.672: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595143ms
    Aug 25 04:20:17.677: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e": Phase="Running", Reason="", readiness=false. Elapsed: 2.00757833s
    Aug 25 04:20:19.677: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007411912s
    STEP: Saw pod success 08/25/22 04:20:19.677
    Aug 25 04:20:19.677: INFO: Pod "pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e" satisfied condition "Succeeded or Failed"
    Aug 25 04:20:19.681: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/25/22 04:20:19.686
    Aug 25 04:20:19.693: INFO: Waiting for pod pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e to disappear
    Aug 25 04:20:19.696: INFO: Pod pod-projected-secrets-c03734fc-8075-48d2-8a72-16e3e89d2f0e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 25 04:20:19.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6035" for this suite. 08/25/22 04:20:19.701
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:20:19.706
Aug 25 04:20:19.706: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename job 08/25/22 04:20:19.708
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:19.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:19.723
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 08/25/22 04:20:19.728
STEP: Ensuring active pods == parallelism 08/25/22 04:20:19.731
STEP: delete a job 08/25/22 04:20:21.737
STEP: deleting Job.batch foo in namespace job-4562, will wait for the garbage collector to delete the pods 08/25/22 04:20:21.737
Aug 25 04:20:21.797: INFO: Deleting Job.batch foo took: 4.47853ms
Aug 25 04:20:21.898: INFO: Terminating Job.batch foo pods took: 100.674786ms
STEP: Ensuring job was deleted 08/25/22 04:20:54.698
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 25 04:20:54.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4562" for this suite. 08/25/22 04:20:54.704
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":332,"skipped":6213,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [35.003 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:20:19.706
    Aug 25 04:20:19.706: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename job 08/25/22 04:20:19.708
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:19.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:19.723
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 08/25/22 04:20:19.728
    STEP: Ensuring active pods == parallelism 08/25/22 04:20:19.731
    STEP: delete a job 08/25/22 04:20:21.737
    STEP: deleting Job.batch foo in namespace job-4562, will wait for the garbage collector to delete the pods 08/25/22 04:20:21.737
    Aug 25 04:20:21.797: INFO: Deleting Job.batch foo took: 4.47853ms
    Aug 25 04:20:21.898: INFO: Terminating Job.batch foo pods took: 100.674786ms
    STEP: Ensuring job was deleted 08/25/22 04:20:54.698
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 25 04:20:54.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4562" for this suite. 08/25/22 04:20:54.704
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:20:54.71
Aug 25 04:20:54.710: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename daemonsets 08/25/22 04:20:54.711
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:54.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:54.724
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Aug 25 04:20:54.737: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:20:54.74
Aug 25 04:20:54.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:20:54.746: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:20:55.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:20:55.756: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update daemon pods image. 08/25/22 04:20:55.771
STEP: Check that daemon pods images are updated. 08/25/22 04:20:55.78
Aug 25 04:20:55.784: INFO: Wrong image for pod: daemon-set-bkhzg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Aug 25 04:20:58.794: INFO: Pod daemon-set-lk284 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 08/25/22 04:20:58.799
Aug 25 04:20:58.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:20:58.808: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:20:59.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 25 04:20:59.815: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:20:59.831
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9516, will wait for the garbage collector to delete the pods 08/25/22 04:20:59.831
Aug 25 04:20:59.891: INFO: Deleting DaemonSet.extensions daemon-set took: 4.65208ms
Aug 25 04:20:59.991: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.684547ms
Aug 25 04:21:02.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 25 04:21:02.696: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 25 04:21:02.701: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56643"},"items":null}

Aug 25 04:21:02.703: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56643"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:21:02.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9516" for this suite. 08/25/22 04:21:02.71
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":333,"skipped":6230,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [8.003 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:20:54.71
    Aug 25 04:20:54.710: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename daemonsets 08/25/22 04:20:54.711
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:20:54.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:20:54.724
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Aug 25 04:20:54.737: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:20:54.74
    Aug 25 04:20:54.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:20:54.746: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:20:55.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:20:55.756: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update daemon pods image. 08/25/22 04:20:55.771
    STEP: Check that daemon pods images are updated. 08/25/22 04:20:55.78
    Aug 25 04:20:55.784: INFO: Wrong image for pod: daemon-set-bkhzg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Aug 25 04:20:58.794: INFO: Pod daemon-set-lk284 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 08/25/22 04:20:58.799
    Aug 25 04:20:58.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:20:58.808: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:20:59.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 25 04:20:59.815: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/25/22 04:20:59.831
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9516, will wait for the garbage collector to delete the pods 08/25/22 04:20:59.831
    Aug 25 04:20:59.891: INFO: Deleting DaemonSet.extensions daemon-set took: 4.65208ms
    Aug 25 04:20:59.991: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.684547ms
    Aug 25 04:21:02.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 25 04:21:02.696: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 25 04:21:02.701: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56643"},"items":null}

    Aug 25 04:21:02.703: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56643"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:21:02.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9516" for this suite. 08/25/22 04:21:02.71
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:02.714
Aug 25 04:21:02.714: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename events 08/25/22 04:21:02.715
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:02.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:02.729
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/25/22 04:21:02.736
Aug 25 04:21:02.741: INFO: created test-event-1
Aug 25 04:21:02.745: INFO: created test-event-2
Aug 25 04:21:02.749: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/25/22 04:21:02.749
STEP: delete collection of events 08/25/22 04:21:02.75
Aug 25 04:21:02.750: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/25/22 04:21:02.756
Aug 25 04:21:02.757: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Aug 25 04:21:02.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9638" for this suite. 08/25/22 04:21:02.761
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":334,"skipped":6245,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.049 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:02.714
    Aug 25 04:21:02.714: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename events 08/25/22 04:21:02.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:02.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:02.729
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/25/22 04:21:02.736
    Aug 25 04:21:02.741: INFO: created test-event-1
    Aug 25 04:21:02.745: INFO: created test-event-2
    Aug 25 04:21:02.749: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/25/22 04:21:02.749
    STEP: delete collection of events 08/25/22 04:21:02.75
    Aug 25 04:21:02.750: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/25/22 04:21:02.756
    Aug 25 04:21:02.757: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Aug 25 04:21:02.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9638" for this suite. 08/25/22 04:21:02.761
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:02.765
Aug 25 04:21:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename kubelet-test 08/25/22 04:21:02.766
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:02.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:02.783
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 25 04:21:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5798" for this suite. 08/25/22 04:21:06.807
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":335,"skipped":6291,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.047 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:02.765
    Aug 25 04:21:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename kubelet-test 08/25/22 04:21:02.766
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:02.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:02.783
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 25 04:21:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5798" for this suite. 08/25/22 04:21:06.807
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:06.812
Aug 25 04:21:06.812: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:21:06.814
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:06.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:06.828
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-9974ade0-705b-49e4-901b-15332c4fcb9b 08/25/22 04:21:06.831
STEP: Creating a pod to test consume configMaps 08/25/22 04:21:06.834
Aug 25 04:21:06.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc" in namespace "configmap-7462" to be "Succeeded or Failed"
Aug 25 04:21:06.842: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821526ms
Aug 25 04:21:08.848: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008477971s
Aug 25 04:21:10.848: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008745784s
STEP: Saw pod success 08/25/22 04:21:10.848
Aug 25 04:21:10.849: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc" satisfied condition "Succeeded or Failed"
Aug 25 04:21:10.853: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc container agnhost-container: <nil>
STEP: delete the pod 08/25/22 04:21:10.858
Aug 25 04:21:10.865: INFO: Waiting for pod pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc to disappear
Aug 25 04:21:10.868: INFO: Pod pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:21:10.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7462" for this suite. 08/25/22 04:21:10.873
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6291,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.065 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:06.812
    Aug 25 04:21:06.812: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:21:06.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:06.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:06.828
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-9974ade0-705b-49e4-901b-15332c4fcb9b 08/25/22 04:21:06.831
    STEP: Creating a pod to test consume configMaps 08/25/22 04:21:06.834
    Aug 25 04:21:06.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc" in namespace "configmap-7462" to be "Succeeded or Failed"
    Aug 25 04:21:06.842: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821526ms
    Aug 25 04:21:08.848: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008477971s
    Aug 25 04:21:10.848: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008745784s
    STEP: Saw pod success 08/25/22 04:21:10.848
    Aug 25 04:21:10.849: INFO: Pod "pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc" satisfied condition "Succeeded or Failed"
    Aug 25 04:21:10.853: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc container agnhost-container: <nil>
    STEP: delete the pod 08/25/22 04:21:10.858
    Aug 25 04:21:10.865: INFO: Waiting for pod pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc to disappear
    Aug 25 04:21:10.868: INFO: Pod pod-configmaps-c4be6a15-ba78-4d48-85da-751c04ee1dfc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:21:10.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7462" for this suite. 08/25/22 04:21:10.873
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:10.878
Aug 25 04:21:10.878: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 04:21:10.879
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:10.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:10.896
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/25/22 04:21:10.903
Aug 25 04:21:10.910: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7092" to be "running and ready"
Aug 25 04:21:10.913: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855694ms
Aug 25 04:21:10.913: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:21:12.918: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352844s
Aug 25 04:21:12.918: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 25 04:21:12.918: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 08/25/22 04:21:12.922
Aug 25 04:21:12.928: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7092" to be "running and ready"
Aug 25 04:21:12.932: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.82168ms
Aug 25 04:21:12.932: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:21:14.938: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009418146s
Aug 25 04:21:14.938: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 25 04:21:14.938: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/25/22 04:21:14.948
STEP: delete the pod with lifecycle hook 08/25/22 04:21:14.954
Aug 25 04:21:14.959: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 25 04:21:14.963: INFO: Pod pod-with-poststart-http-hook still exists
Aug 25 04:21:16.963: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 25 04:21:16.968: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 25 04:21:16.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7092" for this suite. 08/25/22 04:21:16.972
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":337,"skipped":6295,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:10.878
    Aug 25 04:21:10.878: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/25/22 04:21:10.879
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:10.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:10.896
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/25/22 04:21:10.903
    Aug 25 04:21:10.910: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7092" to be "running and ready"
    Aug 25 04:21:10.913: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855694ms
    Aug 25 04:21:10.913: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:21:12.918: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352844s
    Aug 25 04:21:12.918: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 25 04:21:12.918: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 08/25/22 04:21:12.922
    Aug 25 04:21:12.928: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7092" to be "running and ready"
    Aug 25 04:21:12.932: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.82168ms
    Aug 25 04:21:12.932: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:21:14.938: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009418146s
    Aug 25 04:21:14.938: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 25 04:21:14.938: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/25/22 04:21:14.948
    STEP: delete the pod with lifecycle hook 08/25/22 04:21:14.954
    Aug 25 04:21:14.959: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 25 04:21:14.963: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 25 04:21:16.963: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 25 04:21:16.968: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 25 04:21:16.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7092" for this suite. 08/25/22 04:21:16.972
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:16.979
Aug 25 04:21:16.979: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename ephemeral-containers-test 08/25/22 04:21:16.98
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:16.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:16.997
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/25/22 04:21:17.003
Aug 25 04:21:17.009: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-908" to be "running and ready"
Aug 25 04:21:17.011: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.410185ms
Aug 25 04:21:17.011: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:21:19.016: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006804821s
Aug 25 04:21:19.016: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 25 04:21:19.016: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/25/22 04:21:19.025
Aug 25 04:21:19.042: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-908" to be "container debugger running"
Aug 25 04:21:19.045: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.195997ms
Aug 25 04:21:21.049: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007286631s
Aug 25 04:21:23.051: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008855361s
Aug 25 04:21:23.051: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/25/22 04:21:23.051
Aug 25 04:21:23.051: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-908 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 25 04:21:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
Aug 25 04:21:23.052: INFO: ExecWithOptions: Clientset creation
Aug 25 04:21:23.052: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-908/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 25 04:21:23.152: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 25 04:21:23.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-908" for this suite. 08/25/22 04:21:23.163
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":338,"skipped":6310,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [6.189 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:16.979
    Aug 25 04:21:16.979: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/25/22 04:21:16.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:16.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:16.997
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/25/22 04:21:17.003
    Aug 25 04:21:17.009: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-908" to be "running and ready"
    Aug 25 04:21:17.011: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.410185ms
    Aug 25 04:21:17.011: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:21:19.016: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006804821s
    Aug 25 04:21:19.016: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 25 04:21:19.016: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/25/22 04:21:19.025
    Aug 25 04:21:19.042: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-908" to be "container debugger running"
    Aug 25 04:21:19.045: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.195997ms
    Aug 25 04:21:21.049: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007286631s
    Aug 25 04:21:23.051: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008855361s
    Aug 25 04:21:23.051: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/25/22 04:21:23.051
    Aug 25 04:21:23.051: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-908 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 25 04:21:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    Aug 25 04:21:23.052: INFO: ExecWithOptions: Clientset creation
    Aug 25 04:21:23.052: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-908/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 25 04:21:23.152: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 25 04:21:23.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-908" for this suite. 08/25/22 04:21:23.163
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:23.168
Aug 25 04:21:23.168: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:21:23.17
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:23.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:23.185
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-1c1efd8f-81c0-4dd4-9f1b-f65ab3b2c980 08/25/22 04:21:23.193
STEP: Creating the pod 08/25/22 04:21:23.197
Aug 25 04:21:23.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0" in namespace "configmap-4812" to be "running and ready"
Aug 25 04:21:23.206: INFO: Pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134813ms
Aug 25 04:21:23.206: INFO: The phase of Pod pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:21:25.211: INFO: Pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006606403s
Aug 25 04:21:25.211: INFO: The phase of Pod pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0 is Running (Ready = true)
Aug 25 04:21:25.211: INFO: Pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-1c1efd8f-81c0-4dd4-9f1b-f65ab3b2c980 08/25/22 04:21:25.219
STEP: waiting to observe update in volume 08/25/22 04:21:25.223
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:21:27.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4812" for this suite. 08/25/22 04:21:27.239
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":339,"skipped":6313,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.076 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:23.168
    Aug 25 04:21:23.168: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:21:23.17
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:23.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:23.185
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-1c1efd8f-81c0-4dd4-9f1b-f65ab3b2c980 08/25/22 04:21:23.193
    STEP: Creating the pod 08/25/22 04:21:23.197
    Aug 25 04:21:23.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0" in namespace "configmap-4812" to be "running and ready"
    Aug 25 04:21:23.206: INFO: Pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134813ms
    Aug 25 04:21:23.206: INFO: The phase of Pod pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:21:25.211: INFO: Pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006606403s
    Aug 25 04:21:25.211: INFO: The phase of Pod pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0 is Running (Ready = true)
    Aug 25 04:21:25.211: INFO: Pod "pod-configmaps-18dcf914-492f-4eab-bb73-7aecd7d3d9e0" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-1c1efd8f-81c0-4dd4-9f1b-f65ab3b2c980 08/25/22 04:21:25.219
    STEP: waiting to observe update in volume 08/25/22 04:21:25.223
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:21:27.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4812" for this suite. 08/25/22 04:21:27.239
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:27.245
Aug 25 04:21:27.245: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename webhook 08/25/22 04:21:27.246
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:27.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:27.262
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/25/22 04:21:27.274
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 04:21:27.64
STEP: Deploying the webhook pod 08/25/22 04:21:27.648
STEP: Wait for the deployment to be ready 08/25/22 04:21:27.659
Aug 25 04:21:27.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/25/22 04:21:29.68
STEP: Verifying the service has paired with the endpoint 08/25/22 04:21:29.689
Aug 25 04:21:30.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 08/25/22 04:21:30.693
STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 04:21:30.714
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/25/22 04:21:30.727
STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 04:21:30.738
STEP: Patching a validating webhook configuration's rules to include the create operation 08/25/22 04:21:30.746
STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 04:21:30.754
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 25 04:21:30.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5684" for this suite. 08/25/22 04:21:30.767
STEP: Destroying namespace "webhook-5684-markers" for this suite. 08/25/22 04:21:30.772
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":340,"skipped":6316,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [3.558 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:27.245
    Aug 25 04:21:27.245: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename webhook 08/25/22 04:21:27.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:27.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:27.262
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/25/22 04:21:27.274
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/25/22 04:21:27.64
    STEP: Deploying the webhook pod 08/25/22 04:21:27.648
    STEP: Wait for the deployment to be ready 08/25/22 04:21:27.659
    Aug 25 04:21:27.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/25/22 04:21:29.68
    STEP: Verifying the service has paired with the endpoint 08/25/22 04:21:29.689
    Aug 25 04:21:30.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 08/25/22 04:21:30.693
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 04:21:30.714
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/25/22 04:21:30.727
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 04:21:30.738
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/25/22 04:21:30.746
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/25/22 04:21:30.754
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 25 04:21:30.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5684" for this suite. 08/25/22 04:21:30.767
    STEP: Destroying namespace "webhook-5684-markers" for this suite. 08/25/22 04:21:30.772
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:30.807
Aug 25 04:21:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 04:21:30.808
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:30.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:30.821
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-9009 08/25/22 04:21:30.824
STEP: creating service affinity-nodeport-transition in namespace services-9009 08/25/22 04:21:30.824
STEP: creating replication controller affinity-nodeport-transition in namespace services-9009 08/25/22 04:21:30.833
I0825 04:21:30.837304      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9009, replica count: 3
I0825 04:21:33.887639      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 04:21:33.898: INFO: Creating new exec pod
Aug 25 04:21:33.902: INFO: Waiting up to 5m0s for pod "execpod-affinityjn28v" in namespace "services-9009" to be "running"
Aug 25 04:21:33.905: INFO: Pod "execpod-affinityjn28v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196699ms
Aug 25 04:21:35.908: INFO: Pod "execpod-affinityjn28v": Phase="Running", Reason="", readiness=true. Elapsed: 2.006424929s
Aug 25 04:21:35.908: INFO: Pod "execpod-affinityjn28v" satisfied condition "running"
Aug 25 04:21:36.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 25 04:21:37.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 25 04:21:37.114: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:21:37.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.87.36 80'
Aug 25 04:21:37.296: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.87.36 80\nConnection to 10.111.87.36 80 port [tcp/http] succeeded!\n"
Aug 25 04:21:37.296: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:21:37.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 32652'
Aug 25 04:21:37.477: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 32652\nConnection to 86.109.11.217 32652 port [tcp/*] succeeded!\n"
Aug 25 04:21:37.477: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:21:37.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:32652/ ; done'
Aug 25 04:21:37.763: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n"
Aug 25 04:21:37.763: INFO: stdout: "\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-dfg6z\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-dfg6z\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-dfg6z\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-dfg6z"
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
Aug 25 04:21:37.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:32652/ ; done'
Aug 25 04:21:38.034: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n"
Aug 25 04:21:38.034: INFO: stdout: "\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj"
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
Aug 25 04:21:38.034: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9009, will wait for the garbage collector to delete the pods 08/25/22 04:21:38.043
Aug 25 04:21:38.103: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.587129ms
Aug 25 04:21:38.204: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.311094ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 04:21:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9009" for this suite. 08/25/22 04:21:40.022
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":341,"skipped":6391,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [9.218 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:30.807
    Aug 25 04:21:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 04:21:30.808
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:30.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:30.821
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-9009 08/25/22 04:21:30.824
    STEP: creating service affinity-nodeport-transition in namespace services-9009 08/25/22 04:21:30.824
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9009 08/25/22 04:21:30.833
    I0825 04:21:30.837304      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9009, replica count: 3
    I0825 04:21:33.887639      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 04:21:33.898: INFO: Creating new exec pod
    Aug 25 04:21:33.902: INFO: Waiting up to 5m0s for pod "execpod-affinityjn28v" in namespace "services-9009" to be "running"
    Aug 25 04:21:33.905: INFO: Pod "execpod-affinityjn28v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196699ms
    Aug 25 04:21:35.908: INFO: Pod "execpod-affinityjn28v": Phase="Running", Reason="", readiness=true. Elapsed: 2.006424929s
    Aug 25 04:21:35.908: INFO: Pod "execpod-affinityjn28v" satisfied condition "running"
    Aug 25 04:21:36.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Aug 25 04:21:37.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 25 04:21:37.114: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:21:37.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.87.36 80'
    Aug 25 04:21:37.296: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.87.36 80\nConnection to 10.111.87.36 80 port [tcp/http] succeeded!\n"
    Aug 25 04:21:37.296: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:21:37.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 86.109.11.217 32652'
    Aug 25 04:21:37.477: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 86.109.11.217 32652\nConnection to 86.109.11.217 32652 port [tcp/*] succeeded!\n"
    Aug 25 04:21:37.477: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:21:37.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:32652/ ; done'
    Aug 25 04:21:37.763: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n"
    Aug 25 04:21:37.763: INFO: stdout: "\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-dfg6z\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-dfg6z\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-dfg6z\naffinity-nodeport-transition-k5b6d\naffinity-nodeport-transition-dfg6z"
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-k5b6d
    Aug 25 04:21:37.763: INFO: Received response from host: affinity-nodeport-transition-dfg6z
    Aug 25 04:21:37.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9009 exec execpod-affinityjn28v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://86.109.11.217:32652/ ; done'
    Aug 25 04:21:38.034: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n+ echo\n+ curl -q -s --connect-timeout 2 http://86.109.11.217:32652/\n"
    Aug 25 04:21:38.034: INFO: stdout: "\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj\naffinity-nodeport-transition-s46jj"
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Received response from host: affinity-nodeport-transition-s46jj
    Aug 25 04:21:38.034: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9009, will wait for the garbage collector to delete the pods 08/25/22 04:21:38.043
    Aug 25 04:21:38.103: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.587129ms
    Aug 25 04:21:38.204: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.311094ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 04:21:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9009" for this suite. 08/25/22 04:21:40.022
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:21:40.026
Aug 25 04:21:40.026: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename services 08/25/22 04:21:40.027
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:40.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:40.043
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-9862 08/25/22 04:21:40.048
Aug 25 04:21:40.054: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9862" to be "running and ready"
Aug 25 04:21:40.056: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83926ms
Aug 25 04:21:40.056: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:21:42.061: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007418454s
Aug 25 04:21:42.061: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 25 04:21:42.061: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Aug 25 04:21:42.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 25 04:21:42.253: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 25 04:21:42.253: INFO: stdout: "iptables"
Aug 25 04:21:42.253: INFO: proxyMode: iptables
Aug 25 04:21:42.259: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 25 04:21:42.264: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9862 08/25/22 04:21:42.264
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9862 08/25/22 04:21:42.272
I0825 04:21:42.275004      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9862, replica count: 3
I0825 04:21:45.326205      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 25 04:21:45.330: INFO: Creating new exec pod
Aug 25 04:21:45.333: INFO: Waiting up to 5m0s for pod "execpod-affinitym6knl" in namespace "services-9862" to be "running"
Aug 25 04:21:45.335: INFO: Pod "execpod-affinitym6knl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214261ms
Aug 25 04:21:47.341: INFO: Pod "execpod-affinitym6knl": Phase="Running", Reason="", readiness=true. Elapsed: 2.008080369s
Aug 25 04:21:47.341: INFO: Pod "execpod-affinitym6knl" satisfied condition "running"
Aug 25 04:21:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Aug 25 04:21:48.551: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 25 04:21:48.551: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:21:48.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.139.134 80'
Aug 25 04:21:48.736: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.139.134 80\nConnection to 10.99.139.134 80 port [tcp/http] succeeded!\n"
Aug 25 04:21:48.736: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 25 04:21:48.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.139.134:80/ ; done'
Aug 25 04:21:49.024: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n"
Aug 25 04:21:49.024: INFO: stdout: "\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v"
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
Aug 25 04:21:49.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.139.134:80/'
Aug 25 04:21:49.185: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n"
Aug 25 04:21:49.185: INFO: stdout: "affinity-clusterip-timeout-q662v"
Aug 25 04:22:09.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.139.134:80/'
Aug 25 04:22:09.387: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n"
Aug 25 04:22:09.387: INFO: stdout: "affinity-clusterip-timeout-rjzx5"
Aug 25 04:22:09.387: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9862, will wait for the garbage collector to delete the pods 08/25/22 04:22:09.397
Aug 25 04:22:09.457: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.275222ms
Aug 25 04:22:09.557: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.329742ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 25 04:22:11.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9862" for this suite. 08/25/22 04:22:11.575
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":342,"skipped":6395,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [31.553 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:21:40.026
    Aug 25 04:21:40.026: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename services 08/25/22 04:21:40.027
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:21:40.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:21:40.043
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-9862 08/25/22 04:21:40.048
    Aug 25 04:21:40.054: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9862" to be "running and ready"
    Aug 25 04:21:40.056: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83926ms
    Aug 25 04:21:40.056: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:21:42.061: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007418454s
    Aug 25 04:21:42.061: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Aug 25 04:21:42.061: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Aug 25 04:21:42.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Aug 25 04:21:42.253: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Aug 25 04:21:42.253: INFO: stdout: "iptables"
    Aug 25 04:21:42.253: INFO: proxyMode: iptables
    Aug 25 04:21:42.259: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Aug 25 04:21:42.264: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-9862 08/25/22 04:21:42.264
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-9862 08/25/22 04:21:42.272
    I0825 04:21:42.275004      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9862, replica count: 3
    I0825 04:21:45.326205      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 25 04:21:45.330: INFO: Creating new exec pod
    Aug 25 04:21:45.333: INFO: Waiting up to 5m0s for pod "execpod-affinitym6knl" in namespace "services-9862" to be "running"
    Aug 25 04:21:45.335: INFO: Pod "execpod-affinitym6knl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214261ms
    Aug 25 04:21:47.341: INFO: Pod "execpod-affinitym6knl": Phase="Running", Reason="", readiness=true. Elapsed: 2.008080369s
    Aug 25 04:21:47.341: INFO: Pod "execpod-affinitym6knl" satisfied condition "running"
    Aug 25 04:21:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Aug 25 04:21:48.551: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Aug 25 04:21:48.551: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:21:48.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.139.134 80'
    Aug 25 04:21:48.736: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.139.134 80\nConnection to 10.99.139.134 80 port [tcp/http] succeeded!\n"
    Aug 25 04:21:48.736: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 25 04:21:48.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.139.134:80/ ; done'
    Aug 25 04:21:49.024: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n"
    Aug 25 04:21:49.024: INFO: stdout: "\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v\naffinity-clusterip-timeout-q662v"
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Received response from host: affinity-clusterip-timeout-q662v
    Aug 25 04:21:49.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.139.134:80/'
    Aug 25 04:21:49.185: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n"
    Aug 25 04:21:49.185: INFO: stdout: "affinity-clusterip-timeout-q662v"
    Aug 25 04:22:09.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-239949510 --namespace=services-9862 exec execpod-affinitym6knl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.139.134:80/'
    Aug 25 04:22:09.387: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.139.134:80/\n"
    Aug 25 04:22:09.387: INFO: stdout: "affinity-clusterip-timeout-rjzx5"
    Aug 25 04:22:09.387: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9862, will wait for the garbage collector to delete the pods 08/25/22 04:22:09.397
    Aug 25 04:22:09.457: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.275222ms
    Aug 25 04:22:09.557: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.329742ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 25 04:22:11.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9862" for this suite. 08/25/22 04:22:11.575
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:11.58
Aug 25 04:22:11.580: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:22:11.581
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:11.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:11.596
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 08/25/22 04:22:11.6
Aug 25 04:22:11.607: INFO: Waiting up to 5m0s for pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811" in namespace "downward-api-1197" to be "Succeeded or Failed"
Aug 25 04:22:11.610: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913399ms
Aug 25 04:22:13.615: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008387905s
Aug 25 04:22:15.615: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008183331s
STEP: Saw pod success 08/25/22 04:22:15.615
Aug 25 04:22:15.615: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811" satisfied condition "Succeeded or Failed"
Aug 25 04:22:15.620: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-67366973-fde9-40f8-8026-a17157b0e811 container dapi-container: <nil>
STEP: delete the pod 08/25/22 04:22:15.626
Aug 25 04:22:15.635: INFO: Waiting for pod downward-api-67366973-fde9-40f8-8026-a17157b0e811 to disappear
Aug 25 04:22:15.639: INFO: Pod downward-api-67366973-fde9-40f8-8026-a17157b0e811 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 25 04:22:15.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1197" for this suite. 08/25/22 04:22:15.644
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":343,"skipped":6402,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.074 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:11.58
    Aug 25 04:22:11.580: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:22:11.581
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:11.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:11.596
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 08/25/22 04:22:11.6
    Aug 25 04:22:11.607: INFO: Waiting up to 5m0s for pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811" in namespace "downward-api-1197" to be "Succeeded or Failed"
    Aug 25 04:22:11.610: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913399ms
    Aug 25 04:22:13.615: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008387905s
    Aug 25 04:22:15.615: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008183331s
    STEP: Saw pod success 08/25/22 04:22:15.615
    Aug 25 04:22:15.615: INFO: Pod "downward-api-67366973-fde9-40f8-8026-a17157b0e811" satisfied condition "Succeeded or Failed"
    Aug 25 04:22:15.620: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downward-api-67366973-fde9-40f8-8026-a17157b0e811 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 04:22:15.626
    Aug 25 04:22:15.635: INFO: Waiting for pod downward-api-67366973-fde9-40f8-8026-a17157b0e811 to disappear
    Aug 25 04:22:15.639: INFO: Pod downward-api-67366973-fde9-40f8-8026-a17157b0e811 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 25 04:22:15.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1197" for this suite. 08/25/22 04:22:15.644
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:15.655
Aug 25 04:22:15.655: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename sched-pred 08/25/22 04:22:15.656
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:15.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:15.668
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 25 04:22:15.671: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 25 04:22:15.676: INFO: Waiting for terminating namespaces to be deleted...
Aug 25 04:22:15.680: INFO: 
Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
Aug 25 04:22:15.709: INFO: environment-0 from bobymcbobs started at 2022-08-25 03:25:19 +0000 UTC (2 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container environment ready: true, restart count 0
Aug 25 04:22:15.709: INFO: 	Container environment-exporter ready: true, restart count 0
Aug 25 04:22:15.709: INFO: cert-manager-66bd77df8f-qlhqg from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:22:15.709: INFO: cert-manager-cainjector-6495667ff4-x6lll from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:22:15.709: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 from cert-manager started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container cert-manager ready: true, restart count 0
Aug 25 04:22:15.709: INFO: contour-588fc6cc6d-dq6wf from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:22:15.709: INFO: contour-588fc6cc6d-f66zz from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:22:15.709: INFO: envoy-cxlbl from contour-external started at 2022-08-25 03:25:39 +0000 UTC (2 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:22:15.709: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:22:15.709: INFO: contour-8597b78798-5755z from contour-internal started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:22:15.709: INFO: contour-8597b78798-lb4wb from contour-internal started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
Aug 25 04:22:15.709: INFO: envoy-7sxnc from contour-internal started at 2022-08-25 03:25:38 +0000 UTC (2 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container envoy ready: true, restart count 0
Aug 25 04:22:15.709: INFO: 	Container shutdown-manager ready: true, restart count 0
Aug 25 04:22:15.709: INFO: external-dns-67d79cbcd4-bf29h from external-dns started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container external-dns ready: true, restart count 0
Aug 25 04:22:15.709: INFO: helm-operator-7959478576-6mcjz from helm-operator started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container flux-helm-operator ready: true, restart count 0
Aug 25 04:22:15.709: INFO: knative-operator-594876444d-qzjs4 from knative-operator started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container knative-operator ready: true, restart count 0
Aug 25 04:22:15.709: INFO: activator-88b7df5c-cbxk8 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container activator ready: true, restart count 0
Aug 25 04:22:15.709: INFO: autoscaler-7bf8ff94db-98bhj from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container autoscaler ready: true, restart count 0
Aug 25 04:22:15.709: INFO: autoscaler-hpa-776c44cc57-rcqj4 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container autoscaler-hpa ready: true, restart count 0
Aug 25 04:22:15.709: INFO: controller-7fd644cbc6-dfb6n from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:22:15.709: INFO: domain-mapping-5bcd85fbc6-2gshd from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container domain-mapping ready: true, restart count 0
Aug 25 04:22:15.709: INFO: domainmapping-webhook-77f466bbc9-97ft7 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container domainmapping-webhook ready: true, restart count 0
Aug 25 04:22:15.709: INFO: net-certmanager-controller-6c8cb88879-86vmg from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:22:15.709: INFO: net-certmanager-webhook-79cfb96f68-kfnlr from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:22:15.709: INFO: net-contour-controller-5c5fd89fdb-lnf94 from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:22:15.709: INFO: webhook-69d55f5549-ghp24 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container webhook ready: true, restart count 0
Aug 25 04:22:15.709: INFO: coredns-565d847f94-684lz from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:22:15.709: INFO: coredns-565d847f94-lzgv5 from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container coredns ready: true, restart count 0
Aug 25 04:22:15.709: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container etcd ready: true, restart count 0
Aug 25 04:22:15.709: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 25 04:22:15.709: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 25 04:22:15.709: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 25 04:22:15.709: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 25 04:22:15.709: INFO: kubed-568bdbc6c4-vx7m6 from kube-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container kubed ready: true, restart count 0
Aug 25 04:22:15.709: INFO: metrics-server-5cb46dccf6-w9d6b from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container metrics-server ready: true, restart count 0
Aug 25 04:22:15.709: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container weave ready: true, restart count 1
Aug 25 04:22:15.709: INFO: 	Container weave-npc ready: true, restart count 0
Aug 25 04:22:15.709: INFO: local-path-provisioner-56c55476f9-xbkr9 from local-path-storage started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container local-path-provisioner ready: true, restart count 0
Aug 25 04:22:15.709: INFO: controller-77cc7ff558-mrwjl from metallb-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
Aug 25 04:22:15.709: INFO: speaker-f82r7 from metallb-system started at 2022-08-25 03:25:09 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container speaker ready: true, restart count 0
Aug 25 04:22:15.709: INFO: distribution-7f9dff85c4-c276b from pair-system started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container distribution ready: true, restart count 0
Aug 25 04:22:15.709: INFO: environment-exposer-5bb6d44465-8btrx from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container environment-exposer ready: true, restart count 0
Aug 25 04:22:15.709: INFO: powerdns-5c6dbcf579-rgzxs from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container powerdns ready: true, restart count 0
Aug 25 04:22:15.709: INFO: powerdns-db-7c897fddf4-l5cjb from pair-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container mariadb ready: true, restart count 0
Aug 25 04:22:15.709: INFO: public-html-go-http-server-55b9f584d4-4jrp5 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container go-http-server ready: true, restart count 0
Aug 25 04:22:15.709: INFO: reveal-multiplex-6bbbf59d6-k9lt8 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container reveal-multiplex ready: true, restart count 0
Aug 25 04:22:15.709: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 25 04:22:15.709: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container e2e ready: true, restart count 0
Aug 25 04:22:15.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:22:15.709: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
Aug 25 04:22:15.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 25 04:22:15.709: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node bobymcbobs-c849-control-plane-p55pp 08/25/22 04:22:15.742
Aug 25 04:22:15.775: INFO: Pod environment-0 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod cert-manager-66bd77df8f-qlhqg requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod cert-manager-cainjector-6495667ff4-x6lll requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod cert-manager-webhook-59d6cdfb6f-5fml5 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod contour-588fc6cc6d-dq6wf requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod contour-588fc6cc6d-f66zz requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod envoy-cxlbl requesting resource cpu=240m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod contour-8597b78798-5755z requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod contour-8597b78798-lb4wb requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod envoy-7sxnc requesting resource cpu=240m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod external-dns-67d79cbcd4-bf29h requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod helm-operator-7959478576-6mcjz requesting resource cpu=50m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod knative-operator-594876444d-qzjs4 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod activator-88b7df5c-cbxk8 requesting resource cpu=300m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod autoscaler-7bf8ff94db-98bhj requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod autoscaler-hpa-776c44cc57-rcqj4 requesting resource cpu=30m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod controller-7fd644cbc6-dfb6n requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod domain-mapping-5bcd85fbc6-2gshd requesting resource cpu=30m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod domainmapping-webhook-77f466bbc9-97ft7 requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod net-certmanager-controller-6c8cb88879-86vmg requesting resource cpu=30m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod net-certmanager-webhook-79cfb96f68-kfnlr requesting resource cpu=20m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod net-contour-controller-5c5fd89fdb-lnf94 requesting resource cpu=40m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod webhook-69d55f5549-ghp24 requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod coredns-565d847f94-684lz requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod coredns-565d847f94-lzgv5 requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod etcd-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod kube-apiserver-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=250m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=200m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod kube-proxy-b4lgh requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod kube-scheduler-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod kubed-568bdbc6c4-vx7m6 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod metrics-server-5cb46dccf6-w9d6b requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod weave-net-75fql requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod local-path-provisioner-56c55476f9-xbkr9 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod controller-77cc7ff558-mrwjl requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod speaker-f82r7 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod distribution-7f9dff85c4-c276b requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod environment-exposer-5bb6d44465-8btrx requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod powerdns-5c6dbcf579-rgzxs requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod powerdns-db-7c897fddf4-l5cjb requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod public-html-go-http-server-55b9f584d4-4jrp5 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod reveal-multiplex-6bbbf59d6-k9lt8 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod sonobuoy requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod sonobuoy-e2e-job-39386ef5d0694a49 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.775: INFO: Pod sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
STEP: Starting Pods to consume most of the cluster CPU. 08/25/22 04:22:15.775
Aug 25 04:22:15.775: INFO: Creating a pod which consumes cpu=43169m on Node bobymcbobs-c849-control-plane-p55pp
Aug 25 04:22:15.781: INFO: Waiting up to 5m0s for pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9" in namespace "sched-pred-1064" to be "running"
Aug 25 04:22:15.784: INFO: Pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.943775ms
Aug 25 04:22:17.790: INFO: Pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008343268s
Aug 25 04:22:17.790: INFO: Pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/25/22 04:22:17.79
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb660cff4ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1064/filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9 to bobymcbobs-c849-control-plane-p55pp] 08/25/22 04:22:17.794
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb6884fcab9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 08/25/22 04:22:17.794
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb689014a55], Reason = [Created], Message = [Created container filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9] 08/25/22 04:22:17.795
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb690016a48], Reason = [Started], Message = [Started container filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9] 08/25/22 04:22:17.795
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.170e7bb6d9122910], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.] 08/25/22 04:22:17.806
STEP: removing the label node off the node bobymcbobs-c849-control-plane-p55pp 08/25/22 04:22:18.809
STEP: verifying the node doesn't have the label node 08/25/22 04:22:18.824
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:22:18.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1064" for this suite. 08/25/22 04:22:18.831
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":344,"skipped":6406,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [3.183 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:15.655
    Aug 25 04:22:15.655: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename sched-pred 08/25/22 04:22:15.656
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:15.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:15.668
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 25 04:22:15.671: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 25 04:22:15.676: INFO: Waiting for terminating namespaces to be deleted...
    Aug 25 04:22:15.680: INFO: 
    Logging pods the apiserver thinks is on node bobymcbobs-c849-control-plane-p55pp before test
    Aug 25 04:22:15.709: INFO: environment-0 from bobymcbobs started at 2022-08-25 03:25:19 +0000 UTC (2 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container environment ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: 	Container environment-exporter ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: cert-manager-66bd77df8f-qlhqg from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: cert-manager-cainjector-6495667ff4-x6lll from cert-manager started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: cert-manager-webhook-59d6cdfb6f-5fml5 from cert-manager started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container cert-manager ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: contour-588fc6cc6d-dq6wf from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: contour-588fc6cc6d-f66zz from contour-external started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: envoy-cxlbl from contour-external started at 2022-08-25 03:25:39 +0000 UTC (2 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: contour-8597b78798-5755z from contour-internal started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: contour-8597b78798-lb4wb from contour-internal started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container contour ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: envoy-7sxnc from contour-internal started at 2022-08-25 03:25:38 +0000 UTC (2 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container envoy ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: 	Container shutdown-manager ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: external-dns-67d79cbcd4-bf29h from external-dns started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container external-dns ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: helm-operator-7959478576-6mcjz from helm-operator started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container flux-helm-operator ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: knative-operator-594876444d-qzjs4 from knative-operator started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container knative-operator ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: activator-88b7df5c-cbxk8 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container activator ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: autoscaler-7bf8ff94db-98bhj from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: autoscaler-hpa-776c44cc57-rcqj4 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container autoscaler-hpa ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: controller-7fd644cbc6-dfb6n from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: domain-mapping-5bcd85fbc6-2gshd from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container domain-mapping ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: domainmapping-webhook-77f466bbc9-97ft7 from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container domainmapping-webhook ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: net-certmanager-controller-6c8cb88879-86vmg from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: net-certmanager-webhook-79cfb96f68-kfnlr from knative-serving started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: net-contour-controller-5c5fd89fdb-lnf94 from knative-serving started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: webhook-69d55f5549-ghp24 from knative-serving started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container webhook ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: coredns-565d847f94-684lz from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: coredns-565d847f94-lzgv5 from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container coredns ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: etcd-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container etcd ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: kube-apiserver-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: kube-controller-manager-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:33 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: kube-proxy-b4lgh from kube-system started at 2022-08-25 02:40:45 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: kube-scheduler-bobymcbobs-c849-control-plane-p55pp from kube-system started at 2022-08-25 02:40:32 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: kubed-568bdbc6c4-vx7m6 from kube-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container kubed ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: metrics-server-5cb46dccf6-w9d6b from kube-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: weave-net-75fql from kube-system started at 2022-08-25 02:40:45 +0000 UTC (2 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container weave ready: true, restart count 1
    Aug 25 04:22:15.709: INFO: 	Container weave-npc ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: local-path-provisioner-56c55476f9-xbkr9 from local-path-storage started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: controller-77cc7ff558-mrwjl from metallb-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container controller ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: speaker-f82r7 from metallb-system started at 2022-08-25 03:25:09 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container speaker ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: distribution-7f9dff85c4-c276b from pair-system started at 2022-08-25 03:24:39 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container distribution ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: environment-exposer-5bb6d44465-8btrx from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container environment-exposer ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: powerdns-5c6dbcf579-rgzxs from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container powerdns ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: powerdns-db-7c897fddf4-l5cjb from pair-system started at 2022-08-25 03:24:38 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container mariadb ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: public-html-go-http-server-55b9f584d4-4jrp5 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container go-http-server ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: reveal-multiplex-6bbbf59d6-k9lt8 from pair-system started at 2022-08-25 03:24:37 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container reveal-multiplex ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: sonobuoy from sonobuoy started at 2022-08-25 02:48:34 +0000 UTC (1 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: sonobuoy-e2e-job-39386ef5d0694a49 from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container e2e ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj from sonobuoy started at 2022-08-25 02:48:37 +0000 UTC (2 container statuses recorded)
    Aug 25 04:22:15.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 25 04:22:15.709: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node bobymcbobs-c849-control-plane-p55pp 08/25/22 04:22:15.742
    Aug 25 04:22:15.775: INFO: Pod environment-0 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod cert-manager-66bd77df8f-qlhqg requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod cert-manager-cainjector-6495667ff4-x6lll requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod cert-manager-webhook-59d6cdfb6f-5fml5 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod contour-588fc6cc6d-dq6wf requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod contour-588fc6cc6d-f66zz requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod envoy-cxlbl requesting resource cpu=240m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod contour-8597b78798-5755z requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod contour-8597b78798-lb4wb requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod envoy-7sxnc requesting resource cpu=240m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod external-dns-67d79cbcd4-bf29h requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod helm-operator-7959478576-6mcjz requesting resource cpu=50m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod knative-operator-594876444d-qzjs4 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod activator-88b7df5c-cbxk8 requesting resource cpu=300m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod autoscaler-7bf8ff94db-98bhj requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod autoscaler-hpa-776c44cc57-rcqj4 requesting resource cpu=30m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod controller-7fd644cbc6-dfb6n requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod domain-mapping-5bcd85fbc6-2gshd requesting resource cpu=30m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod domainmapping-webhook-77f466bbc9-97ft7 requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod net-certmanager-controller-6c8cb88879-86vmg requesting resource cpu=30m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod net-certmanager-webhook-79cfb96f68-kfnlr requesting resource cpu=20m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod net-contour-controller-5c5fd89fdb-lnf94 requesting resource cpu=40m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod webhook-69d55f5549-ghp24 requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod coredns-565d847f94-684lz requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod coredns-565d847f94-lzgv5 requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod etcd-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod kube-apiserver-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=250m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod kube-controller-manager-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=200m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod kube-proxy-b4lgh requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod kube-scheduler-bobymcbobs-c849-control-plane-p55pp requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod kubed-568bdbc6c4-vx7m6 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod metrics-server-5cb46dccf6-w9d6b requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod weave-net-75fql requesting resource cpu=100m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod local-path-provisioner-56c55476f9-xbkr9 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod controller-77cc7ff558-mrwjl requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod speaker-f82r7 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod distribution-7f9dff85c4-c276b requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod environment-exposer-5bb6d44465-8btrx requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod powerdns-5c6dbcf579-rgzxs requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod powerdns-db-7c897fddf4-l5cjb requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod public-html-go-http-server-55b9f584d4-4jrp5 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod reveal-multiplex-6bbbf59d6-k9lt8 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod sonobuoy requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod sonobuoy-e2e-job-39386ef5d0694a49 requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.775: INFO: Pod sonobuoy-systemd-logs-daemon-set-f2dc82bbf24a425c-wlrdj requesting resource cpu=0m on Node bobymcbobs-c849-control-plane-p55pp
    STEP: Starting Pods to consume most of the cluster CPU. 08/25/22 04:22:15.775
    Aug 25 04:22:15.775: INFO: Creating a pod which consumes cpu=43169m on Node bobymcbobs-c849-control-plane-p55pp
    Aug 25 04:22:15.781: INFO: Waiting up to 5m0s for pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9" in namespace "sched-pred-1064" to be "running"
    Aug 25 04:22:15.784: INFO: Pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.943775ms
    Aug 25 04:22:17.790: INFO: Pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008343268s
    Aug 25 04:22:17.790: INFO: Pod "filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/25/22 04:22:17.79
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb660cff4ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1064/filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9 to bobymcbobs-c849-control-plane-p55pp] 08/25/22 04:22:17.794
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb6884fcab9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 08/25/22 04:22:17.794
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb689014a55], Reason = [Created], Message = [Created container filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9] 08/25/22 04:22:17.795
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9.170e7bb690016a48], Reason = [Started], Message = [Started container filler-pod-3bf11a70-070b-444a-a81b-f8935d0ba1c9] 08/25/22 04:22:17.795
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.170e7bb6d9122910], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.] 08/25/22 04:22:17.806
    STEP: removing the label node off the node bobymcbobs-c849-control-plane-p55pp 08/25/22 04:22:18.809
    STEP: verifying the node doesn't have the label node 08/25/22 04:22:18.824
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:22:18.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1064" for this suite. 08/25/22 04:22:18.831
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:18.839
Aug 25 04:22:18.839: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename secrets 08/25/22 04:22:18.84
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:18.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:18.854
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-b5cdbf93-41a4-4d50-9506-94e6cad314e7 08/25/22 04:22:18.857
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 25 04:22:18.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9087" for this suite. 08/25/22 04:22:18.861
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":345,"skipped":6411,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.031 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:18.839
    Aug 25 04:22:18.839: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename secrets 08/25/22 04:22:18.84
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:18.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:18.854
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-b5cdbf93-41a4-4d50-9506-94e6cad314e7 08/25/22 04:22:18.857
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 25 04:22:18.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9087" for this suite. 08/25/22 04:22:18.861
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:18.871
Aug 25 04:22:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename runtimeclass 08/25/22 04:22:18.872
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:18.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:18.886
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3837-delete-me 08/25/22 04:22:18.893
STEP: Waiting for the RuntimeClass to disappear 08/25/22 04:22:18.897
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 25 04:22:18.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3837" for this suite. 08/25/22 04:22:18.91
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":346,"skipped":6419,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.043 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:18.871
    Aug 25 04:22:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename runtimeclass 08/25/22 04:22:18.872
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:18.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:18.886
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3837-delete-me 08/25/22 04:22:18.893
    STEP: Waiting for the RuntimeClass to disappear 08/25/22 04:22:18.897
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 25 04:22:18.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3837" for this suite. 08/25/22 04:22:18.91
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:18.916
Aug 25 04:22:18.916: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename controllerrevisions 08/25/22 04:22:18.917
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:18.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:18.932
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-csf44-daemon-set" 08/25/22 04:22:18.944
STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:22:18.949
Aug 25 04:22:18.955: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 0
Aug 25 04:22:18.955: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:22:19.965: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 0
Aug 25 04:22:19.965: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
Aug 25 04:22:20.961: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 1
Aug 25 04:22:20.961: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset e2e-csf44-daemon-set
STEP: Confirm DaemonSet "e2e-csf44-daemon-set" successfully created with "daemonset-name=e2e-csf44-daemon-set" label 08/25/22 04:22:20.964
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-csf44-daemon-set" 08/25/22 04:22:20.972
Aug 25 04:22:20.975: INFO: Located ControllerRevision: "e2e-csf44-daemon-set-8494d99547"
STEP: Patching ControllerRevision "e2e-csf44-daemon-set-8494d99547" 08/25/22 04:22:20.977
Aug 25 04:22:20.985: INFO: e2e-csf44-daemon-set-8494d99547 has been patched
STEP: Create a new ControllerRevision 08/25/22 04:22:20.985
Aug 25 04:22:20.989: INFO: Created ControllerRevision: e2e-csf44-daemon-set-86d598764c
STEP: Confirm that there are two ControllerRevisions 08/25/22 04:22:20.989
Aug 25 04:22:20.989: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 25 04:22:20.992: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-csf44-daemon-set-8494d99547" 08/25/22 04:22:20.992
STEP: Confirm that there is only one ControllerRevision 08/25/22 04:22:20.996
Aug 25 04:22:20.996: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 25 04:22:20.998: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-csf44-daemon-set-86d598764c" 08/25/22 04:22:21.001
Aug 25 04:22:21.009: INFO: e2e-csf44-daemon-set-86d598764c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/25/22 04:22:21.009
W0825 04:22:21.014544      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/25/22 04:22:21.014
Aug 25 04:22:21.014: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 25 04:22:22.018: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 25 04:22:22.023: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-csf44-daemon-set-86d598764c=updated" 08/25/22 04:22:22.023
STEP: Confirm that there is only one ControllerRevision 08/25/22 04:22:22.029
Aug 25 04:22:22.029: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 25 04:22:22.033: INFO: Found 1 ControllerRevisions
Aug 25 04:22:22.036: INFO: ControllerRevision "e2e-csf44-daemon-set-6d6f6685bb" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-csf44-daemon-set" 08/25/22 04:22:22.04
STEP: deleting DaemonSet.extensions e2e-csf44-daemon-set in namespace controllerrevisions-9436, will wait for the garbage collector to delete the pods 08/25/22 04:22:22.041
Aug 25 04:22:22.101: INFO: Deleting DaemonSet.extensions e2e-csf44-daemon-set took: 5.563683ms
Aug 25 04:22:22.101: INFO: Terminating DaemonSet.extensions e2e-csf44-daemon-set pods took: 40.156µs
Aug 25 04:22:23.205: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 0
Aug 25 04:22:23.205: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-csf44-daemon-set
Aug 25 04:22:23.209: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"57696"},"items":null}

Aug 25 04:22:23.211: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"57696"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Aug 25 04:22:23.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-9436" for this suite. 08/25/22 04:22:23.223
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":347,"skipped":6431,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.313 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:18.916
    Aug 25 04:22:18.916: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename controllerrevisions 08/25/22 04:22:18.917
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:18.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:18.932
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-csf44-daemon-set" 08/25/22 04:22:18.944
    STEP: Check that daemon pods launch on every node of the cluster. 08/25/22 04:22:18.949
    Aug 25 04:22:18.955: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 0
    Aug 25 04:22:18.955: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:22:19.965: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 0
    Aug 25 04:22:19.965: INFO: Node bobymcbobs-c849-control-plane-p55pp is running 0 daemon pod, expected 1
    Aug 25 04:22:20.961: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 1
    Aug 25 04:22:20.961: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset e2e-csf44-daemon-set
    STEP: Confirm DaemonSet "e2e-csf44-daemon-set" successfully created with "daemonset-name=e2e-csf44-daemon-set" label 08/25/22 04:22:20.964
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-csf44-daemon-set" 08/25/22 04:22:20.972
    Aug 25 04:22:20.975: INFO: Located ControllerRevision: "e2e-csf44-daemon-set-8494d99547"
    STEP: Patching ControllerRevision "e2e-csf44-daemon-set-8494d99547" 08/25/22 04:22:20.977
    Aug 25 04:22:20.985: INFO: e2e-csf44-daemon-set-8494d99547 has been patched
    STEP: Create a new ControllerRevision 08/25/22 04:22:20.985
    Aug 25 04:22:20.989: INFO: Created ControllerRevision: e2e-csf44-daemon-set-86d598764c
    STEP: Confirm that there are two ControllerRevisions 08/25/22 04:22:20.989
    Aug 25 04:22:20.989: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 25 04:22:20.992: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-csf44-daemon-set-8494d99547" 08/25/22 04:22:20.992
    STEP: Confirm that there is only one ControllerRevision 08/25/22 04:22:20.996
    Aug 25 04:22:20.996: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 25 04:22:20.998: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-csf44-daemon-set-86d598764c" 08/25/22 04:22:21.001
    Aug 25 04:22:21.009: INFO: e2e-csf44-daemon-set-86d598764c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/25/22 04:22:21.009
    W0825 04:22:21.014544      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/25/22 04:22:21.014
    Aug 25 04:22:21.014: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 25 04:22:22.018: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 25 04:22:22.023: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-csf44-daemon-set-86d598764c=updated" 08/25/22 04:22:22.023
    STEP: Confirm that there is only one ControllerRevision 08/25/22 04:22:22.029
    Aug 25 04:22:22.029: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 25 04:22:22.033: INFO: Found 1 ControllerRevisions
    Aug 25 04:22:22.036: INFO: ControllerRevision "e2e-csf44-daemon-set-6d6f6685bb" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-csf44-daemon-set" 08/25/22 04:22:22.04
    STEP: deleting DaemonSet.extensions e2e-csf44-daemon-set in namespace controllerrevisions-9436, will wait for the garbage collector to delete the pods 08/25/22 04:22:22.041
    Aug 25 04:22:22.101: INFO: Deleting DaemonSet.extensions e2e-csf44-daemon-set took: 5.563683ms
    Aug 25 04:22:22.101: INFO: Terminating DaemonSet.extensions e2e-csf44-daemon-set pods took: 40.156µs
    Aug 25 04:22:23.205: INFO: Number of nodes with available pods controlled by daemonset e2e-csf44-daemon-set: 0
    Aug 25 04:22:23.205: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-csf44-daemon-set
    Aug 25 04:22:23.209: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"57696"},"items":null}

    Aug 25 04:22:23.211: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"57696"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Aug 25 04:22:23.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-9436" for this suite. 08/25/22 04:22:23.223
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:23.232
Aug 25 04:22:23.232: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename downward-api 08/25/22 04:22:23.233
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:23.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:23.249
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 08/25/22 04:22:23.252
Aug 25 04:22:23.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242" in namespace "downward-api-2725" to be "Succeeded or Failed"
Aug 25 04:22:23.261: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795217ms
Aug 25 04:22:25.265: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00716628s
Aug 25 04:22:27.267: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00920196s
STEP: Saw pod success 08/25/22 04:22:27.267
Aug 25 04:22:27.267: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242" satisfied condition "Succeeded or Failed"
Aug 25 04:22:27.272: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242 container client-container: <nil>
STEP: delete the pod 08/25/22 04:22:27.277
Aug 25 04:22:27.287: INFO: Waiting for pod downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242 to disappear
Aug 25 04:22:27.290: INFO: Pod downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 25 04:22:27.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2725" for this suite. 08/25/22 04:22:27.294
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":348,"skipped":6464,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.066 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:23.232
    Aug 25 04:22:23.232: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename downward-api 08/25/22 04:22:23.233
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:23.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:23.249
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 08/25/22 04:22:23.252
    Aug 25 04:22:23.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242" in namespace "downward-api-2725" to be "Succeeded or Failed"
    Aug 25 04:22:23.261: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795217ms
    Aug 25 04:22:25.265: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00716628s
    Aug 25 04:22:27.267: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00920196s
    STEP: Saw pod success 08/25/22 04:22:27.267
    Aug 25 04:22:27.267: INFO: Pod "downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242" satisfied condition "Succeeded or Failed"
    Aug 25 04:22:27.272: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242 container client-container: <nil>
    STEP: delete the pod 08/25/22 04:22:27.277
    Aug 25 04:22:27.287: INFO: Waiting for pod downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242 to disappear
    Aug 25 04:22:27.290: INFO: Pod downwardapi-volume-7028f8af-af69-4e37-a4f1-f213592b3242 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 25 04:22:27.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2725" for this suite. 08/25/22 04:22:27.294
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:22:27.301
Aug 25 04:22:27.301: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 04:22:27.302
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:27.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:27.319
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 08/25/22 04:22:27.321
Aug 25 04:22:27.328: INFO: Waiting up to 2m0s for pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" in namespace "var-expansion-3345" to be "running"
Aug 25 04:22:27.330: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.566581ms
Aug 25 04:22:29.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008001019s
Aug 25 04:22:31.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008050782s
Aug 25 04:22:33.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008269557s
Aug 25 04:22:35.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007978721s
Aug 25 04:22:37.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007461102s
Aug 25 04:22:39.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007769704s
Aug 25 04:22:41.334: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006856959s
Aug 25 04:22:43.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008239083s
Aug 25 04:22:45.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007110335s
Aug 25 04:22:47.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008012438s
Aug 25 04:22:49.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008368794s
Aug 25 04:22:51.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007931124s
Aug 25 04:22:53.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008057372s
Aug 25 04:22:55.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008430159s
Aug 25 04:22:57.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008307618s
Aug 25 04:22:59.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008968335s
Aug 25 04:23:01.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007832786s
Aug 25 04:23:03.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008912776s
Aug 25 04:23:05.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008338079s
Aug 25 04:23:07.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008174089s
Aug 25 04:23:09.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008182424s
Aug 25 04:23:11.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007091773s
Aug 25 04:23:13.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008498862s
Aug 25 04:23:15.334: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006687564s
Aug 25 04:23:17.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008800637s
Aug 25 04:23:19.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008868277s
Aug 25 04:23:21.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007320182s
Aug 25 04:23:23.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007998997s
Aug 25 04:23:25.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008365687s
Aug 25 04:23:27.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008947347s
Aug 25 04:23:29.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008541803s
Aug 25 04:23:31.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007255017s
Aug 25 04:23:33.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008965084s
Aug 25 04:23:35.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008345205s
Aug 25 04:23:37.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009146221s
Aug 25 04:23:39.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008197751s
Aug 25 04:23:41.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00717051s
Aug 25 04:23:43.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008770937s
Aug 25 04:23:45.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008925084s
Aug 25 04:23:47.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008778799s
Aug 25 04:23:49.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007808057s
Aug 25 04:23:51.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00797664s
Aug 25 04:23:53.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008029847s
Aug 25 04:23:55.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007585986s
Aug 25 04:23:57.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007698955s
Aug 25 04:23:59.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008883756s
Aug 25 04:24:01.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007504956s
Aug 25 04:24:03.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008683523s
Aug 25 04:24:05.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008908756s
Aug 25 04:24:07.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008611277s
Aug 25 04:24:09.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007664054s
Aug 25 04:24:11.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008006917s
Aug 25 04:24:13.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008434777s
Aug 25 04:24:15.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008593855s
Aug 25 04:24:17.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008416412s
Aug 25 04:24:19.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007413162s
Aug 25 04:24:21.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00747512s
Aug 25 04:24:23.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009109359s
Aug 25 04:24:25.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008516351s
Aug 25 04:24:27.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008229092s
Aug 25 04:24:27.340: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012202608s
STEP: updating the pod 08/25/22 04:24:27.34
Aug 25 04:24:27.854: INFO: Successfully updated pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f"
STEP: waiting for pod running 08/25/22 04:24:27.854
Aug 25 04:24:27.854: INFO: Waiting up to 2m0s for pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" in namespace "var-expansion-3345" to be "running"
Aug 25 04:24:27.858: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236875ms
Aug 25 04:24:29.863: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008696823s
Aug 25 04:24:29.863: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" satisfied condition "running"
STEP: deleting the pod gracefully 08/25/22 04:24:29.863
Aug 25 04:24:29.863: INFO: Deleting pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" in namespace "var-expansion-3345"
Aug 25 04:24:29.869: INFO: Wait up to 5m0s for pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 04:25:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3345" for this suite. 08/25/22 04:25:01.892
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":349,"skipped":6491,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [154.597 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:22:27.301
    Aug 25 04:22:27.301: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 04:22:27.302
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:22:27.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:22:27.319
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 08/25/22 04:22:27.321
    Aug 25 04:22:27.328: INFO: Waiting up to 2m0s for pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" in namespace "var-expansion-3345" to be "running"
    Aug 25 04:22:27.330: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.566581ms
    Aug 25 04:22:29.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008001019s
    Aug 25 04:22:31.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008050782s
    Aug 25 04:22:33.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008269557s
    Aug 25 04:22:35.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007978721s
    Aug 25 04:22:37.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007461102s
    Aug 25 04:22:39.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007769704s
    Aug 25 04:22:41.334: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006856959s
    Aug 25 04:22:43.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008239083s
    Aug 25 04:22:45.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007110335s
    Aug 25 04:22:47.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008012438s
    Aug 25 04:22:49.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008368794s
    Aug 25 04:22:51.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007931124s
    Aug 25 04:22:53.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008057372s
    Aug 25 04:22:55.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008430159s
    Aug 25 04:22:57.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008307618s
    Aug 25 04:22:59.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008968335s
    Aug 25 04:23:01.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007832786s
    Aug 25 04:23:03.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008912776s
    Aug 25 04:23:05.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008338079s
    Aug 25 04:23:07.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008174089s
    Aug 25 04:23:09.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008182424s
    Aug 25 04:23:11.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007091773s
    Aug 25 04:23:13.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008498862s
    Aug 25 04:23:15.334: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006687564s
    Aug 25 04:23:17.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008800637s
    Aug 25 04:23:19.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008868277s
    Aug 25 04:23:21.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007320182s
    Aug 25 04:23:23.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007998997s
    Aug 25 04:23:25.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008365687s
    Aug 25 04:23:27.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008947347s
    Aug 25 04:23:29.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008541803s
    Aug 25 04:23:31.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007255017s
    Aug 25 04:23:33.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008965084s
    Aug 25 04:23:35.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008345205s
    Aug 25 04:23:37.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009146221s
    Aug 25 04:23:39.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008197751s
    Aug 25 04:23:41.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00717051s
    Aug 25 04:23:43.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008770937s
    Aug 25 04:23:45.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008925084s
    Aug 25 04:23:47.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008778799s
    Aug 25 04:23:49.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007808057s
    Aug 25 04:23:51.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00797664s
    Aug 25 04:23:53.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008029847s
    Aug 25 04:23:55.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007585986s
    Aug 25 04:23:57.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007698955s
    Aug 25 04:23:59.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008883756s
    Aug 25 04:24:01.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007504956s
    Aug 25 04:24:03.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008683523s
    Aug 25 04:24:05.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008908756s
    Aug 25 04:24:07.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008611277s
    Aug 25 04:24:09.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007664054s
    Aug 25 04:24:11.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008006917s
    Aug 25 04:24:13.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008434777s
    Aug 25 04:24:15.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008593855s
    Aug 25 04:24:17.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008416412s
    Aug 25 04:24:19.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007413162s
    Aug 25 04:24:21.335: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00747512s
    Aug 25 04:24:23.337: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009109359s
    Aug 25 04:24:25.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008516351s
    Aug 25 04:24:27.336: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008229092s
    Aug 25 04:24:27.340: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012202608s
    STEP: updating the pod 08/25/22 04:24:27.34
    Aug 25 04:24:27.854: INFO: Successfully updated pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f"
    STEP: waiting for pod running 08/25/22 04:24:27.854
    Aug 25 04:24:27.854: INFO: Waiting up to 2m0s for pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" in namespace "var-expansion-3345" to be "running"
    Aug 25 04:24:27.858: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236875ms
    Aug 25 04:24:29.863: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008696823s
    Aug 25 04:24:29.863: INFO: Pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" satisfied condition "running"
    STEP: deleting the pod gracefully 08/25/22 04:24:29.863
    Aug 25 04:24:29.863: INFO: Deleting pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" in namespace "var-expansion-3345"
    Aug 25 04:24:29.869: INFO: Wait up to 5m0s for pod "var-expansion-988199c0-1de9-4190-9f06-9d72ab0bed7f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 04:25:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3345" for this suite. 08/25/22 04:25:01.892
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:01.898
Aug 25 04:25:01.898: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename pods 08/25/22 04:25:01.899
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:01.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:01.921
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/25/22 04:25:01.924
STEP: submitting the pod to kubernetes 08/25/22 04:25:01.924
STEP: verifying QOS class is set on the pod 08/25/22 04:25:01.929
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Aug 25 04:25:01.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8131" for this suite. 08/25/22 04:25:01.936
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":350,"skipped":6491,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.042 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:01.898
    Aug 25 04:25:01.898: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename pods 08/25/22 04:25:01.899
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:01.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:01.921
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/25/22 04:25:01.924
    STEP: submitting the pod to kubernetes 08/25/22 04:25:01.924
    STEP: verifying QOS class is set on the pod 08/25/22 04:25:01.929
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Aug 25 04:25:01.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8131" for this suite. 08/25/22 04:25:01.936
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:01.94
Aug 25 04:25:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename var-expansion 08/25/22 04:25:01.941
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:01.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:01.952
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 08/25/22 04:25:01.955
Aug 25 04:25:01.960: INFO: Waiting up to 5m0s for pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6" in namespace "var-expansion-3480" to be "Succeeded or Failed"
Aug 25 04:25:01.971: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.07413ms
Aug 25 04:25:03.977: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01677589s
Aug 25 04:25:05.976: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016485788s
STEP: Saw pod success 08/25/22 04:25:05.976
Aug 25 04:25:05.976: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6" satisfied condition "Succeeded or Failed"
Aug 25 04:25:05.981: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6 container dapi-container: <nil>
STEP: delete the pod 08/25/22 04:25:06.001
Aug 25 04:25:06.008: INFO: Waiting for pod var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6 to disappear
Aug 25 04:25:06.010: INFO: Pod var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 25 04:25:06.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3480" for this suite. 08/25/22 04:25:06.014
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":351,"skipped":6504,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.079 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:01.94
    Aug 25 04:25:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename var-expansion 08/25/22 04:25:01.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:01.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:01.952
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 08/25/22 04:25:01.955
    Aug 25 04:25:01.960: INFO: Waiting up to 5m0s for pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6" in namespace "var-expansion-3480" to be "Succeeded or Failed"
    Aug 25 04:25:01.971: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.07413ms
    Aug 25 04:25:03.977: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01677589s
    Aug 25 04:25:05.976: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016485788s
    STEP: Saw pod success 08/25/22 04:25:05.976
    Aug 25 04:25:05.976: INFO: Pod "var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6" satisfied condition "Succeeded or Failed"
    Aug 25 04:25:05.981: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6 container dapi-container: <nil>
    STEP: delete the pod 08/25/22 04:25:06.001
    Aug 25 04:25:06.008: INFO: Waiting for pod var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6 to disappear
    Aug 25 04:25:06.010: INFO: Pod var-expansion-380511f6-d83d-4eea-b838-a25fe74200d6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 25 04:25:06.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3480" for this suite. 08/25/22 04:25:06.014
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:06.021
Aug 25 04:25:06.021: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename disruption 08/25/22 04:25:06.023
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:06.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:06.038
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 08/25/22 04:25:06.043
STEP: Waiting for the pdb to be processed 08/25/22 04:25:06.045
STEP: updating the pdb 08/25/22 04:25:08.053
STEP: Waiting for the pdb to be processed 08/25/22 04:25:08.062
STEP: patching the pdb 08/25/22 04:25:10.07
STEP: Waiting for the pdb to be processed 08/25/22 04:25:10.081
STEP: Waiting for the pdb to be deleted 08/25/22 04:25:12.092
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 25 04:25:12.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8315" for this suite. 08/25/22 04:25:12.099
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":352,"skipped":6514,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:06.021
    Aug 25 04:25:06.021: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename disruption 08/25/22 04:25:06.023
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:06.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:06.038
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 08/25/22 04:25:06.043
    STEP: Waiting for the pdb to be processed 08/25/22 04:25:06.045
    STEP: updating the pdb 08/25/22 04:25:08.053
    STEP: Waiting for the pdb to be processed 08/25/22 04:25:08.062
    STEP: patching the pdb 08/25/22 04:25:10.07
    STEP: Waiting for the pdb to be processed 08/25/22 04:25:10.081
    STEP: Waiting for the pdb to be deleted 08/25/22 04:25:12.092
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 25 04:25:12.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8315" for this suite. 08/25/22 04:25:12.099
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:12.103
Aug 25 04:25:12.103: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:25:12.104
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:12.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:12.119
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-17988a01-493d-4a66-84a7-0376860c9f24 08/25/22 04:25:12.126
STEP: Creating configMap with name cm-test-opt-upd-f056c18a-97ba-4d5c-adef-a5498af3919f 08/25/22 04:25:12.129
STEP: Creating the pod 08/25/22 04:25:12.132
Aug 25 04:25:12.143: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982" in namespace "projected-7793" to be "running and ready"
Aug 25 04:25:12.145: INFO: Pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530886ms
Aug 25 04:25:12.145: INFO: The phase of Pod pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:25:14.151: INFO: Pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982": Phase="Running", Reason="", readiness=true. Elapsed: 2.008523591s
Aug 25 04:25:14.151: INFO: The phase of Pod pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982 is Running (Ready = true)
Aug 25 04:25:14.151: INFO: Pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-17988a01-493d-4a66-84a7-0376860c9f24 08/25/22 04:25:14.171
STEP: Updating configmap cm-test-opt-upd-f056c18a-97ba-4d5c-adef-a5498af3919f 08/25/22 04:25:14.176
STEP: Creating configMap with name cm-test-opt-create-09961721-eb5b-4b52-9e21-17db1841433d 08/25/22 04:25:14.181
STEP: waiting to observe update in volume 08/25/22 04:25:14.185
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 25 04:25:18.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7793" for this suite. 08/25/22 04:25:18.22
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":353,"skipped":6517,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:12.103
    Aug 25 04:25:12.103: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:25:12.104
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:12.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:12.119
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-17988a01-493d-4a66-84a7-0376860c9f24 08/25/22 04:25:12.126
    STEP: Creating configMap with name cm-test-opt-upd-f056c18a-97ba-4d5c-adef-a5498af3919f 08/25/22 04:25:12.129
    STEP: Creating the pod 08/25/22 04:25:12.132
    Aug 25 04:25:12.143: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982" in namespace "projected-7793" to be "running and ready"
    Aug 25 04:25:12.145: INFO: Pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530886ms
    Aug 25 04:25:12.145: INFO: The phase of Pod pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:25:14.151: INFO: Pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982": Phase="Running", Reason="", readiness=true. Elapsed: 2.008523591s
    Aug 25 04:25:14.151: INFO: The phase of Pod pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982 is Running (Ready = true)
    Aug 25 04:25:14.151: INFO: Pod "pod-projected-configmaps-4a4de406-8739-4f35-b234-1e5345a21982" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-17988a01-493d-4a66-84a7-0376860c9f24 08/25/22 04:25:14.171
    STEP: Updating configmap cm-test-opt-upd-f056c18a-97ba-4d5c-adef-a5498af3919f 08/25/22 04:25:14.176
    STEP: Creating configMap with name cm-test-opt-create-09961721-eb5b-4b52-9e21-17db1841433d 08/25/22 04:25:14.181
    STEP: waiting to observe update in volume 08/25/22 04:25:14.185
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 25 04:25:18.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7793" for this suite. 08/25/22 04:25:18.22
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:18.225
Aug 25 04:25:18.226: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:25:18.227
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:18.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:18.242
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-4130/configmap-test-65aa80c9-8346-40c8-847a-17aad9e82bc5 08/25/22 04:25:18.247
STEP: Creating a pod to test consume configMaps 08/25/22 04:25:18.25
Aug 25 04:25:18.256: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca" in namespace "configmap-4130" to be "Succeeded or Failed"
Aug 25 04:25:18.258: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624201ms
Aug 25 04:25:20.264: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008815992s
Aug 25 04:25:22.264: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008015612s
STEP: Saw pod success 08/25/22 04:25:22.264
Aug 25 04:25:22.264: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca" satisfied condition "Succeeded or Failed"
Aug 25 04:25:22.268: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca container env-test: <nil>
STEP: delete the pod 08/25/22 04:25:22.274
Aug 25 04:25:22.282: INFO: Waiting for pod pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca to disappear
Aug 25 04:25:22.285: INFO: Pod pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:25:22.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4130" for this suite. 08/25/22 04:25:22.289
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":354,"skipped":6523,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.069 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:18.225
    Aug 25 04:25:18.226: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:25:18.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:18.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:18.242
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-4130/configmap-test-65aa80c9-8346-40c8-847a-17aad9e82bc5 08/25/22 04:25:18.247
    STEP: Creating a pod to test consume configMaps 08/25/22 04:25:18.25
    Aug 25 04:25:18.256: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca" in namespace "configmap-4130" to be "Succeeded or Failed"
    Aug 25 04:25:18.258: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624201ms
    Aug 25 04:25:20.264: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008815992s
    Aug 25 04:25:22.264: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008015612s
    STEP: Saw pod success 08/25/22 04:25:22.264
    Aug 25 04:25:22.264: INFO: Pod "pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca" satisfied condition "Succeeded or Failed"
    Aug 25 04:25:22.268: INFO: Trying to get logs from node bobymcbobs-c849-control-plane-p55pp pod pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca container env-test: <nil>
    STEP: delete the pod 08/25/22 04:25:22.274
    Aug 25 04:25:22.282: INFO: Waiting for pod pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca to disappear
    Aug 25 04:25:22.285: INFO: Pod pod-configmaps-a7ec63fa-e30c-485e-89dd-fca5541fe0ca no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:25:22.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4130" for this suite. 08/25/22 04:25:22.289
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:22.298
Aug 25 04:25:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename cronjob 08/25/22 04:25:22.299
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:22.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:22.314
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/25/22 04:25:22.318
STEP: creating 08/25/22 04:25:22.318
STEP: getting 08/25/22 04:25:22.322
STEP: listing 08/25/22 04:25:22.327
STEP: watching 08/25/22 04:25:22.33
Aug 25 04:25:22.330: INFO: starting watch
STEP: cluster-wide listing 08/25/22 04:25:22.332
STEP: cluster-wide watching 08/25/22 04:25:22.335
Aug 25 04:25:22.335: INFO: starting watch
STEP: patching 08/25/22 04:25:22.337
STEP: updating 08/25/22 04:25:22.344
Aug 25 04:25:22.352: INFO: waiting for watch events with expected annotations
Aug 25 04:25:22.352: INFO: saw patched and updated annotations
STEP: patching /status 08/25/22 04:25:22.352
STEP: updating /status 08/25/22 04:25:22.356
STEP: get /status 08/25/22 04:25:22.362
STEP: deleting 08/25/22 04:25:22.365
STEP: deleting a collection 08/25/22 04:25:22.375
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 25 04:25:22.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2316" for this suite. 08/25/22 04:25:22.386
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":355,"skipped":6565,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.093 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:22.298
    Aug 25 04:25:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename cronjob 08/25/22 04:25:22.299
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:22.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:22.314
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/25/22 04:25:22.318
    STEP: creating 08/25/22 04:25:22.318
    STEP: getting 08/25/22 04:25:22.322
    STEP: listing 08/25/22 04:25:22.327
    STEP: watching 08/25/22 04:25:22.33
    Aug 25 04:25:22.330: INFO: starting watch
    STEP: cluster-wide listing 08/25/22 04:25:22.332
    STEP: cluster-wide watching 08/25/22 04:25:22.335
    Aug 25 04:25:22.335: INFO: starting watch
    STEP: patching 08/25/22 04:25:22.337
    STEP: updating 08/25/22 04:25:22.344
    Aug 25 04:25:22.352: INFO: waiting for watch events with expected annotations
    Aug 25 04:25:22.352: INFO: saw patched and updated annotations
    STEP: patching /status 08/25/22 04:25:22.352
    STEP: updating /status 08/25/22 04:25:22.356
    STEP: get /status 08/25/22 04:25:22.362
    STEP: deleting 08/25/22 04:25:22.365
    STEP: deleting a collection 08/25/22 04:25:22.375
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 25 04:25:22.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2316" for this suite. 08/25/22 04:25:22.386
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:22.398
Aug 25 04:25:22.398: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename projected 08/25/22 04:25:22.399
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:22.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:22.411
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 08/25/22 04:25:22.415
Aug 25 04:25:22.421: INFO: Waiting up to 5m0s for pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5" in namespace "projected-5236" to be "running and ready"
Aug 25 04:25:22.423: INFO: Pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464334ms
Aug 25 04:25:22.423: INFO: The phase of Pod annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:25:24.428: INFO: Pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00710906s
Aug 25 04:25:24.428: INFO: The phase of Pod annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5 is Running (Ready = true)
Aug 25 04:25:24.428: INFO: Pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5" satisfied condition "running and ready"
Aug 25 04:25:24.949: INFO: Successfully updated pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 25 04:25:26.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5236" for this suite. 08/25/22 04:25:26.967
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":356,"skipped":6676,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [4.574 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:22.398
    Aug 25 04:25:22.398: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename projected 08/25/22 04:25:22.399
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:22.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:22.411
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 08/25/22 04:25:22.415
    Aug 25 04:25:22.421: INFO: Waiting up to 5m0s for pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5" in namespace "projected-5236" to be "running and ready"
    Aug 25 04:25:22.423: INFO: Pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464334ms
    Aug 25 04:25:22.423: INFO: The phase of Pod annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:25:24.428: INFO: Pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00710906s
    Aug 25 04:25:24.428: INFO: The phase of Pod annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5 is Running (Ready = true)
    Aug 25 04:25:24.428: INFO: Pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5" satisfied condition "running and ready"
    Aug 25 04:25:24.949: INFO: Successfully updated pod "annotationupdate26f69359-34c5-4bca-aad2-bf5c3ece83a5"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 25 04:25:26.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5236" for this suite. 08/25/22 04:25:26.967
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:26.973
Aug 25 04:25:26.973: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename resourcequota 08/25/22 04:25:26.974
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:26.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:26.99
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 08/25/22 04:25:26.995
STEP: Getting a ResourceQuota 08/25/22 04:25:26.999
STEP: Listing all ResourceQuotas with LabelSelector 08/25/22 04:25:27.002
STEP: Patching the ResourceQuota 08/25/22 04:25:27.005
STEP: Deleting a Collection of ResourceQuotas 08/25/22 04:25:27.012
STEP: Verifying the deleted ResourceQuota 08/25/22 04:25:27.018
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 25 04:25:27.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7385" for this suite. 08/25/22 04:25:27.025
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":357,"skipped":6684,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.057 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:26.973
    Aug 25 04:25:26.973: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename resourcequota 08/25/22 04:25:26.974
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:26.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:26.99
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 08/25/22 04:25:26.995
    STEP: Getting a ResourceQuota 08/25/22 04:25:26.999
    STEP: Listing all ResourceQuotas with LabelSelector 08/25/22 04:25:27.002
    STEP: Patching the ResourceQuota 08/25/22 04:25:27.005
    STEP: Deleting a Collection of ResourceQuotas 08/25/22 04:25:27.012
    STEP: Verifying the deleted ResourceQuota 08/25/22 04:25:27.018
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 25 04:25:27.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7385" for this suite. 08/25/22 04:25:27.025
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:27.031
Aug 25 04:25:27.031: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename configmap 08/25/22 04:25:27.032
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:27.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:27.053
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-7e70403e-7312-4576-9b4a-d1a8984c58ee 08/25/22 04:25:27.061
STEP: Creating configMap with name cm-test-opt-upd-5d5164a0-6344-4494-a05c-11fa8b5b0a06 08/25/22 04:25:27.064
STEP: Creating the pod 08/25/22 04:25:27.068
Aug 25 04:25:27.075: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940" in namespace "configmap-2462" to be "running and ready"
Aug 25 04:25:27.079: INFO: Pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106344ms
Aug 25 04:25:27.079: INFO: The phase of Pod pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940 is Pending, waiting for it to be Running (with Ready = true)
Aug 25 04:25:29.083: INFO: Pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940": Phase="Running", Reason="", readiness=true. Elapsed: 2.008392344s
Aug 25 04:25:29.083: INFO: The phase of Pod pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940 is Running (Ready = true)
Aug 25 04:25:29.083: INFO: Pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-7e70403e-7312-4576-9b4a-d1a8984c58ee 08/25/22 04:25:29.101
STEP: Updating configmap cm-test-opt-upd-5d5164a0-6344-4494-a05c-11fa8b5b0a06 08/25/22 04:25:29.105
STEP: Creating configMap with name cm-test-opt-create-1bb8e60e-1dbe-49a1-bdf3-7365840381d7 08/25/22 04:25:29.11
STEP: waiting to observe update in volume 08/25/22 04:25:29.114
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 25 04:25:33.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2462" for this suite. 08/25/22 04:25:33.147
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":358,"skipped":6688,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:27.031
    Aug 25 04:25:27.031: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename configmap 08/25/22 04:25:27.032
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:27.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:27.053
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-7e70403e-7312-4576-9b4a-d1a8984c58ee 08/25/22 04:25:27.061
    STEP: Creating configMap with name cm-test-opt-upd-5d5164a0-6344-4494-a05c-11fa8b5b0a06 08/25/22 04:25:27.064
    STEP: Creating the pod 08/25/22 04:25:27.068
    Aug 25 04:25:27.075: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940" in namespace "configmap-2462" to be "running and ready"
    Aug 25 04:25:27.079: INFO: Pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106344ms
    Aug 25 04:25:27.079: INFO: The phase of Pod pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940 is Pending, waiting for it to be Running (with Ready = true)
    Aug 25 04:25:29.083: INFO: Pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940": Phase="Running", Reason="", readiness=true. Elapsed: 2.008392344s
    Aug 25 04:25:29.083: INFO: The phase of Pod pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940 is Running (Ready = true)
    Aug 25 04:25:29.083: INFO: Pod "pod-configmaps-8e83df4a-03ea-4cfa-9db7-7c6e6949b940" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-7e70403e-7312-4576-9b4a-d1a8984c58ee 08/25/22 04:25:29.101
    STEP: Updating configmap cm-test-opt-upd-5d5164a0-6344-4494-a05c-11fa8b5b0a06 08/25/22 04:25:29.105
    STEP: Creating configMap with name cm-test-opt-create-1bb8e60e-1dbe-49a1-bdf3-7365840381d7 08/25/22 04:25:29.11
    STEP: waiting to observe update in volume 08/25/22 04:25:29.114
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 25 04:25:33.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2462" for this suite. 08/25/22 04:25:33.147
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/25/22 04:25:33.152
Aug 25 04:25:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
STEP: Building a namespace api object, basename runtimeclass 08/25/22 04:25:33.154
STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:33.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:33.17
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 25 04:25:33.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4707" for this suite. 08/25/22 04:25:33.184
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":359,"skipped":6698,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
------------------------------
• [0.036 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/25/22 04:25:33.152
    Aug 25 04:25:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-239949510
    STEP: Building a namespace api object, basename runtimeclass 08/25/22 04:25:33.154
    STEP: Waiting for a default service account to be provisioned in namespace 08/25/22 04:25:33.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/25/22 04:25:33.17
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 25 04:25:33.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4707" for this suite. 08/25/22 04:25:33.184
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":359,"skipped":6705,"failed":3,"failures":["[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]"]}
Aug 25 04:25:33.190: INFO: Running AfterSuite actions on all nodes
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Aug 25 04:25:33.190: INFO: Running AfterSuite actions on node 1
Aug 25 04:25:33.190: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Aug 25 04:25:33.190: INFO: Running AfterSuite actions on all nodes
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Aug 25 04:25:33.190: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Aug 25 04:25:33.190: INFO: Running AfterSuite actions on node 1
    Aug 25 04:25:33.190: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.056 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------


Summarizing 3 Failures:
  [FAIL] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  vendor/github.com/onsi/ginkgo/v2/internal/suite.go:596
  [FAIL] [sig-apps] Daemon set [Serial] [It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:434
  [FAIL] [sig-architecture] Conformance Tests [It] should have at least two untainted nodes [Conformance]
  vendor/github.com/onsi/ginkgo/v2/internal/suite.go:596

Ran 362 of 7067 Specs in 5802.648 seconds
FAIL! -- 359 Passed | 3 Failed | 0 Pending | 6705 Skipped
--- FAIL: TestE2E (5802.81s)
FAIL

Ginkgo ran 1 suite in 1h36m42.886689028s

Test Suite Failed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.4[0m

